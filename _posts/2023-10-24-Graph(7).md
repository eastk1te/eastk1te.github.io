---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)



## A singl layer of a GNN

GNN layer = Message + Aggregation
GNN 층의 아이디어는 벡터들의 집합을 하나의 벡터로 압축하는 것이다.
두가지 절차로
1. message
    - $$m_u^{(l)}= MSG^{(l)}(h_u^{(l-1)})$$
    - 각 노드는 message를 생성하고 다른 노드들에게 나중에 보낸다.
    - ex) linear layer $$m_u^{(l)}= W^{(l)}(h_u^{(l-1)})$$
2. Aggregation
    - 각 노드는 v의 이웃들로 부터 각 메시지를 종합한다.
    - $$h_u^{(l)}= SUM(m^{(l)}_u, u \in N(v))$$
    - $$h_u^{(l)}= SUM(m^{(l)}_u, u \in N(v))$$

node v 스스로 정보를 잃어버리게 된다.
$$h_v^{(l)}$$은 $$h_v^{(l-1)}$$에 직접적으로 의존하지 않는다.

따라서 $$h_v^{(l-1)}$$을 $$h_v^{(l)}$$계산에 포함시킨다.

- $$m_u^{(l)}= W^{(l)}(h_u^{(l-1)})$$
- $$m_v^{(l)}= B^{(l)}(h_v^{(l-1)})$$
- $$h_v^{(l)}= CONCAT(AGG(m^{(l)}_u, u \in N(v)), m_v^{(l)})$$
- 이웃들을 먼저 종합하고 node 자기 자신을 다시 합침. 

Classical GNN Layers
GCN

_Figure 7-1_

GraphSAGE

현재 노드와 이웃 노드를 분리.

_Figure 7-2_
two-state, state 1 :$$A_{N(v)}^{(l)} \leftarrow AGG(\{h_u^{(l-1)}, \forall u \in N(v)\})$$
노드 이웃들을 종합, State 2 노드 스스로 종합함.

Graph Attestion Networks
_Figure 7-3_

$$a_{vu}= \frac{1}{|N(v)|}$$로 u에서 v로 보내는 메시지의 가중치 factor이다.
그래프의 구조적 속성에 기반하여 명시적으로 정의되어있다.
모든 노드의 이웃이 동일하게 중요하지는 않다.,
Attetion은 cognitive attention에서 영감을 받았다.
attetion $$a_{vu}$$는 입력 데이터의 중요한 부분만 집중하고 나머지는 흐려지게 만든다.

Attetion Mechanism
$$a_{vu}$$를 어텐션 메커니즘의 부산물로 두자.
1. 어텐션 상호효과 $$e_{vu}$$는 $$a$$를 가지고 계산한다. 
    _Figure 7-4:상호효과는 u에서 노드 v로 가는 메시지의 중요도 지표이다._
2. Normalize $$e_{vu}$$ 어텐션 가중치에
   _Figure 7-5 : softmax 함수를 사용_
3. 가중 합 진행 FIgure 7-3처럼


- concat → Linear(scalar)
- 두 node 사이의 상관관계를 scoring
- Q) Edge를 반영할 수 있나요?
    - 넵 끼어들 여지가 많아보입니다
    - 망상 방법 1)
        - Linear(Concat(h_A, h_B, edge 정보)) → edge정보로 attention weight를 조절
    - 망상 방법 2)
        - edge의 case 마다 서로 다른 W를 사용
- NLP에도 비슷한 방법이 있었음!
    - [Lee, K., He, L., Lewis, M., & Zettlemoyer, L. (2017). End-to-end Neural Coreference Resolution. *ArXiv, abs/1707.07045*.](https://arxiv.org/abs/1707.07045)
    - 목표 : span(연속된 token)에 대한 embedding을 만들어보자~
        - 차이 : 이웃 node를 고려한 encoding ↔ 주변에 존재하는 token을 고려한 encoding
    - 상호참조해결
        - **나**는 **허철훈**.. 내일 **회사**를 폭발시킬 예정이다.. 기다려라 **엔씨소프트**..
    
    - [system] 너는 주어진 문장에 대해 상호참조해결을 진행해야돼ㅑ. [input] **나**는 **허철훈**.. 내일 **회사**를 폭발시킬 예정이다.. 기다려라 **엔씨소프트**.. [output]
    
    _Figure 7-9_
    
    - 해당 task는 상호참조해결이기 때문에, span이 명사류인지 아닌지도 중요함!
    - 명사인지 아닌지는 span의 첫번째, 마지막 token을 보면 쉽게 알 수 있음!
    - 따라서 최종 embedding = [attention으로 만든 emb; 첫번째 토큰 emb; 마지막 토큰 emb; ...]


_Figure 7-6_

파라미터 a는 결합적으로 학습된다. 가중치 행렬과 함께 파라미터는 학습된다. 

멀티 헤드 어텐션 : 어텐션 매커니즘의 학습 절차를 안정시킨다.
다중 어텐션 점수를 만든다.
이렇게해서 나온 결과들을 종합한다.

- 헉 이거 트랜스포머에서 보던거 아님?

![Untitled](https://prod-files-secure.s3.us-west-2.amazonaws.com/91873a7c-0609-4f38-bb43-d305be3a248d/bdc7930a-0607-4356-be38-4837975afbb8/Untitled.png)

[출처](https://www.blossominkyung.com/deeplearning/transformer-mha)

_Figure 7-7_

이러한 어텐션 매커니즘의 이익으로 다른 이웃들의 다른 중요도 값 $$a_{vu}$$을 암묵적으로 명시하는것을 허락한다.

계산적 효율성 : 그래프의 모든 edge들에 대해 병렬적으로 어텐셔널 coefficient를 계산할 수 있다.
저장 효율성 : 희소 행렬은 $$O(V+E)$$보다 크게 요구되지않는다. 그래프 크기에 구애받지않고 파라미터의 수가 고정된다.
Localized : local 네트워크 이웃들만 주의를 기울인다
귀납 능력 : edge-wise 매커니즘으로 공유된다. 글로벌 그래프 구조에 더이상 의존하지않는다.

Cora Citation Net
_Figure 7-8_


실제로 이러한 classic GNN layer는 훌륭한 시작지점이 된다.
일반적인 GNN layer 디자인을 고려함으로 더 좋은 성능을 낼 수 있다. 현대의 심층신경망 module를 포함시켜 많은 도메인에 활용할 수 있다.

_Figure 7-10_
배치 정규화 : 신경망 학습을 안정화, 임력값ㄷ에 배치를 줌. 노드 임베딩의 평균을 0으로 re-center, 변수를 유닛 variance로 re-scale 함
드랍 아웃 : 오버피팅 방지, 학습동안 무작위로 뉴런을 turn off 함, 테스팅 동안에는 모든 뉴련을 사용함, GNN에서 드랍아웃은 메시지 함수에서 선형층에 사용할 수 있음.
어텐션, 게이팅 : 중요한 메시지를 조절함
다른 유용한 심층신경망 모듈들
activation : 비선형, Rectified linear unit ReLU, sigmoid, Parametric ReLU(ReLU보다 실증적으로 성능이 좋다.)

[GraphGym](https://github.com/snap-stanford/GraphGym)을 통해 GNN의 다양한 디자인을 탐구해볼 수 있다.

### Stacking Layers

GNN은 어떻게 건설해야하는가?
기본적인 방법은 GNN 층을 연속적으로 쌓는 방법이다. 입력으로 초기의 raw 노드 특징 $$x_v$$를 넣고, L GNN Layer를 거친 $$h_v^{(L)}$$ 노드 임베딩을 얻게된다.

_Figure 7-11_

over-smoothing problem
많은 GNN을 쌓는 것은 over-smoothing에서 야기되는 문제가 있다 
over-smoothing은 모든 노드 임베딩이 같은 값으로 임베딩 되는 것으로, 노드들의 임베딩으로 노드들을 구분$$_{differentiate}$$짓고 싶어하기떄문에 문제가 된다.
왜 이러한 문제가 발생하는가?

수용적인 영역에서노드들의 집합은 관심있는 노드의 임베딩을 결정한다.

_Figure 7-12_
K 층을 가진 GNN에서 각 노드들은 K-hop 떨어진 이웃들은 수용 영역을 가진다.
hop 수를 늘리면(GNN 층을 늘리면) 공유되는 이웃들이 빠르게 증가한다.

- 즉, layer를 쌓을수록 점점 더 많은 주변 node의 features를 반영하게 된다.
- 즉 layer를 깊게 쌓을수록 서로 다른 node가 참고하는 node가 겹치게 되어 embedding space 내에 비슷한 공간으로 mapping하게 된다. (Over-smoothing)
- Q) dropout으로 해결할 수 있나요?
    - 아니요. dropout은 NN 내의 cell의 의존성을 줄여 overfitting을 피하는 방법입니다.
    - 요문제를 해결하기 위해서는
        - 겹치는 node를 막거나 (Do not stack)
        - 어떻게든 잘 피해서 표상을 하거나 (Skip Connection), ~~NLP충이라서 요친구에 대해 잘 모릅니다..~~
        - [요런 것도 있네요](https://ydy8989.github.io/2021-03-03-GAT/#over-smoothing-problem:~:text=%EB%82%B4%EC%97%90%EC%84%9C%20%ED%95%A9%EC%82%B0%ED%95%A0%20%EB%95%8C-,Over%2Dsmoothing%20Problem,-Over%2Dsmoothing(%EC%A7%80%EB%82%98%EC%B9%9C)

수용영역 정의를 통해 오버 스무딩을 설명할 수 있었다.
노드의 임베딩은 수용영역에 의해 결정된다. 만약 두 노드가 심하게 수용영역이 겹치면 두 노드의 임베딩은 매우 유사할 것(구분되지 못할정도로)이다.

GNN 층을 많이 쌓으면 -> 노드들의 수용영역이 많이 겹칠것이고 -> 각 임베딩은 매우 유사해질것이고 -> 오버스무딩 문제를 야기한다.

어떻게 극복하는가?>

우리는 오버 스무딩 문제에서 뭘 배울 수 있었는가?
GNN의 층을 늘릴떄는 조심해야한다. 다른 도메인의 신경망과는 다르게 CNN 고 같은 GNN에서는 항상 도움이 되지는 않는다. 또한, 문제를 풀떄는 필수 수용영역을 분석해야한다. 그리고 우리가 좋아하는 수용영역보다 조금 작게 GNN층을 설정해야한다. L을 필요 이상으로 크게하지 마라

그럼 GNN의 표현력을 어떻게 강화시킬까? GNN의 층 수가 작으면?

Shallow GNN의 표현력
Shallow GNN을 어떻게 더 표현력있게 만들 수 있을까?
solutiopn 1. 각 GNN의 층의 표현력을 더 늘린다. 우리는 Agregation, Transformation을 심층 신경망으로 만들 수 있다. 만약 피요하다면 각 box를 3-layer MLP로 만든다.
solution2. 메시지를 넘기지않는 층을 늘린다.
GNN 층을 포함한 GNN만 있을 필요는 없다. 따라서 GNN layer 앞뒤로 MLP layer를 더 깔아서 pre-process, post-process layer를 만들 수 있따. 각 layer는 노드 특징을 인코딩할 필요가 있을떄 중요하고, 노드 임베딩을 reasoning, transformation 하는게 필요할떄 중요하다. 예를들어 노드가 이미지나 텍스트를 표현하거나 graph classfication, knowlege graph 일때 실제로 이러한 층을 넣는게 잘 적용된다.

만약 우리가 더 많은 GNN 층이 필요할떄는 어떻게해야할까?
Lesson 2. GNN에서 skip connection을 추가한다.
오버 스무딩에서 관찰했듯이 때때로 초기 GNN층에서 노드임베딩은 더 노드를 잘 구별했다. 따라서 우리는 초기 층에서의 영향력을 마지막 노드 임베딩에 늘리면된다 GNN의 지름길 shortcut을 추가함으로써
_Figure 7-13_

Residual connetion과 유사하징 않나?

왜 skip connection이 먹히나?
스킵 커넥션은 모듈들의 혼합을 만들어 낸다. N 스킵 커넥션은 $$2^N$$의 가능한 통로를 만들어내고 자동적으로 얕은 GNNㅇ과 깊은 GNN들의 혼합물을 만들어 낸다.

_Figure 7-14_
[Veit et al. Residual Networks Behave Like Ensembles of Relatively Shallow Networks, ArXiv 2016](https://arxiv.org/abs/1605.06431)

다른 옵션으로는 마지막 층에 직접적으로 넘기는 방법이있다. 따라서 이전층에서의 모든 노드 임베딩을 직접적으로 종합한다.


## Graph Manipulation in GNNs

일반적인 GNN Framework에서는 Raw 입력 그래프와 계산가능한 그래프는 같지 않다. 그래프의 feature 확대$$_{augmentation}$$, 그래프 구조를 조작$$_{Manipulation}$$해야한다.

_Figure 7-15_

우리는 지금까지 임력 그래프가 계산가능한 그래프라고 가정해왔다.
이러한 가정을 걔는 이유는
Feature level에서 입력 그래프가 feature의 부족하면 feature augmentation 증대를 해야하고,
structure level에서 그래프가 너무 희소하면 효율적이지 못한 메시지 패싱이 일어나고 그래프가 너무 밀도가 높으면 메시지 전달에 너무 많은 비용이 들고, 그래프가 너무 크면 GPU에 계산가능하게 적용하지 못한다.

입력 그래프는 임베딩을 위한 최적의 계산 그래프가 될 수 없다.

따라서 우리는 차례떼ㅐ로 
feature augmentation, 
기본적인 접근법으로 
a)정수 값을 배정, 모든 노드는 구별가능하나 GNN은 그래프의 구조에서 학습하므로 표현력은 중간정도, 새로운 노드에 대해 귀납적인 학습이가능해 일반화가 높다. 계산 비용이 1차원이므로 매우 낮다, 아무 그래프나 inductive settings
b) 노드의 unique ID를 배정. 특정 노드의 정보가 저장되어 표현력이 높음, 새로운 노드를 일반화하지 못해 귀납 능력이 낮다. one-hodt node feature이므로 차원이 높아져 계싼 비용이 높다, 작은 그래프나 transductive setting에 사용
GNN으로 학습하기 어려운 확실한 구조일때 진행햐아함. GNN은 노드가 l길이의 cycle에 있는지 학습할 수 있는가?
안타깝게도 불가능하다.노드 v는 어떤 그래프 안에 존재하는지 구별하지 못한다. 왜냐하면 그래프에에서 모든 노드는 2 degree를 가지기 떄문에 계산가능한 그래프는 항상 같은 이진트리가 될 것이다. 따라서, 노드 feature로 cycle lenfth 차원에서 해당 length의 개수를 센다. 다른 특징을 증가하는 방법으로 군집 계수, PageRank, 중심성 등등 2번째 강의에서 설명한 특징들을 사용할 수 있다.


add virtual nodes/ edges, 일반적인 접근법으로 2-hop 이웃들을 가상의 edge로 연결한다. 인접행렬을 A가 아닌 $$A+A^2$$을 사용한다. Bipartite graph 양자 그래프에서 사용한다.
또는, 가상의 노드를 그래프의 모든 노드와 연결한다. 이러한 노드를 추가한 후에 모든노드들의 거리는 2가 되어 희소 그래프에서 메시지 전달이 매우 향상된다.

메시지 패싱할떄 이웃들을 sampling, 
이전에는 모든 노드들이 메시지 전달을 위해 사용되었지만 임베딩 계싼을 할떄마다 메시지 전달에 노드들을 (무작위하게) 표집한다.
기대한 바에 따르면 모든 노드를 사용한 임베딩과 유사하게 될것이고 계산 비용을 매우 낮춘다. 그리고 실제로도 잘 먹힌다.

부분 그래프의 sample을 임베딩 계산하는데 사용 하는 방법들로 해결이 가능(Scaling up GNN에서 배울 내용.)하다. 




<br><br>
---