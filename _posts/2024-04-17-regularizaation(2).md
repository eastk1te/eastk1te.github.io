---
title: '[Paper]Regularization(2)'
author: east
date: 2024-04-17 12:38:00 +09:00
categories: [Paper, Regularization]
tags: [Paper, Regularization, Data Augmentation, BatchNormalization, Dropout, EarlyStopping]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
# 
# ⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻ
---


이전 포스팅에서 explicit regluarization을 소개했습니다. 다음으로 Implicit regularization으로 흔히 사용되는 Data augmentation, EarlyStopping, Batch Normalization, Dropout 등을 소개하겠습니다.
Label Smoothing(라벨 스무딩은 모델이 너무 확신하기 쉬운 예측을 방지하기 위해 정답 라벨에 대한 신뢰도를 조금 낮추는 기법입니다. 이를 통해 모델이 일반적인 특징에만 의존하여 좀 더 일반화된 결정을 내릴 수 있습니다.)

_Figure 2 : paperswithcode_

> ## Ⅰ. Implicit Regularization

Explicit가 아닌 기술들은 자동적으로 Implicit 카테고리로 들어가게 됩니다. 즉, 손실함수에 직접적으로 명시하지 않고도 과적합을 방지하는 방법을 의미합니다.

> ## Ⅱ. Data augmentation

과적합은 학습 데이터에 너무 맞춰져 있어 새로운 데이터에 대한 일반화 성능이 저하되는 현상입니다. 따라서 이를 해결하기 위해 학습 데이터를 늘려서  다양성을 증가시켜 모델이 일반적인 패턴을 파악하도록 하는 방법입니다. 이러한 방법은 다른 방버들과 다르게 일반화 성능의 일관된 향상을 제공해 파라미터를 재조정하는 등의 작업이 필요하지 않습니다.

_Figure 3 : 일반화 성능 비교, (라이트 증강) 수평, 수직 이동 등과 같은 간단한 변환 (헤비 증강) 보다 넓은 범위의 아핀 변환 등을 수행_

보통 이미지 데이터에는 미러링, 크롭핑, 필터 적용 등과 같은 변환 기법을 적용합니다. 또한 라이트닝, 블러링, 쉬프팅 등과 같은 효과를 추가하여 다양성을 높입니다.

또한, 텍스트 데이터들에는 번역, 동의어 및 유의어 교체, 문장 순서의 재구성 등으로 증강이 가능합니다.





> ## Ⅲ. Early stopping

EarlyStopping은 학습 중 가장 좋은 가중치를 저장하고 업데이트하며 더 이상 성능의 개선이 이루어지지 않을때 학습을 중지하는 방법으로 최적화 과정을 매개변수 공간으로 제한하는 정규화 기법으로 작동합니다.

신경망의 지도 학습 중에 과적합이 시작되는 시점을 감지하는데 검증 데이터 셋을 사용할 수 있으며 과적합을 피하기 위해 수렴하기 전에 아래와 같이 중지합니다. 그러나 이러한 방법은 조기 종료의 일반적인 기준을 선택하기가 어렵고, 학습이 갑자기 중단되는 경우가 많습니다.

_Figure 1 : 이상적인 학습 및 검증 오차 곡선_

일반적인 지도 학습에서 Figure 1과 같은 이상적인 다이어그램을 확인할 수 있습니다. 그러나 실제로는 위처럼 이상적으로 나오지않고 일반화 오류가 증가해도 유효성 검사 오류가 더 낮아질 수 있어 이러한 여러 지역 최소값을 갖게됩니다. 따라서 훈련 시간과 일반화 오류 사이의 최적의 결과값을 찾기란 쉽지 않습니다.



> ### On EarlyStopping in Gradient desent learning

해당 논문을 통해 조기 종료 규칙(epoch 수에 관한 함수)으로 일반화 오류의 확률적 상한선을 제안하여 EarlyStopping을 통해 일반화 오류를 줄이는 원리를 설명하였습니다.

EarlyStopping은 ML에서 비모수 회귀 문제를 정규화하는데 사용될 수 있는데, 주어진 입력 공간 X, 출력 공간 Y에서 주어진 확률분포 ρ에서 특정 입력 x를 기반으로 하는 회귀함수 $$f_{ ρ}$$를 찾는 것입니다. 즉, 해당 회귀함수와 가까워지는 과정에서 EarlyStopping을 통해 모델이 학습 데이터에만 너무 많이 적합되는 것을 방지할 수 있습니다.

$$ f_{ρ}(x) = \int_{Y} y \, d ρ(y \mid x), \, x \in X $$

회귀 함수($$f_{ ρ}$$)를 근사화하는 데 일반적으로 선택되는 한 가지 방법은 재생 커널 힐베르트 공간에서 함수를 사용하는 것입니다. 이러한 공간은 무한 차원이 될 수 있으며, 이는 임의 크기의 훈련 세트에 과적합되는 해결책을 제공할 수 있습니다. 

`Reproducing Kernel Hilbert Space`
: 재생커널힐베르트 공간은 함수 공간의 특별한 유형으로, 함수를 포함하고 있는 유클리드 공간의 개념을 무한 차원으로 일반화한 Hilbert 공간입니다. 이 공간은 임의의 입력값에 대한 함수의 값들을 내적으로 계산할 수 있도록 하는 커널의 일종으로 재생 성질($$f(y) = \langle f, K(\cdot, y) \rangle, \; y \in \forall $$)을 만족하는 재생 커널(reproducing kernel)을 지니고 있습니다. $$K(x_i, x_j) = \langle \Phi(x_i), \Phi(x_j) \rangle$$ 에서  $$K(\cdot)$$ 는 커널 함수를 나타내며,  $$\Phi(\cdot)$$ 는 입력 데이터  $$x_i$$를 고차원 특징 공간으로 매핑하는 함수입니다. 즉, 커널 함수를 사용하여 데이터를 고차원 특징 공간으로 매핑하는 함수 공간입니다.

이러한 비모수 회귀 문제를 정규화하는 한 가지 방법은 경사 하강법과 같은 반복적인 절차에 조기 중단 규칙을 적용하는 것입니다. 제안된 조기 중단 규칙은 반복 횟수의 함수로 일반화 오류에 대한 상한에 대한 분석을 기반으로 합니다. 

즉, 반복횟수가 증가함에 따라 일반화 오류가 어떻게 변화하는지를 고려한 상한을 분석하여 모델의 학습을 언제 중단해야할지 결정합니다.


> ### Example of MSE

MSE를 예시로 $$X \subseteq \mathbb{R}^n$$이고 $$Y = \mathbb{R}$$일 때, 독립적으로 추출된 샘플 집합 $$\mathbf{z} = {(x_i, y_i) \in X \times Y : i = 1, \dots, m} \in Z^m $$ 에 대해, 다음과 같은 기능을 최소화합니다.

$${\mathcal{E}}(f) = \int_{X \times Y} (f(x) - y)^{2} \, d ρ$$

여기서 f는 재생성 커널 힐베르트 공간 $$\mathcal{H}$$의 구성원이고, 우리는 최소 제곱 손실 함수에 대한 기대 오차를 최소화해야합니다. $$\mathcal{E}$$가 알려지지 않은 확률 측도  ρ에 의존하기 때문에 계산에 사용할 수 없습니다. 따라서 아래와 같이 실제 데이터 집합에 대한 경험적인 손실함수의 기댓값을 사용하여 모델을 조정하고 최적화합니다.

$$\mathcal{E}_{\mathbf{z}}(f) = \frac{1}{m} \sum_{i=1}^{m} (f(x_i) - y_i)^{2} \tag{1}$$

따라서, 우리는 아래와 같이 $$f_z$$와 $$f_ρ$$의 기대 위험의 차이를 제어하고 싶습니다. 

$$\mathcal{E}(f_{t}^{\mathbf{z} })-{\mathcal{E}}(f_ρ)$$

이러한 차이는 두 가지 항의 합으로 다시 쓸 수 있습니다.

$${\mathcal {E}}(f_{t}^{\mathbf {z} })-{\mathcal {E}}(f_{ ρ }) = \left[{\mathcal {E}}(f_{t}^{\mathbf {z} })-{\mathcal {E}}(f_{t})\right]+\left[{\mathcal {E}}(f_{t})-{\mathcal {E}}(f_{ ρ })\right]$$

위 방정식은 편향-분산의 균형을 제시하며 이를 해결하는 것으로 최적의 조기 종료 규칙을 얻을 수 있습니다. 이 규칙은 일반화 오류에 대한 확률적 한계와 연관되어 교차 검증과 같은 데이터 기반 방법을 사용한 적응형 조기 종료 규칙을 얻을 수 있습니다.

위의 나온 (1)을 최적화 하기위해 아래와 같은 경사하강법으로 현재 모델과 기준 모델 사이의 차이에 대한 커널 손실이 적용됩니다.

$$\begin{align}
f_{t+1} &= f_t - \gamma_t L_K(f_t - f_{\rho}), \quad f_0 = 0 \\
\gamma_t &= \frac{1}{\kappa^2(t + 1)^\theta} \;, \theta \in [0, 1] \\
(L_K f)(x') &= \int_{X} K(x', x) f(x) \, d\rho_X \\
∇E(f) &= L_Kf - L_Kf_{\rho}\\
∇E_z(f) &= \frac{1}{m} \sum_{i=1}^{m} (f(x_i) - y_i)K(x_i) \\
\end{align}$$ 

위와 같은 방법을 통해 함수 f의 (1) RKHS에 대한 목적 함수와 (2) 일반화 오차를 최소화하는 값을 보여줍니다.


_Figure 4 : 조기중단 규칙 하 모델의 일반화 오류에 대한 관계_

Figure 4를 통해 모델의 근사 오차를 최소화하는 학습 데이터(m)과 관련된 최적의 조기 중단 규칙을 설정할 수 있었습니다. 이것은 해당 규칙 아래 모델의 일반화 오류가 데이터 크기 m에 따라 어떻게 감소하는지에 대한 내용도 나타냅니다.

_Figure 5 : 일반화 오류에 대한 두번째 상한선._

Figure 5에서 일반화의 오류를 분산(Sample Error)과 편향(Approximation Error)에 대한 식으로 표현한 후 이 상한값을 최소화하는 t값을 찾기위해 최소 정수로 설정하여 계산합니다. 이를 통해 최적의 조기 종료 시점을 사용했을때의 상한선을 추정합니다. 

즉, Figure 5 마지막에사 최적의 조기 종료 규칙($$t^*(m)$$)에 따른 `상한을 추정하여 학습을 조기에 중단하여 해당 상한 이상으로 학습이 진행되지 않도록 제한함으로써 과적합을 방지`하게 되었습니다. 따라서, 이러한 EarlyStopping을 통해 통해 학습 프로세스가 이 상한값을 넘어가지 않도록 설정하는 것이 중요합니다.

이렇게 이론적으로 도출된 최적의 조기 종료 시간은 근사값일 뿐이라 실제 적용에서는 해당 값에 근거하여 초기 가이드라인을 제안하되 조정할 필요가 있습니다.

> ### Code

Keras에서는 Loss가 몇번의 epoch 동안 개선되지 않으면 학습을 조기에 중단하도록 하거나 가장 좋았던 지점의 가중치를 복원하는 옵션등을 사용합니다.

```python
callback = keras.callbacks.EarlyStopping(monitor='loss', patience=3)
...
history = model.fit(..., callbacks=[callback])
```





> ## Ⅳ. Batch Normalization

 2015 · 54311회 인
[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate  Shift](https://arxiv.org/pdf/1502.03167.pdf)
배치 정규화 (Batch Normalization)
가중치의 활성화값이 적당히 퍼지게끔 '강제'로 적용시키는 것

미니배치 단위로 데이터의 평균이 0, 표준편차가 1로 정규화

학습을 빨리 진행할 수 있음

초기값에 크게 의존하지 않아도 됨

과적합을 방지

보통 Fully-Connected와 활성화함수(비선형) 사이에 놓임

https://www.jeremyjordan.me/batch-normalization/


미니배치 크기(Mini Batch Size)
미니 배치 학습

한번 학습할 때 메모리의 부족현상을 막기 위해
전체 데이터의 일부를 여러번 학습하는 방식
한번 학습할 때마다 얼마만큼의 미니배치 크기를 사용할지 결정

배치 크기가 작을수록 학습 시간이 많이 소요되고,
클수록 학습 시간이 학습 시간은 적게 소요된다.


> ## Ⅴ. Dropout

역전파 알고리즘, DBN, t-SNE 등등 선구적인 논문을 낸 Geoffrey E. Hinton가 고안한 



[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
 2014 · 49856회 인용

_Figure 6 : (a) 일반적인 신경망 (b) 드랍아웃을 적용한 신경망_

간단히 말해서, 학습 중에는 신경망의 연결 일부를 무작위로 끊어서, 각 부분이 너무 서로 의존적으로 발달하는 것을 막습니다. 이 과정을 통해 마치 여러 다른 신경망을 학습시키는 것과 같은 효과를 낼 수 있어요. 그리고 실제로 사용할 때는, 이렇게 학습된 여러 신경망의 평균 예측값을 내는 것과 비슷한 결과를 내는데, 이는 마치 연결을 끊지 않은, 즉 더 많은 연결을 가진 단 하나의 신경망을 사용하는 것으로 단순화해서 해결할 수 있습니다. 이렇게 하면 학습된 모델이 더 잘 일반화되어 새로운 데이터에 대해 더 좋은 성능을 낼 수 있게 됩니다.

간단히 말해서 학습 중 신경망의 연결을 무작위로 삭제하는 것으로 unit들이 너무 많은 co-adapting을 예방합니다.학습동안 dropout은 지수적으로 다른 학습된 네트워크를 sample하고 테스트시 이러한 모든 학습된 네트워크의 예측 평균의 효과를 근사하는게 간단하게 작은 가중치를 가지는 단일 unthinned network를 사용함으로써 쉬워집니다. 




딥 신경망은 여러 비선형 은닉층을 포함하고 있어 입력과 출력 간의 매우 복잡한 관계를 학습할 수 있는 매우 표현력이 높은 모델입니다. 그러나 제한된 훈련 데이터에서는 이러한 복잡한 관계 중 많은 것이 샘플링 노이즈의 결과이기 때문에 동일한 분포에서 추출되었더라도 실제 테스트 데이터에는 존재하지 않을 수 있습니다. 이는 과적합을 초래하며 많은 방법이 이를 줄이기 위해 개발되었습니다. 이러한 방법에는 검증 세트의 성능이 점점 나빠지기 시작할 때 훈련을 중지하는 것, L1 및 L2 정규화와 소프트 가중치 공유와 같은 다양한 종류의 가중치 페널티를 도입하는 것이 포함됩니다. 무제한 계산이 가능한 경우, 고정 크기의 모델을 "정규화"하는 가장 좋은 방법은 각 매개변수 설정의 사후 확률에 따라 가중치를 적용하여 모든 가능한 설정의 예측을 평균화하는 것입니다. 이것은 간단하거나 작은 모델에 대해서는 종종 꽤 잘 근사될 수 있지만, 더 적은 계산을 사용하여 베이지안 골드 표준의 성능에 다가가고 싶습니다. 우리는 매개변수를 공유하는 학습된 모델의 지수적인 수의 예측을 근사하는 동일한 가중치의 기하평균을 근사함으로써 이를 제안합니다. 모델 결합은 거의 항상 기계 학습 방법의 성능을 향상시킵니다. 그러나 대규모 신경망의 경우, 여러 개별적으로 훈련된 네트워크의 출력을 평균화하는 명백한 아이디어는 막대한 비용이 발생합니다. 여러 모델을 결합하는 것은 개별 모델이 서로 다를 때 가장 도움이 됩니다. 신경망 모델을 다르게 만들기 위해서는 다양한 아키텍처를 가져야 하거나 서로 다른 데이터를 기반으로 훈련되어야 합니다. 다양한 아키텍처를 훈련하는 것은 각 아키텍처에 대한 최적의 하이퍼파라미터를 찾는 것이 어려운 작업이고 각 대규모 네트워크를 훈련하는 데는 많은 계산이 필요합니다. 또한, 대규모 네트워크는 보통 많은 양의 훈련 데이터가 필요하며 서로 다른 데이터의 하위 집합에 대해 서로 다른 네트워크를 훈련시키는 데 충분한 데이터가 없을 수 있습니다. 다양한 대규모 네트워크를 훈련하는 것이 가능하더라도 테스트 시에 이를 모두 사용하는 것은 중요한 응용 프로그램에서 빠르게 응답하는 것이 중요한 경우에는 불가능합니다. 드롭아웃은 이러한 두 가지 문제를 모두 해결하는 기술입니다. 이는 과적합을 방지하고 지수적으로 다양한 신경망 아키텍처를 효율적으로 근사적으로 결합하는 방법을 제공합니다. "드롭아웃"이라는 용어는 신경망에서 단위(은닉 및 가시)를 삭제하는 것을 의미합니다. 단위를 삭제한다는 것은 네트워크에서 임시로 해당 단위와 그 모든 들어오고 나가는 연결을 제거하는 것을 의미합니다. 어떤 단위를 삭제할지 선택하는 것은 무작위입니다. 가장 간단한 경우에는 각 단위가 다른 단위와 독립적으로 고정 확률 p로 보존됩니다. 여기서 p는 검증 세트를 사용하여 선택할 수 있거나 간단히 0.5로 설정할 수 있습니다. 그러나 입력 단위의 경우 일반적으로 보존 확률이 1에 더 가까울 때 최적입니다. 신경망에 드롭아웃을 적용하는 것은 해당 신경망에서 "얇은" 네트워크를 샘플링하는 것과 같습니다. 얇은 네트워크는 드롭아웃을 생존한 모든 단위로 구성됩니다. n 단위의 신경망은 2n개의 가능한 얇은 신경망 모음으로 볼 수 있습니다. 이러한 네트워크들은 모두 가중치를 공유하기 때문에 총 매개변수 수는 여전히 O(n2) 또는 이하입니다. 각 훈련 사례의 각 제시에 대해 새로운 얇은 네트워크가 샘플링되고 훈련됩니다. 따라서 드롭아웃이 적용된 신경망을 훈련하는 것은 매우 드물게 또는 전혀 훈련되지 않은 상태에서 매우 많은 수의 얇은 네트워크 모음을 훈련하는 것으로 볼 수 있습니다. 테스트 시에는 지수적으로 많은 얇은 모델의 예측을 명시적으로 평균화하는 것은 불가능합니다. 그러나 매우 간단한 근사 평균화 방법이 실제로 잘 작동합니다. 아이디어는 드롭아웃 없이 단일 신경망을 사용하는 것입니다. 이 네트워크의 가중치는 훈련된 가중치의 축소된 버전입니다. 훈련 중에 단위가 확률 p로 보존된다면, 해당 단위의 발산 가중치는 테스트 시에 p로 곱해집니다. 이를 통해 각 은닉 단위의 기대 출력(훈련 시 단위를 삭제하는 데 사용된 분포 하에서)이 테스트 시 실제 출력과 동일하게 유지됩니다. 이 스케일링을 통해 가중치를 공유하는 2n개의 네트워크를 테스트 시에 단일 신경망으로 결합할 수 있습니다. 드롭아웃을 통한 네트워크 훈련 및 이 근사 평균화 방법을 테스트하는 것은 다양한 분류 문제에 대해 다른 정규화 방법으로 훈련하는 것보다 일반화 오차가 상당히 낮아짐을 발견했습니다. 드롭아웃의 아이디어는 피드포워드 신경망에만 국한되지 않습니다. 이는 볼츠만 머신과 같은 그래픽 모델에 보다 일반적으로 적용될 수 있습니다. 이 논문에서는 드롭아웃을 적용한 제한 볼츠만 머신 모델을 소개하고 표준 제한 볼츠만 머신과 비교합니다. 실험 결과, 드롭아웃 제한 볼츠만 머신이 특정 측면에서 표준 제한 볼츠만 머신보다 우수함을 보여줍니다. 이 논문은 다음과 같이 구성되어 있습니다. 섹션 2에서는이 아이디어에 대한 동기를 설명합니다. 섹션 3에서는 관련 이전 연구를 설명합니다. 섹션 4에서는 드롭아웃 모델을 형식적으로 설명합니다. 섹션 5에서는 드롭아웃 네트워크를 훈련하는 알고리즘을 제시합니다. 섹션 6에서는 다양한 도메인의 문제에 드롭아웃을 적용하고 다른 형태의 정규화 및 모델 결합과 비교합니다. 섹션 7에서는 신경망의 다양한 특성에 대한 드롭아웃의 영향을 분석하고 드롭아웃이 네트워크의 하이퍼파라미터와 상호 작용하는 방법을 설명합니다. 섹션 8에서는 드롭아웃 제한 볼츠만 머신 모델을 설명합니다. 섹션 9에서는 드롭아웃을 마주하고 있는 아이디어를 탐색합니다. 부록 A에서는 드롭아웃 네트워크를 훈련하는 실제 가이드를 제시합니다. 이는 드롭아웃 네트워크를 훈련할 때 하이퍼파라미터를 선택하는 데 관련된 실제 고려 사항에 대해 자세히 분석합니다.







```python

class Dropout:
  def __init__(self, dropout_ratio = .5):
    self.dropout_ratio = dropout_ratio
    self.mask = None

  def forward(self,input_data, is_train=True):
    if is_train:
      self.mask = np.random.rand(*input_data.shape) > self.dropout_ratio
      return input_data * self.mask

    else:
      return input_data * (1.0 - self.dropout_ratio)

  def backward(self,dout):
    return dout * self.mask
```




["Data augmentation instead of explicit regularization", 2020](https://arxiv.org/pdf/1806.03852.pdf)

[Introduction to RKHS, and some simple kernel algorithms](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/lecture4_introToRKHS.pdf)

[Y. Yao at el, "ON EARLY STOPPING IN GRADIENT DESCENT LEARNING",  2007](https://yao-lab.github.io/publications/earlystop.pdf)