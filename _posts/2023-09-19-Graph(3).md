---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)


## Node Embeddings

Graph representation learning
그래프 표현 학습은 매번 feature engineering을 하는 것을 완화$$_{alleviates}$$한다.

그래프에서 특정 작업에 의존하지 않고$$_{task-independent}$$ 효과적으로 데이터의 특징을 학습하고 ML을 수행하는 것을 목표로합니다.

_Figure 3- 1_


왜 embedding을 사용하는가?
node들을 embedding space에서 투영$$_{projection}$$하는 것이 목표이다.

노드들 사이의 임베딩의 유사도는 네트워크에서 그들이 유사도로 나타낸다.
Encode network information
많은 downstream prediction에서 잠재적으로 사용될 수 있다. 노드 분류, 링크 예측, 그래프 분류, 이상치$$_{Anomalous}$$ 노드 탐지, 군집화 등등

_Figure 3-2_

Encoder and Decoder

그래프 G는 인접행렬 A(이진값)와 V(꼭지점$$_{vertex}$$ 집합)를 가지고 있다고 가정하자.
간단하게, 노드에 대한 추가적인 정보나 특징이 사용되지 않는다.

Embedding nodes
노드를 임베딩 space에 있는 유사도와 graph에있는 유사도가 근사하게 encode하는 것이 목표이다.

_Figure 3-3_

$$similarity(u,v) \approx z_v^Tz_u$$
$$similarity(\cdot)$$가 $$ENC(u)$$이며 정의할 필요가 있다.

노드에서 임베딩하는 Encoder map
노드 유사도 함수를 정의
임베딩에서 유사도 score로 매핑하는 Decoder DEC
encoder를 아래와 깉이 최적화
$$similarity(u,v) \approx z_v^Tz_u$$

두개의 주요한 요소
Encoder : 각 노드를 저차원 벡터$$z_v$$로 그려야한다.

$$ENC(v)=z_v$$

similarity function은 벡터 공간의 관계가 original 망으로 mapping되는지 명시한다.
$$similarity(u,v) \approx z_v^Tz_u$$
$$z_v^Tz_u$$ : 두 노드 임베딩을 dot product

가장 간단한 encoding 접근법은 embedding-looup으로
$$ENC(v)=z_v=Z\cdot v$$이다.

$$Z \in \mathbb{R}^{d\times|V|}$$, 각 컬럼이 node embedding인 행렬(학습하고 최적해야할)이다.
$$v \in \mathbb{I}^{|V|}$$, indicator vector로 하나만 뺴고 모두 1인 벡터 노드이다.

"Shallow" Encoding

간단한 방법으로 encoder가 embedding-lookup하는 것이다.

_Figure 3-4_

각 노드를 unique한 임베딩 벡터에 배정한다.
DeepWalk, node2vec

Framework summary
Encoder + Decoder Framework
shallow encoder : embedding looup
파라미터 최적화 : Z
Decoder : 노드 유사도에 기반
Objective : 각 노드 쌍의 유사도인 $$z_v^Tz_u$$를 최대화

주된 keypoint는 노드 유사도를 어떻게 정의하는 가 이다.
두 노드가 유사하게 임베딩되어있으면, 두 노드는 연결되었는가? 이웃을 공유하는가? 구조적인 역할이 유사한가? 등
random walk를 통해 노드 유사도를 정의하고 이러한 유사도 측정을 하는 임베딩을 최적화할 것인지 배울것이다.


비지도,자기학습 방법으로 노드 임베딩을 학습한다.
node label, feature를 활용하지 않는다.

목표는 network의 구조(DEC를 통해 수집된) 측면의 노드들을  조직화(embedding)한 집합을 직접적으로 추정하는 것이다.

이러한 임베딩은 특정 작업에 독립적으로 작용해 어떤 작업에도 사용이 가능하다.


## Random Walk Approaches for Node Embeddings

Vector $$z_u$$ 노드 u의 임베딩 벡터
Probability $$P(v|z_u)$$: $$z_u$$에 기반하여 예측을 진행한다.
노드 u에서 시작하여 random walk를 통해 노드 v를 방문할 (예측된)확률

비선형 함수를 사용하여 확률을 추정한다.
Softmax function : K 크기의 vector를 K개의 확률값으로 변환
Sigmoid function : (0,1)의 값으로 변환한다.

Random Walk
_Figure 3-5_
그래프와 시작지점이 주어졌을때 이웃을 랜덤으로 선택하여 해당 이웃으로 랜덤하게 이동한다 그리고나서 해당지점에서 다음 이웃으로 랜덤하게 이동한다. 이러한 방법으로 랜덤하게 방문된 sequence순차열이 그래프에서의 random walk이다.


$$
z_uz_v^T \approx 
\begin{matrix} \text{graph의 u,v 에서} \\
\text{rankdom walk가 함께 나타날 확률}
\end{matrix}
$$

<!-- align, equation, matrix, array, theorem, proof -->


<!-- $$
\begin{align}
z_uz_v^T \approx & \text{graph의 u,v 에서 rankdom walk가} \\
&\text{함께 나타날 확률}
\end{align}
$$ -->



<!-- $$
\begin{align*}
\text{수식의 왼쪽 항} &= \text{오른쪽 항의 첫 번째 줄} \\
&= \text{오른쪽 항의 두 번째 줄}
\end{align*}
$$ 
\partial
$$\max\limits_{f}$$
-->

Random walk embedding
1. 노드 u에서 출발하여 Random walk 전력 R을 사용하여 노드 v를 방문할 확률을 추정한다.$$P_R(v|u)$$
2. 이 random walk statistics를 encode한 임베딩을 최적화한다. $$ z_iz_j^T \approx cos(\theta) \propto P_R(v|u)$$
    - 임베딩 공간에서의 유사도(내적을 cos($$\theta$$) 가정)를 random walk "유사도"로 encode한다.

왜 랜덤워크를 쓰는가?
1. 표현력 : local과 higher-order된 이웃의 정보를 포함하는 노드 유사도를 유연한 확률적 정의가 가능하다.
2. 효율성 : 학습할때 모든 노드의 쌍을 고려할 필요가 없다; random walk를 통해 동일하게 발생한 쌍만 고려하면 된다.

비지도 표현학습
intuition
유사도를 유지하는 d 차원 공간에 있는 노드의 임베딩을 찾는다.
아이디어는 다른 노드 근처에 있는 노드를 네트워크에서도 서로 가까워지도록 하는 임베딩을 학습하는 것입니다.
노드 u가 주어졌을때, 근처에있는 노드는 어떻게 정의하는가?
$$N_R(u)$$는 Random walk 전략 R을 사용하여 얻어진 u의 이웃을 나타낸다.

그래프 $$G=(V,E)$$가 주어졌을때, 우리의 목표는 $$f:u \rightarrow \mathbb{R}^d$$로 매핑하는 $$f(u)=z_u$$를 학습하는 것이다.

Log-likelihood objective로

$$\max\limits_{f}\sum_{u \in V}logP(N_R(u)|z_u)$$

$$N_R(u)$$는 R 전략을 사용하여 노드 u의 이웃이다.
노드 u가 주어졌을때 random walk $$N_R(u)$$ 이웃의 노드를 예측하는 특징 표현을 학습하는 것이다.

Randomwalk optimization
1. 랜덤 워크 전략 R을 사용하여 각 노드 u에서 시작하는 짧은 고정 길이의 무작위 경로를 생성한다.
2. 각 노드 u는 $$N_R(u)$$를 수집하고, u에서부터 무작위 경로를 사용하여 방문한 node들의 multiset를 구한다.
   - multiset :  $$N_R(u)$$ 노드가 여러번 랜덤워크를 실행했기 떄문에 반복적인 요소$$_{elements}$$를 가질 수 있다.
3. 임베딩을 최적화한다
   - node u가 주어졌을떄 이웃 $$N_R(u)$$을 예측한다.
   - $$\max\limits_{f}\sum_{u \in V}logP(N_R(u)|z_u) \Rightarrow \text{Maximum likelihood objective}$$
   - $$\mathcal{L}=\sum_{u \in V}\sum_{v \in N_R(u)}-log(P(v|z_u))$$
     - $$z_u$$를 동시발생한 무작위 보행의 가능도확률을 최대화하는 임베딩을 최적화한다. 
     - $$P(v|z_u)$$를 softmax를 사용하여 Parameterize한다.
       - $$P(v|z_u)=\frac{exp(z_u^Tz_v)}{\sum_{n \in V}exp(z_u^Tz_v)}$$
       - 왜 소프트 맥스를 쓰는가? 우리는 모든 노드들 중에서 노드 u와 가장 비슷한 노드 v를 얻기 위해. 직관적으로 $$\sum_i exp(x_i) \approx \max\limits_{i} exp(x_i)$$

Simplest idea: Just run fixed-length, unbiased 
random walks starting from each node (i.e., 
[Perozzi et al. 2014. DeepWalk: Online Learning of Social Representations. KDD](https://arxiv.org/abs/1403.6652))
▪ The issue is that such notion of similarity is too constrained


Random walk optimization

_Figure 3-6_

무작위 보행 임베딩을 최적화하는 것은 L을 최소화하는 임베딩 $$z_u$$를 찾는것과 같다.

그러나 순진하게 위와 같이 실행하기에는 너무 비싼 비용이 든다.

중첩된 $$\sum_{n \in V}$$은 $$O(|V|^2)$$의 복잡도를 준다.

따라서 softmax안에 있는 정규화 항이 범인이다 우리는 근사로 바꿀 수 있을까?

Negative sampling으로 우리는 해결할 수 있다.

$$log(P(v|z_u)) = log(\frac{exp(z_u^Tz_v)}{\sum_{n \in V}exp(z_u^Tz_v)}) \approx log(\sigma(z^T_uz_v)) - \sum^k_{i=1}log(\sigma(z^T_uz_v)), n_i~P_V$$

sigmoid function 각 항의 확률을 0과 1사이의 값으로 변환해줌.
random distribution over nodes $$n_i~P_V$$

왜 근사 방법이 유효한가?
기술적으로 이것은 다른 목적 함수이다 그러나 Negative sampling은 Noise Contrastive Estimation(NCE)의 형태로 softmax의 log 확률로 근사된다.
새로운 공식은 $$P_{v\cdot}$$분포로 부터 샘플 추출된 $$n_i$$ 노드들로 부터 로지스틱 회귀인 sigmoid 함수를 사용해서 목표 노드 v를 구별하는 방식으로 상호작용한다 
More at https://arxiv.org/pdf/1402.3722.pdf

모든 노드를 정규화하는 것 대신에 k번 랜덤 "negative samples"인 $$n_i$$를 정규화하는것으로 대체할 수 있다.
Negative sampling은 빠른 가능도 계산을 허락한다.

k개의 negative 노드 $$n_i$$ sample은 각 노드를 선택할 확률을 가지고 있고, degree에 비례$$_{proportional}$$한다.

k에 관해서 두가직 고려사항이 있다.
1. robust estimates보다 더 높은 k를 지정해야한다.
2. 경험적으로 부정적인 결과가 나오는 5-20 보다 높은 k를 지정해야한다.

이제 목적 함수를 얻은 이후 어떻게 최적화(값을 최소화)해야 할까?

Gradient Descent가 간단한 방법으로 $$\mathcal{L}$$을 최소화할 수 있다.
$$z_u$$를 모든 노드 u의 값을 무작위로 지정하여 초기화한다.
수렴할때까지 아래를 반복한다.
   - 모든 u에 대해서 derivative $$\frac{\partial\mathcal{L}}{\partial z_u}$$를 계산한다
   - 모든 u에 대해서 $$z_u \leftarrow z_u - \eta\frac{\partial\mathcal{L}}{\partial z_u}$$로 미분값의 반대 방향으로 업데이트한다.

Stochastic Gradient Descent
모든 샘플에 대한 경사를 평가하는 것 대신에 개별적인 훈련 샘플을 평가한다.

$$z_u$$를 모든 노드 u에 대해 무작위 값으로 초기화한다.

$$\mathcal{L}=\sum_{v \in N_R(u)}-log(P(v|z_u))$$가 수렴할때까지 반복한다.
- 샘플 노드 u에 대해 모든 v의 미분값 $$\frac{\partial\mathcal{L}^{(u)}}{\partial z_v}$$를 계산한다.
- 모든 v에 대해서 $$z_v \leftarrow z_v - \eta\frac{\partial\mathcal{L}^{(u)}}{\partial z_v}$$로 미분값의 반대 방향으로 업데이트한다.

Node2vec
노드 임베딩을 사용하여 유사한 노드를 특성 공간에서 가깝게 표현하는 것이 목표입니다.
해당 목표를 최대 가능도 최적 문제로 생각하고 downsteram prediction 작업과 독립적으로 생각합니다.

주된 observation은 
노드 u의 이웃 네트워크 $$N_R(u)$$가 부유한 노드 임베딩으로 이끌것이라는 유연한 개념이다.
2nd order 무작위 보행 R을 통해 만들어진 u의 이웃 네트워크 $$N_R(u)$$를 발전시킨다.
Reference: [Grover et al. 2016. node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf). KDD.

node2vec: biased walks
아이디어는 네트워크의 local과 global view간의 tarde off가 되는 편향된 무작위 보행을 유연하게 사용하는 것이다.
[Grover and Leskovec, 2016](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf)


노드 u의 이웃들 $$N_R(u)$$를 정하는 두개의 전략을 정의한다.
$$N_{BFS}(u)$$, local microsocopic view 주변만 도니까
$$N_{DFS}(u)$$, global microscopic view 멀리까지 나아가니까

interpolction BFS, DFS
편향된 고정된 길이의 무작위 보행 R은 노드 u로 부터 이웃들 $$N_R(u)$$를 생성한다
두개의 파라미터가 필요
- p : return parameter, 이전 노드에서 다시 가져오는 노드
- q : in-out parameter, outward(DFS) vs inward(BFS)로 q는 "ratio"이다.

편향된 2순위 무작위 보행은 이웃들의 네트워크를 탐험한다.
Rnd. 보행자는 edge($$s_1$$, w)를 가로지르$$_{traversed}$$고, 현재 w에 있다.

w의 이웃들은 아래와 같다.
_Figure 3-7_

주된 아이디어는 어디서 걸어왔는지를 기억하는 것이다.
보행자가 edge($$s_1$$, w)를 가로지르고 w에 있으면 다음에는 어디로 갈 것인가?

p,q는 unnormalized된 모델의 transition 확률들이다.
왔던곳으로 다시 돌아갈 확률 return parameter p
다른곳으로 걸어갈 확률을 "walk away" parameter라고 한다.
Figuure 3-7에서 s3로 갈 확률은 1/q이고, s1은 1/p, s2는 1이다.

따라서 는 

$$
w = 
\begin{matrix}
s1 \\
s2 \\
s3 
\end{matrix}
=
\begin{matrix}
1/p \\
1 \\
1/q 
\end{matrix}
$$

따라서 
$$N_{R}(u)$$는 편향된 보행을 통해 방문한 노드들이다.

node2vec algorithm
1. 무작위 보행 확률을 계싼한다
2. 노드 u에서 시작하는 l 길이의 무작위 보행을 r번 시뮬레이션한다
3. node2vec 목적함수를 확률적경사하강법을 통해 최적화한다.

시간복잡도는 선형적이고 이 세 단계는 개별적으로 병렬화가능하다.

다양한 랜덤워크가 있다.
 Different kinds of biased random walks:
▪ [Based on node attributes (Dong et al., 2017).](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf)
▪ [Based on learned weights (Abu-El-Haija et al., 2017)](https://arxiv.org/abs/1710.09599)
 Alternative optimization schemes:
▪ Directly optimize based on 1-hop and 2-hop random walk 
probabilities (as in [LINE from Tang et al. 2015](https://arxiv.org/abs/1503.03578)).
 Network preprocessing techniques:
▪ Run random walks on modified versions of the original 
network (e.g., [Ribeiro et al. 2017’s struct2vec](https://arxiv.org/pdf/1704.03165.pdf), [Chen et al. 2016’s HARP](https://arxiv.org/abs/1706.07845)).

original network에서 노드들간의 유사도를 반영하는 임베딩 스페이스에 노드를 임베딩하는 것이 주된 아이디어이다.
노드 유사도에 관한 다양한 개념
- 나이브 : 두 노드가 연결되어있으면 유사하다
- 이웃들 overlap : 이웃들이 겹치면 유사하다
- 무작위 보행 접근법

어떤 방법을 사용해야할까?
모든 경우에 성공적인 하나의 방법은 없다.
E.g., node2vec performs better on node classification 
while alternative methods perform better on link prediction ([Goyal and Ferrara, 2017 survey](https://arxiv.org/abs/1705.02801))

그러나 무작위 보행이 대체적으로 더 효과적이다.
일반적으로 적용하고자 하는 것에 유사도를 맞춰서 선택해야한다.


## Embedding Entire Graphs

목표는 부분그래프나 전체 그래프 G를 $$z_G$$로임베딩하는것이다.

_Figure 3-8_

독성이나 무독성의 원자$$_{molecules}$$들을 구별하거나 변칙적인 그래프를 식별하는데 사용한다.

Approach 1
간단하지만 효과적인 접근법

$$z_G = \sum_{v \in G}z_v$$

(sub)graph G들의 노드 임베딩을 모두 더하거나 평균하는 방법.

[Duvenaud et al., 2016](https://arxiv.org/abs/1509.09292)에서 그들의 그래프 구조에 기반한 원자들$$_{molecules}$$을 분류하는데 사용됨.

Approach 2

"virtual node"로 (sub)graph를 표현하는데 사용하고, 정규 그래프 임베딩 기법을 사용함.

(sub)graph 에 걸쳐있는 가상의 super-node를 만들고 그 노드를 임베딩한다.

[Li et al., 2016](https://arxiv.org/abs/1511.05493)에서 부분 그래프 임베딩을 하는 일반적인 기술로 제안하였다

![3-9](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/2b785991-d3f4-4f61-82cd-221003bbcf7c)
_Figure 3-9_






Approach 3
Anonymous Walk Embeddings
무작위 보행으로 우리가 처음 방문한 노드의 인덱스들의 상태를 익명 보행이라고 한다.

[Anonymous Walk Embeddings, ICML 2018](https://arxiv.org/pdf/1805.11921.pdf) 

이러한 익명 보행은 지수적으로 증가한다.


따라서 쉽게 사용하기 ㅟ해서는 l번째 익명 보행 $$w_i$$을 시뮬레이션하고 그들의 count를 기록하여 그들의 보행을 확률 분포의 그래프로 표현해야한다.
$$Z_G[i]$$는 G에서 익명 보행 $$w_i$$의 확률이다.

이러한 sampling  익명 보행은 독립적인 무작위 보행 m의 집합을 독립적으로 생성한다.
얼마나 많은 무작위 보행 m이 필요한가?

_Figure 3-10_
\epsilon 보다 적은 확률의 오차와 \delta보다 작은 분포를 원한다.

새로운 아이디어로 Walk embeddingd을 학습한다
각 보행을 몇번 발생했는지 빈도를 간단하게 표현하는 방법보다 우리는 익명 보행 w_i의 임베딩 z_i를 학습하는게 더 좋다
익명 보행의 임베딩 z_i의 집합 Z_G를 학습한다.
보행을 어떻게 임베딩 하는가? 보행을 다음 보행이 예측되도록 임베딩한다.

노드 1에서 시작한 익명 보행의 샘플을 $$\Delta$$-size window로 상호 발생한 예측 보행을 학습한다.
$$
$$max\sum^{T-\Delta}_{t=\Delta}logP(w_t|w_{t-\Delta}, ..., w_{t+\Delta}, z_G)$$

그래프의 모든 노드의 목적함수를 합한다.


u에서 l 길이의 무작위 보행 T개를 얻게된다.

$$N_R(u) = \{w_1^u,...,w_T^u,\}$$

$$\Delta$$ 크기의 window에서 상호 발생한 보행을 예측한다.

_Figure 3-12_

우리는 최적화한 그래프 임베딩을 얻었다/
이러한 임베딩으로 예측을 진행하는데 사용한다.
내적곱 kernel $$z_{G_1}^tz_{G_2}$$
신경망을 사용하여 해당 ㅇ님베딩을 분류하는데 사용한다.

_Figure 3-11_



그래프 임베딩의 조금 더 진화된 버전은 8강의에서 배울것이다. 그래프에서 hierarchically 노드를 군집화할 수 있다.그리고 해당 군집에 관하여 노드 임베딩을 합하거나 평균할 수 있다.

노드 $$z_i$$ 임베딩을 어떻게 사용하는가?
군집, 커뮤니티 탐지 : CLuter points $$z_i$$
노드 분류 : $$z_i$$의 기반한 노드 i의 label을 예측
연결 예측 : $$(z_i, z_j)$$ 에 기반한 edge (i,j)를 예측
    - 임베딩 간에 여러 계산방법이 있음
    - concatenate  : $$f(z_i, z_j)=g([z_i, z_j])$$
    - hadamard  : $$f(z_i, z_j)=g(z_i * z_j)$$
    - sum/avg  : $$f(z_i, z_j)=g(z_i+ z_j)$$
    - distance  : $$f(z_i, z_j)=g(||z_i - z_j||_2)$$
그래프 분류 : 노드 임베딩이나 가상의 노드들을 합$$_{aggregate}$$하여 graph 임베딩 $$z_G$$를 얻어서  해당 임베딩을 기반으로 label을 예측할 수 있다.

ENcoder-decoder framework
Encoder : lookup 임베딩
Decoder : 노드 유사도가 맞는 임베딩이 기반하여 점수를 예측
Node similairt measure (편향된) 무작위 보행
그래프 임베딩의 Extension 노드 임베딩의 종합과 익명 보행 임베딩.

<br><br>
---