---
title: 'Lasso and Lidge'
author: east
date: 2023-07-17 12:38:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

<!-- https://chirpy.cotes.page/ -->

ⅠⅡⅢⅣⅤⅥⅦⅧⅨⅩⅪⅫ
ⅰⅱⅲⅳⅴⅵⅶⅷⅸⅹⅺⅻ
⒈⒉⒊⒋⒌⒍⒎⒏⒐⒑⒒⒓
⑴⑵⑶⑷⑸⑹⑺⑻⑼⑽⑾⑿
머신러닝에 사용되는 정규화 기법은 주로 모델의 복잡도를 줄이고 과적합(overfitting)을 방지하여 일반화 성능을 향상시키기 위한 목적으로 쓰입니다.

asd
L1 정규화 (Lasso 정규화) - L1 정규화는 비용 함수에 가중치의 절대값 합에 정규화 계수를 곱한 패널티 항을 추가합니다. 이로 인해 일부 가중치가 정확히 0이 되어 변수 선택에 도움이 됩니다.
L2 정규화 (Ridge 정규화) - L2 정규화는 비용 함수에 가중치의 제곱 합에 정규화 계수를 곱한 패널티 항을 추가합니다. 이 기법은 가중치를 작게 만들어 과적합을 방지하며, L1 정규화와 달리 가중치를 완전히 0으로 만들지는 않습니다.
Elastic Net 정규화 - Elastic Net 정규화는 L1 정규화와 L2 정규화를 모두 사용하는 기법입니다. 이 방법은 두 정규화 기법의 장점을 모두 활용하여 변수 선택과 과적합 방지를 동시에 달성합니다.
Dropout - 딥러닝에서 사용되는 정규화 기법으로, 학습 과정에서 무작위로 일부 뉴런을 비활성화시키는 방법입니다. 이를 통해 모델의 복잡성을 줄이고, 과적합을 방지할 수 있습니다.
Early Stopping - 머신러닝 학습 중에 검증 데이터 세트의 성능이 더 이상 개선되지 않거나 감소하는 시점에서 학습을 멈추는 기법입니다. 이 방식을 통해 최적의 모델을 찾을 수 있으며 과적합을 피할 수 있습니다.
Batch Normalization - 딥러닝에서 사용되는 기법으로, 학습 과정에서 각 계층의 입력 분포를 정규화하여 학습 속도를 높이고, 과적합을 방지하는 역할을 합니다.
Weight Decay - 가중치 감쇠라고도 하며, 학습 과정에서 가중치가 너무 커지지 않도록 감쇠시키는 기법입니다. 이 방법은 L2 정규화와 유사한 효과를 얻을 수 있습니다.
위 정규화 기법들은 다양한 머신러닝 알고리즘과 문제에 사용되며, 적절한 정규화 방법과 하이퍼파라미터의 선택이 중요한 요소입니다. 이를 통해 모델의 일반화 성능을 개선하고, 과적합 문제를 완화시킬 수 있습니다.


> ## Lasso

데이터 전처리 과정에서 정규화는 특성의 스케일을 조절.

모델 학습 과정에서의 정규화는 과적합을 방지하고 일반화 성능을 향상시키려는 목적. 비용 함수에 패널티 항을 추가함으로써 가중치의 값을 줄이고, 모델의 복잡성을 제한함.


L1 정규화 Lasso regularization. 가중치의 절대값 합을 추가한 패널티함. 람다를 곱해 정규화 강도를 조절.
L2 정규화 Lidge regularization. 비용함수에 가중치의 제곱합을 추가. 람다를 곱해 정규화 강도를 조절.
Elastic net 정규화 L1과 L2정규화를 결합한 방식으로 L1과 L2 패널티항 동시에 추가하고 하이퍼 파라미터로 조절.


> ## Lidge


[Hoerl, A.E. and Kennard, R.W., 1970. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1), pp.55-67.](http://homepages.math.uic.edu/~lreyzin/papers/ridge.pdf)
Ridge 회귀에 대한 핵심 논문으로, 그 기법의 발명과 함께 이론적 배경을 소개하였습니다.

Nonorthogonal(비직교) 문제의 편향 추정

abstract
다항회귀에서 최소 잔차 합의 제곱에 기반한 파라미터 추정은 높은 확률로 불만족스럽다. 만약 그렇지 않다면 예측 벡터들은 직교하지 않는다. $$X'X$$ 행렬의 대각선에 작은 양수를 더하는 추정 방법을 제안합니다. Ridge trace로 해당 방법은 비직각인 두 차원의 영향을 보여줍니다. 그런다음 매개변수 $$X'X$$ 행렬이 어떻게 작은 MSE를 가지는 편향 추정을 얻는지 보여줍니다.

introduction
다항 선형회귀 $$Y=X\beta + \epsilon$$의 표준 모델에서 $$X$$는 $$p$$ rank의 $$(n \times p)$$이고, $$\beta$$는 $$(p \times 1)$$에 unknown이다. 여기서 rank는 독립적인 열의 개수를 의미한다.
unknown $\beta$를 추정하는 방법은 보통 비편항이고 최소 분산을 가지는 Gauss-Markov0linear function $Y$을 사용한다. 해당 추정방법은 $X'X$가 상관관계행렬과 같은 unit matrix라면 좋지만 그렇지 않다면 최소제곱추정은 "error"들의 수에 민감해진다. 이러한 error들의 결과는 $X\beta$가 true model에 명확해질때 치명적이다. 최소제곱추정법은 데이터를 생성하는 과정의 물리,화학, 엔지니어링 맥락에서 종종 납득이 되지 않습니다. 이러한 경우 블랙박스나  drop factor와 같은 추정된 예측 함수를 강제로 사용하게 되면 $X'X$형태의 $X_i$ 사이의 상관관계의 연결을 파괴 하게 됩니다.이런 두경우의 대안은 제어와 최적화를 위한 추정 예측자를 사용하는 의도에서 만족시킬수없습니다. 만약 블랙박스의 결과만 다루면 모델을 사용자에게 편미분을 하지말라고 경고(실제로 쓸모없는)할 것입니다. 다른 경우에는 제어가능하거나 관측가능한 것들에 달려있는 집합을 잃을것입니다. $X'X$ 보다 $X'X + kI_p, \; k \geq 0$ 기반한 추정에서 최소제곱추정과 관련된 많은 어려움을 피해가는 데 도움이된다. 트깋, 해당 절차는 데이터 집합의 부분을 추정하는 민감도로 묘사된다. 그리고 작은 MSE 값의 점추정도 얻을 수 있다.

<!-- matrix rank의 의미는 선형 변환의 차원을 의미. -->

1. Properties of best linear unbiased estimation
최소분한이나 MLE를 가지고 비편향 선형 추정은 아래와 같다.

$$\hat{\beta}=(X'X)^{-1}X'Y$$

$\beta$의 추정치가 주어졌을때, 잔차의 제곱합의 최소값은 아래와 같다.

$$\phi(\hat{\beta})=\epsilon'\epsilon=(Y-X\hat{\beta})'(Y-X\hat{\beta})$$

$\hat{\beta}$의 성질은 잘 알것이다. 여기서 염려하는 바는 $X'X$가 단위 행렬이 아닐때 조금더 특정 지어서 상관 행렬행태가 아닐때이다. $\beta$의 추정 조건의 효과를 설명하기 위해서 $\hat{\beta}$의 예측값의 분산-공분산 행렬과 거리 두 가지 성질을 고려했다.

[최소제곱추정](https://jangpiano-science.tistory.com/103)

β추정 시 OLS 최소제곱추정 방법으로 오차를 최소화하는 β를 추정해야함.
$$
오차 제곱합 S = ε'ε = (Y - Xβ)'(Y - Xβ)\\
∂S/∂β = -2X'(Y - Xβ) = 0\\
2X'(Y - Xβ) = 0 \\
X'(Y - Xβ) = 0 \\
X'Y - X'Xβ = 0 \\
X'Xβ = X'Y
$$
$$\hat{β} = (X'X)^{-1}X'Y$$


$X'X$는 공분산행렬의 일부를 표현함.
공분산 행렬의 역행렬을 곱하는 이유는 선형 회귀 추정치의 불확실성을 정량화 하기 위해서,.


$$(i) VAR(\hat{\beta}) = \sigma^2(X'X)^{-1}$$
선형 회귀 모델에 대해 다음과 같은 관계를 가집니다.
Y = Xβ + ε
최소제곱 추정치 (Ordinary Least Squares - OLS)를 사용하여 베타의 추정값을 구합니다.
$$
β_{hat} = (X'X)^(-1)X'Y\\
      = (X'X)^(-1)X'(Xβ + ε)\\
      = (X'X)^(-1)(X'X)β + (X'X)^(-1)X'ε\\
      E(ε) = 0,\\
E(β_{hat}) = E(β)\\
$$

베타의 참값과 추정값 사이의 차이를 나타내는 항을 구합니다.
$$
β_{hat} = (X'X)^(-1)X'(Xβ + ε)\\
β - β_{hat} = β - (X'X)^(-1)X'(Xβ + ε)\\
          = β - (X'X)^(-1)(X'X)β + (X'X)^(-1)X'ε\\
          = β - β + (X'X)^(-1)X'ε\\
          = (X'X)^(-1)X'ε
$$

Var(β_{hat}) = E[(β_{hat} - E[β_{hat}])(β_{hat} - E[β_{hat}])']

이 차이 항에 대한 공분산을 구합니다. (Cov(β_{hat}, ε) = 0임을 이용)
$$
Var(β_{hat}) = E[ (β - β_{hat})(β - β_{hat})' ]\\
β - β_{hat} 표현을 식에 대입하고 계산합니다.\\
Var(β_{hat}) = E[ (X'X)^{-1}X'ε ((X'X)^{-1}X'ε)']\\
Var(β_{hat}) = (X'X)^{-1} * σ^2
$$
독립변수 X와 오차 e 사이에는 상관관계까 없음. 외생성가정
E(e) 오차 기대값은 0 선형회귀 가정
trace 대각합
$$
L_1 \equiv Distance \; from \; \hat(\beta) \; to \; \beta \\ L_1^2 = (\hat{\beta} - \beta)'(\hat{\beta} - \beta) \\
E[L_1^2] = \sigma^2Trace(X'X)^{-1} \\
E[\hat{\beta}'\hat{\beta}] = \beta'\beta + \sigma^2Trace(X'X)^{-2}\\
$$

```
eigenvalue : Av = λv, 행렬 A가 벡터 v에 작용할때, 벡터의 방향은 유지하되 크기만 λ배 변하는 것을 의미

선형 변환의 관점에서 이해하는 것이 가장 직관적, 선형 변환이란 즉, 벡터를 다론 공간으로 옮기는 연ㅅ나으로 투영으로 생각된다.이러한 선형 변환 후에도 벡터의 방향이 바뀌지 않고 스케일만 바뀌는 경우 이런 벡터를 eigenvetor라 부르고 스케일 변경 비율을 λ라고 부름.
```


$$E[(b - \hat{β})(b - \hat{β})] = E[(b - X'a)(b - X'a)]$$ 

여기서 b_hat = X'a, a는 임의의 상수 벡터입니다. 이제 두 번째 항에 주목해주세요

$$(b - \hat{β})(b - \hat{β}) \\= (b - X'a)(b - X'a) \\ = (X'a - b)(X'a - b) \\ = (aX' - b')(aX' - b) \\ = aX'X'a - 2aX'b + b'b \\ = aX'X'a - 2a(X'b) + b'b $$

$$ Trace(X) = \sum_iX_{ii} \\ = \sum_i\lambda_i, \; \lambda_i=eigen(X)$$

$$aX'X'a - 2a(X'b) + bb \\ = aX'(X'X)^{-1}X'Xa - 2aX'b + bb \\ = 2Trace(X'(X'X)^{-1}X')a $$

$$X′Xb = X′y \\ => (X′X)⁻¹X′Xb = (X′X)⁻¹X′y \\=> b = (X′X)⁻¹X′y \\ a′X′(X′X)⁻¹X′Xa - 2a′X′b + b′b$$




따라서 원래 식인 E[L^2] = 2 Trace (X'X)^-1 와 동치임을 알 수 있습니다. 이제 질문에 대한 원래 답변에 와서, 트레이스(Trace) 함수는 여기에 등장하는 이유는 기존 식에서 2차 형식(즉, a'X'Xa와 같은 형태)가 포함되어 있는데, 이 형식의 기댓값을 구하는 중간 단계에서 보통 행렬의 트레이스를 사용합니다. 행렬의 트레이스는 대각선 항목의 합을 의미하며, 최종적으로 실제 모수 값과 추정치 간의 거리를 구하는 과정에서 중요한 역할을 합니다. 이 과정을 통해 필요한 통계량을 추정할 수 있는데, 당신이 질문한 식에서 트레이스 함수는 이러한 과정에서 유용하게 활용됩니다.



$$Var(\hat{β}) \\= E[(\hat{β} - E[\hat{β}])(\hat{β} - E[\hat{β}])']\\= E[((X'X)^{-1}X'(y - Xβ))(X'(y - Xβ))'] \\ = E[((X'X)^{-1}X'e)(X'e)'] \\ = (X'X)^{-1}X'E[(ee')]X(X'X)^{-1} \\ E[ee'] = σ^2I이므로, \\ Var(\hat{β}) = σ^2(X'X)^{-1}X'IX(X'X)^{-1} \\ = σ^2(X'X)^{-1} \\따라서, E[L_1^2] = σ^2Trace(X'X)^{-1}이 성립$$

이 회귀 모형에서 오차항 e의 기댓값은 0이고, 분산은 ^2I라고 가정했습니다.


Error는 정규 분포를 따르기 때문에 $$VAR[L_1^2] = 2\sigma^2Trace(X'X)^{-2}$$

해당 특성은 $\hat{\beta}$의 불확실성을 보여준다. $X'X$가 unit matrix에서 조건수$$_{ill-conditioned matrix}, ||A||$$의 하나로 움직일때. 만약 $X'X$의 고유값이 
$$\lambda_{max} = \lambda_1 \geq lambda_2 \geq  \ldots lambda_p = lambda_{min} > 0$$으로 나타날떄, $\hat{\beta}$와 $\beta$ 사이 거리 제곱의 평균 값은
$$E[L_1^2]=\sigma^2\sum_{i=1}^p(1/\lambda_i)$$로 주어진다.
그리고 $$VAR[L_1^2] = 2\sigma^4\sum(1/\lambda_i)^2$$로 정규분포를 따르는 오차이다.

해당 평균과 분산의 하한은 각각 $\sigma^2/\lambda_{min}$과 $2\sigma^4/\lambda_{min}^2$이다. 따라서 요인factor 공간의 shape는 $X'X$에서 하나 이상의 작은 고유값이 작을수록 해당 distance는 커진다. 
절대값이 큰 추정 계수에서 비직교 데이터에서 존재하는 문제에 관측되었습니다.
최소제곱추정은 점추정 시 수학적이고 최적의 기술의 부족함에 고통받아왔습니다. \
추정 절차는 최적화 기준에 대한 민감도를 묘사하는 방법이 없습니다.

2. ridge regression

A. E. Hoerl은 1962년에 최소제곱추정과 연관한 팽창과 불안정성을 제어하는 방법을 제안했습니다. 

$$\hat{\beta}^*=[X'X + kI]^{-1}X'Y;k\geq0\\=WX'Y$$

이차응답함수란 예측변수의 이차항을 포함한 회귀모델을 뜻하고 해당 관계가 단순 선형이 아닌 하나 이상의 최솟값 혹은 최댓값을 갖는 복잡한 형태일때 유용하게 사용됩니다. 이는 곡선형태의관계, 최적화 문제, 모수 추정 등 유사한 점이있습니다.
$k \geq 0$를 만족하는 추정치들의 집합은 이차 반응 함수의 묘사와 많은 수학적인 유사성이 있다. 이러한 이유때문에 추정과 분석은 "ridge regression"으로 명시되었다. ridge 추정 관계는 다음 대체 형태로 주어집니다,

$$\hat{\beta}^*=[I_p + l(X'X)^{-1}]^{-1}\hat{\beta} \\ =Z\hat{\beta}$$

이러한 관계는 차후 섹션에서 탐구하고 

$(i) \; \xi(W)$와 $\xi(Z)$를 각각 W와 Z의 고유값으로 표현하면 

$$\xi(W)=1/(\lambda_i+k) \\ \xi(Z)= \lambda_i/(\lambda_i+k)$$

이고 ,$\lambda_i$는 $X'X$의 고유값이다. 해당 결과는 W와 Z로 정의되는 특유의 방정식인 $|W-\xi I|=0$과 $|Z-\xi I|=0$를 따른다.

$(ii) Z - I - k(X'X + kI)^{-1}=I-kW$
해당관계는 Z = (X'X + kI)-1 X'X = WX'X의 형태로 대체하면서 손쉽게 증명할수있고, 
양쪽을 왼쪽에서 W-1로 곱함으로써 쉽게 확인

세 가지 내용은 릿지 회귀(ridge regression)에서 행렬 W와 Z의 고유값과 추정치의 잔차 제곱합에 대한 관계를 설명하고 있습니다. 각 내용의 의미는 다음과 같습니다. (i) W와 Z의 고유값 관계: 이 식들은 W와 Z의 고유값이 릿지 회귀의 정규화 상수 k와 X'X 행렬의 고유값에 의해 어떻게 결정되는지를 보여줍니다. 릿지 회귀의 솔루션이 어떻게 일반 선형 회귀로부터 변화하는지에 대한 정보를 제공합니다. (ii) Z와 정규화 상수 k의 관계: 이 식은 정규화 상수 k가 Z 행렬에 어떻게 영향을 미치는지 보여줍니다. 릿지 회귀에서 k 값이 증가하면 Z가 0에 가까워집니다. 이는 릿지 회귀에서 더 큰 k 값이 회귀 계수에 더 많은 규제를 부여함을 나타냅니다. (iii) 추정치의 잔차 제곱합: 잔차 제곱합은 회귀 모델의 성능을 측정하는 중요한 지표로, 예측값과 실제값 간의 차이를 계산합니다. 이 식들은 릿지 회귀의 정규화 상수 k가 잔차 제곱합에 영향을 미치는 것을 보여줍니다. 이를 통해 릿지 회귀 모델에서 최적의 정규화 상수 k를 선택하여 과적합을 방지하고 모델 성능을 최적화하는 데 도움이 됩니다. 이 세 가지 내용들은 릿지 회귀에서 행렬 W와 Z의 고유값, 정규화 상수 k, 회귀 계수 추정 및 잔차 제곱합에 대한 관계와 속성을 이해하는 데 중요한 역할을 합니다. 이를 통해 릿지 회귀가 다중공선성이 있는 데이터셋에 대한 문제를 완화하고, 일반화 능력을 높이는 데 어떻게 기여하는지 이해할 수 있습니다.



3. The Ridge Trace

4. MSE properties of ridge regression

5. General form of ridge regression






[Tibshirani, R., 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), pp.267-288.](http://www-personal.umich.edu/~jizhu/jizhu/wuke/Tibs-JRSSB96.pdf)
이 논문은 Ridge 회귀와 관련된 L1 정규화인 Lasso 회귀를 소개합니다. Lasso 회귀는 일부 가중치를 정확히 0으로 만들어 변수 선택에 좋은 성능을 발휘하는 것이 특징입니다.

[Zou, H., and Hastie, T., 2005. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, 67(2), pp.301-320.](https://hastie.su.domains/Papers/elasticnet.pdf)
이 논문은 L1 정규화와 L2 정규화(Ridge)를 결합한 정규화 기법인 Elastic Net을 소개합니다. 이 방법은 Ridge 회귀와 Lasso 회귀의 장점을 모두 활용할 수 있습니다.

[Hastie, T., Tibshirani, R., and Friedman, J., 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Science & Business Media.](https://hastie.su.domains/Papers/ESLII.pdf)
머신러닝과 통계학의 핵심 개념과 알고리즘을 다루는 이 책에서는 Ridge 회귀와 관련된 많은 주제와 함께 Lasso 회귀, Elastic Net 회귀를 포함한 정규화 회귀 기법을 소개합니다.