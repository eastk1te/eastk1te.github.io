---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)

ML Tasks
Node-level prediction 
Node-feature $$\in \mathbb{R}^D$$
Link-level prediction
Link-feature $$\in \mathbb{R}^D$$
Graph-level prediction
Graph-feature $$\in \mathbb{R}^D$$

Traditional ML Pipeline
Train an ML model -> Apply the model


## Feature Design
효과적인 특징 $$x$$를 사용하는 것이 모델의 성능을 이끌어내는 중요한 key이다.

이전의 전통적인 ML 파이프라인은 hand-designed feature를 사용했다.

Graph에서 ML이란
object 집합에서 좋은 예측을 하는 것이 목표이다.

Design choices로
Fetures : d-dimensional vectors $$x$$
Objects : 노드, 엣지, 노드 집합, 전체 그래프
Objective function : 풀려고하는 작업에 따라

Node-level prediction에서
$$G=(V,E)$$가 주어지면, $$f:V->\mathbb{R}$$로 학습한다.
input feature와 bias and weights를 Net input function을 사용하여, 학습을 진행하면 $$p(y=1|x)$$ class membership probability.를 얻을 수 있따.

## Node-Level Tasks and Features

Goal : 구조와 그래프에서 node의 위치를 특징화$$_{Characterize}$$한다.

- Node degree
  - degree $$k_v$$는 node $$v$$의 연결된 edge의 개수이다.
  - 노드 degree는 이웃한 노드들의 중요도를 수집하는거 없이 세는게 가능하다.
- Node centrality
  - 노드 중심성 $$C_v$$는 그래프에서 노드의 중요도를 고려합니다.
  - 중요도를 고려하는 다양한 방법
    - Eigenvector centrality
      - node $$v$$가 $$u \in N(v)$$인 중요한 이웃 노드들에 둘러쌓여 있으면 $$v$$도 중요하다
      - node $$v$$의 중솜성을 아래와 같이 정의한다.
      - $$c_v = \frac{1}{\lambda}\sum_{u\in N(v)}c_u$$
      - $$\lambda$$는 normlization constant이다. A의 가장 큰 eigenvalue으로 변한다.
      - 위의 모델 중심성 방정식$$_{equation}$$은 재귀적인 방식으로 진행하는 것을 알아야합니다.
      - 그러면 어떻게 풀어야할까요? 
      - 재귀적인 방정식을 matrix form으로 다시 작성해야합니다.
      - $$\lambda c = Ac$$
      - $$A$$ : Adjacency matrix, $$A_{uv}$$=1 if $$u \in N(v)$$
      - $$c$$ : Centrality vector
      - $$\lambda$$ : Eigenvalue
      - 우리는 $$c$$를 $$A$$의 eigenvector로 볼 수 있게 된다.
      - $$\labmda_{max}$$는 항상 양수$$_{positive}$$하고 unique한다. (Perron-Frobenius Theorem)에 의해.
      - $$\lambda_{max}$$와 상호작용하는 $$c_{max}$$는 중심성으로 사용됩니다,.
    - Betweenness centrality
      - 다른 노드들 사이에서 많은 짧은 통로가 놓여있다면 해당 노드는 중요하다.
      - $$c_v=\sum_{S\neq v\neq t}\frac{\#\text{v를 포함하는 s와 t사이의 짧은 통로}}{\#(\text{s와 t 사이의 짧은 통로})}$$
      - 
    - Closeness centrality
      - 다른 모든 노드에서 가장 짧은 최단 path를 가지고 있을 수록 해당 노드는 중요하다
      - $$c_v=\frac{}{\sum_{u\neq v}\text{u와 v의 최단 거리}}$$
    - and many others...
- Clustering coefficient

노드 $$v$$와 연결된 이웃 노드들을 어떻게 연결되었는지 측정하는 방법
$$e_v = \frac{\#(\text{이웃 노드들 사이의 edge들})}{\begin{pmatrix} k_v \\ 2  \end{pmatrix}}$$
$$\begin{pmatrix} k_v \\ 2  \end{pmatrix}$$는 $$\#(k_v의 이웃 노드들사이의 노드 pairs)$$

$$A = \begin{pmatrix} 
0 & 1 & 1 & 1 & 1 \\ 
1 & 0 & 1 & 1 & 1 \\
1 & 1 & 0 & 1 & 1 \\
1 & 1 & 1 & 0 & 1 \\
1 & 1 & 1 & 1 & 0 \\  \end{pmatrix}$$

A Graph에서 v가 1번째 node라고 가정하면 $$e_v = 1$$이다.

Observation : coefficient 군집화는 #(triangles)로 eco-network에서 count한다.

위의 정의를 #(pre-specified subgraphs, i.e., graphlets)로 일반화가 가능하다.

- Graphlets

Goal : 노드 $$u$$ 주위의 network 구조를 묘사하는 것.
Graphlets는 $$u$$의 이웃 노드들의 구조를 묘사하는 작은 부분 그래프들이다.

Analogy
degree는 해당 노드와 연결된 #(edges)
clustering coeffieient는 노드와 연결된 #(traiangles)
Graphlet degree vector GDV : Graphlet-base features for nodes
GDV는 연결된 #(graphlets) 로 count된다.

 size가 2-5인 노드들의 graphlets을 고려하면 node의 이웃 topology를 묘사하는 노드의 특징인 73개의 coordinates vector를 얻게 된다.

 Graphley degree vector는 node의 local network topolgy의 측정 방법을 제공한다.

 두 노드에서 얻어진 벡터들을 비교함으로써 좀 더 상세한 지역 topological 유사도를 얻을 수 있다. node degree, clustering coefficient보다

 Induced Subgraph & Isomorphism

다른 그래프에서 꼭지점$$_{vertices}$$과 부분집합의 꼭지점에서 모든 연결의 subgraph를 유도$$_{induced}$$한다.

$$A = \begin{pmatrix} 
0 & 1 & 1\\ 
1 & 0 & 1\\
1 & 1 & 0
\end{pmatrix}$$
A는 induced subgraph

A'는 not induced subgraph이다.
$$A' = \begin{pmatrix} 
0 & 0 & 1\\ 
0 & 0 & 1\\
1 & 1 & 0
\end{pmatrix}$$


Graph Isomorphism
: 같은 노드의 수를 가지고 있고 같은 방향으로 똑같이 연결된 두개의 그래프를 Isomorphism이라고 한다.

![2-1](https://github.com/eastk1te/P.T/assets/77319450/2c018a43-9096-49da-8bc0-e5741bef0794)
_Figure 1 : Isomorphism_

![2-2](https://github.com/eastk1te/P.T/assets/77319450/3de0a194-2369-4e87-8680-5fb42724dcab)
_Figure 1 : Isomorphism_


Graphlets Rooted connected 
induced non-isomorphic subgraphs:

Graphlet Degree Vector GDV
: a주어진 노드에서 수집된 graphlets의 count vector

![2-3](https://github.com/eastk1te/P.T/assets/77319450/ef01eb41-0dea-445b-b89c-361e6aa40c03)
_Figure 1 : Isomorphism_


They can be categorized as:
▪ Importance-based features:
▪ Node degree
▪ Different node centrality measures
▪ Structure-based features:
▪ Node degree
▪ Clustering coefficient
▪ Graphlet count vecto


Importance-based features: capture the 
importance of a node in a graph
▪ Node degree:
▪ Simply counts the number of neighboring nodes
▪ Node centrality:
▪ Models importance of neighboring nodes in a graph
▪ Different modeling choices: eigenvector centrality, 
betweenness centrality, closeness centrality
 Useful for predicting influential nodes in a graph
▪ Example: predicting celebrity users in a social 
network

Structure-based features: Capture topological 
properties of local neighborhood around a node.
▪ Node degree:
▪ Counts the number of neighboring nodes
▪ Clustering coefficient:
▪ Measures how connected neighboring nodes are
▪ Graphlet degree vector:
▪ Counts the occurrences of different graphlets
 Useful for predicting a particular role a node 
plays in a graph:
▪ Example: Predicting protein functionality in a 
protein-protein interaction network

![2-4](https://github.com/eastk1te/P.T/assets/77319450/c40182f3-5b4f-4ad6-b94f-5513cd7db727)
_Figure 1 : Isomorphism_


### Link Predcition Task and Features

이미 존재하는 연결을 기반으로 새로운 연결$$_{link}$$를 예측하는 작업이다.
주요한 포인트는 노드들의 쌍의 특징을 잘 디자인하는 것이다.

두개의 공식이 존재한다.
1. Links missing at random
    - 임의로 연결을 지우고 예측하려고 해보자
2. Links over time
    - $$t_0'$$시간까지 $$G[t_0, t_0']$$가 주여졌을때, $$G[t_1, t_1']$$까지 나타날 edges들의 순위 list를 output으로 나타내라.
    - 평가 
      - $$n=|E_{new}|$$ : $$[t_1, t_1']$$기간 동안 나타는 edge들의 수
      - 상위 n개 중에서 올바르게 나타낸 edge들의 개수를 맞춰라

Methodology
각각 노드의 쌍 (x,y)에서 score c(x,y)를 계산한다. 예를 들어 해당 score는 # of common neighbors of x and y
각 쌍 (x,y)를 score를 가지고 내림차순으로 정렬한다.
상위 n 개의 pair를 새로운 연결로 예측한다.
해당 연결은 $$G[t_1, t_1']$$에서 나타나게 되어있다.


#### Distance-based feature

두 노드의 최단 거리 길이.
그러나 해당 feature는 이웃이 겹치는 degree를 수집하지 못한다.
ex) A-B-C,F-D-C 라고할떄 (A,C), (F,C)는 모두 같은 2 이지만, A-D-C가 존재할떄 (A,C)는 두 루트를 구분하지 못한다.


#### Local neighborhood overlap

두 노드들간의 공유된 이웃 노드들을 수집한다.

Common neighbors
$$|N(v_1) \cap N(v_2)|$$
Jaccard's coefficient
$$\frac{|N(v_1) \cap N(v_2)|}{|N(v_1) \cup N(v_2)|}$$
Adamic-Adar index
$$\sum_{u \in N(v_1) \cap N(v_2)}\frac{1}{log(k_u)}$$


#### Global neighborhood overlap

local neighborhood features의 제한을 걸음.
$$|N_A \cap N_E| = 0$$
두 노드가 공통으로 이웃을 가지고 있지 않으면 metric은 항상 0이다 
그러나 두 노드는 미래에 잠재적으로 연결되어있다.
전체 그래프를 고려함으로써 resolve the limitation 

Katz index : 주어진 노드 쌍의 가능한 모든 walk의 길이를 센다.
어떻게 두 노드의 #walks 를 계산하는가?
그래프 인접 행렬을 사용하여 풀 수 있다.

직관력$$_{Intuition}$$ : power of adj Matrics

Recall : 
$$A_{uv} = 1 \text{, if }u \in N(v)$$
$$P_{uv}^{(K)} = u와 v 노드 사이의 #walks of length K$$
$$P^{(K)}=A^k$$가 된다.

어떻게 $$P^{(2)}_{uv}$$를 계산해야할까?
1. 우선 u와 v 노드에서 이웃노드까지의 길이 1의 #walks를 계산한다.
2. #walks를 건너는 u의 이웃들을 모두 합한다.

_Figure 5 : 1->2->2, 1->4->2_
Katz index - 노드 상뜰의 모든 길이의 walk를 count한다.

인접 행렬의 힘으로 풀 수 있다.

$$A^l_{uv}$$는 length l의 #walks를 알 수 있다.

따라서, Katz index는 Sum over all walk lengths이다.

_Figure 2-6_


## Graph-Level Features

전체 그래프의 구조를 특징화하는 feature를 얻어내는 것이다.

선후 지식$$_{Backgorund}$$

Kernel Methods
그래프 레벨 예측을 진행할때 전통적으로 흔히 사용되는 ML 방법이다.
아이디어는 feauter vector들을 대신하여 kernel을 디자인한다.

kernel에 대해 쉽게말해서

Kernel$$K(G,G') \in \mathbb{R}$$는 유사도 b/w data로 측정된다.
Kernel matrix $$K = (K(G,G'))_{G,G'}$$ 항상 positive로 준정의$$_{semidefinite}$$된다.
representation $$\phi(\cdot)$$이 존재한다. $$K(G,G')=\phi(G)^T\phi(G')$$
kernel이 kernel SVM과 같은 off-the-shelf ML model로 정의되었으면 예측을 하는데 사용할 수 있다.

Graph Kernels : 두 그래프의 유사도를 측정
1. Graphlet Kernel
  
그래프의 특징 vector $$\phi{Q}$$를 디자인하는것이 목표이다.
주된 아이디어는 Bag-of-Words(Bow)를 graph에 적용하는것이다.
BoW는 문서의 특징인 word count를 할떄 간단하게 사용한다
Naive extension to a graph : 노드를 workds로

Bag of node degrees를 사용하게 되면 어떻게 될까?

_Figure 2-7_

위와 같이 두개의 Graphlet Kernel과 Weisfeiler-Lehman(WL) Kernel이 그래프의 표현으로 Bag-of-*를 사용하면, *는 노드 degree보다 더 정교해$$_{sophisticated}$$진다.

그래프에서 각기 다른 graphlets의 수를 세는 것이 주된 아이디어로.
node-level feature의 graphlet 정의와 조금 다름을 알아야한다.
두 가지 다른 점은 
node graphlet은 연결이 필요하지않다(isolated nodes를 허락한다.)
여기서 graphlets은 고정되어있지않다.

그래프 G가 주어졌을때, graphlet list $$g_k = (g_1, g_2, ..., g_{n_k})$$는 $$f_G \in \mathbb{R}^{n_k}$$로  아래와 같이 정의된다.

$$(f_G)_i=\#(g_i \subseteq G) \text{ for i = 1,2,..., }n_k$$

_Figure  2-8_

두개의 그래프 $$G, G'$$가 주여졌을때 graphlet kernel은 아래와 같이 계산된다.

$$K(G, G') = f_G^Tf_{G'}$$

문제는 G, G'가 다른 사이즈인경우 값을 크게 왜곡$$_{skew}$$한다.
해결 방법으로는 각 특징 벡터들을 정규화$$_{normalize}$$ 하는 방법으로

$$h_G = \frac{f_G}{Sum(f_G)},\space K(G, G')=h_G^Th_{G'}$$

한계점으로 graplets을 계싼하는것은 expensive하다.
이러한 worst-case를 피할수는 없다. subgraph isomorphism test가 NP-hard이다.
만약 graph의 노드 degree가 $$d$$의 범주안에 있다면, graphlet 사이즈가 k의 복잡도는 $$O(nd^{k-1})$$ 이다.

더 효과적으로 그래프 kernel을 디자인 하는 방법은?

2. Weisfeiler-Lehman Kernel
 
더 효과적인 graph feature descriptor인 $$\phi(G)$$를 디자인하는것이다.

아이디어는 이웃 구조를 사용해서 반복적으로 노드의 어휘$$_{vocabulary}$$를 풍요롭게하는 것이다.
일반화된 버전으로 Bag of node degress가 있다. 

해당 방법을 사용하는 알고리즘으로 "COlor refinement"가 있다.

Color Refinement
노드 V의 집합을 가진 그래프 G가 주어졌을때 초기 색상 $$c^{(0)}(v)$$을 각 노드 v에 부여한다.
반복적으로 node의 색상을 아래와 같은 방법으로 통해 재정의한다.

$$c^{(k+1)}(v)=HASH({c^{(k)}(v), c^{(k)}(u)_{u \in N(v)}})$$

HASH 는 다른 input을 다른 색상으로 매핑해준다.

K번의 색상 재정의 이후에 $$c^{(k+1)}(v)$$는 K-hop 이웃의 구조를 요약한다.

_Figure 2-9_
_Figure 2-10_

위 과정을 k번 반복.
color refinement가 이후에 WL kernel은 주어진 숫자의 node들의 숫자를 count 한다.

이렇게 생성된 color num 차원의 vector들의 inner product를 통해 $$K(G,G')$$를 계산하게 됩니다.

WL kernel은 효과적인 계산이 가능합니다.
각 color refinement 스텝의 시간 복잡도는 #(edges)에 선형적이고, 이웃 색상을 더하는 과정이 포함되기떄문입니다./
kernel value를 계산할때 색상은 두 그래프를 따라가면서 만들어지기떄문에 #(colors)도 node의 최대 숫자만큼 됩니다.

1. Rankdom-walk kernel, SHortedst-path graph kernel 등등


Bag-of-Words
: 문서 가방으로써 각 문서는 어휘 사전의 크기와 같은 차원을 가지는 BoW 벡터로 표현되고 벡터의 각 차원은 어휘 사전의 하나의 단어에 대응하며, 문서에서 단어 빈도를 계산하여 벡터를 구성합니다. 이는 간단하고 해석하기 쉽지만, 문맥 정보를 무시하고 단어 순서를 고려하지 않는다는 한계가 있습니다.




<br><br>
---