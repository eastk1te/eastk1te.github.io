---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)

## GNN Augmentation and Training

## Prediction with GNNs

다음으로 우리는 GNN을 어떻게 학습시켜야할까?
Learning objective

_Figure 8-1_
GNN을 통과하여 나온 결과로 노드 임베딩 집합이 나온다 $$\{h_v^{(L)}, \forall v \in G\}$$

서로 다른 작업 수준은 다른 prediction heads를 필요로 합니다.
### 1. Node-level predction

노드 임베딩을 활용해 직접적으로 예측이 가능합니다. GNN 계산 이후 우리는 d 차원의 노드 임베딩을 얻게되는데.
k-way의 예측을 만들고 싶어한다고 가정하면, 예츠긍로는 개의 카테고리들 사이에서 분류를 진행하고, 회귀에서는 k 개의 목표르ㅜㄹ regress 합니다.

$$\hat{y}_v=Head_{node}(h_v^{(L)})=W^{(H)}h_v^{(L)}$$

$$W^{(H)} \in \mathbb{R}^{k * d}$$ : $$h^{(L)}_v \in \mathbb{R}^{d}$$에서 $$\hat{y}_v \in \mathbb{R}^k$$로 노드 임베딩을 매핑한다. 그러면 우리는 loss를 계싼할 수 있따/.

### 2. Edge-level prediction

노드 임베딩 쌍을 사용하여 예측을 만든다. k-way 예측값을 만드는 걸 가정하고 $$\hat{y}_{uv}=Head_{edge}(h_u^{(L)},h_v^{(L)})$$ 
option은 뭐가있을까?
1. concatenation + Linear : graph attention에서도 봤듯이 
    - $$\hat{y}_{uv}=Linear(Concat(h_u^{(L)},h_v^{(L)}))$$ 
    - linear가 2차원(concat을 통해 생성된)에서 k-dim으로 임베딩한다.
2. Dot product
    - $$\hat{y}_{uv}=(h_u^{(L)})^Th_v^{(L)}$$ 
    - 해당방법은 1-way 예측만 적용가능하다. link prediction:edge존재여부 등
    - k-way로 적용하기위해서는
    - $$\hat{y}_{uv}^{(k)}=(h_u^{(L)})^TW^{(k)}h_v^{(L)}$$ 
    - $$\hat{y}_{uv}=Concat(\hat{y}_{uv}^{(1)},..., \hat{y}_{uv}^{(k)}) \in \mathbb{R}^k$$ 


### 3. Graph-level prediction

그래프에서 모든 노드 임베딩을 활용하여 예측을 만들엊냄
k-way의 prediction을 만들기 원한다고 가정함.
$$\hat{y}_{G}=Head_{graph}(\{h_v^{(L)} \in \mathbb{R}^d,\forall v \in G\})$$ 
$$Head_{graph}(\cdot)$$은 GNN의 $$AGG(\cdot)$$과 유사함
options.
1. Global mean pooling
   - $$\hat{y}_{G}=Mean{graph}(\{h_v^{(L)} \in \mathbb{R}^d,\forall v \in G\})$$ 
2. Global max pooling
   - $$\hat{y}_{G}=Max{graph}(\{h_v^{(L)} \in \mathbb{R}^d,\forall v \in G\})$$ 
3. Global sum pooling
   - $$\hat{y}_{G}=Sum{graph}(\{h_v^{(L)} \in \mathbb{R}^d,\forall v \in G\})$$ 

이러한 옵션들은 작은 그래프에서 작 먹힘. 그러면 큰 그래프에서는?
Global pooling은 근 그래프에서 사용하면 정보를 잃을 수도 있음.
만약 노드 임베딩의 scale 값이 다른 두 그래프에서 sum pooling을 진행했는데 같이 나와버리면 다른 그래프의 임베딩인데 같이 나오니 정보를 잃는 현상이 발생함.
해결방법으로 모든 노드임베딩을 계층적$$_{hierarchically}$$으로 종합함.
예를 들어 방금 위의 sum poolingdmf ReLU(Sum())의pooling으로 변환하면 괜찮아짐.

DiffPool
_Figure 8-2_

각 level에서 독립적인 두개의 GNN을 활용한다$$_{leverage}$$
GNN A : 노드 임베딩을 계싼
GNN B : 노드가 속하는 계층$$_{cluster}$$를 계싼.
두 GNN은 각 level에서 병렬적으로 수행 가능함.
각 pooling layer는 GNN B를 통해 cluster를 할당하고 GNN A로 생성된 노드 임베딩을 종합한다.
각 군집에서 새롭게 단일 노드를 생성하고 군집간의 연결을 새롭게 pool 된 네트워크에서 유지한다. 그러면 GNN A와 GNN B는 결합적으로 학습이 된다.

## Training GNN


## Setting-up GNN Predction Tasks

<br><br>
---












