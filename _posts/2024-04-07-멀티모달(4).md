---
title: '[Study]Chapter 4. Case Studies and Real-world Application'
author: east
date: 2024-04-07 11:00:00 +09:00
categories: [Study, Multimodal]
tags: [Study, Multimodal]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

다시한번 멀티모달에 대해 간략히하면 최근에 대두된 멀티모달은 Large scale의 데이터들로 LLM 기반의 모델들이 주를 이룬다고 합니다. 관련되어 '24년에 Large Multimodal Agent(LMA)에 관한 survey가 있어 짧게 요약하며 시작하겠습니다.

![2](https://github.com/eastk1te/P.T/assets/77319450/e615808a-5b6a-4827-88f7-7efb1df23b90)
_Figure 1 : Top AI conference에 발표된 LMAs의 Milestones_

해당 연구에서 LMA를 강화학습 기반으로 4가지 구성요소(perception, planning, action, memory)로 나누어 네 가지 유형의 학습 모델 아키텍처(LMAs)를 만들었으며 다음과 같습니다.

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/7a6c07b3-34ba-4fd3-abd1-6c2028f80db7){: w="500"}
_Figure 2 : (a) 장기메모리 없는 폐쇄 소스 LLMs (b) 장기메모리없이 미세조정된 LLMs (c) 간접적으로 장기메모리를 가진 LLMs (d) 장기메모리를 지닌 LLMs_

이러한 LMA들은 다양한 분야의 적용이 가능한데 아래의 그림처럼 다양한 분야와 작업이 가능합니다.

![2](https://github.com/eastk1te/P.T/assets/77319450/7911a977-b646-4e42-8e0f-debf700a6105)
_Figure 2 : A variety of applications of LMAs_

- GUI 자동화 
  
  사용자 인터페이스를 보다 쉽게 접근(특히, 장애나 기술적 제한이 있는 사람들)

- 로보틱스 
  
  로봇의 지각, 추론 및 행동 능력을 환경의 물리적 상호작용과 통합.

- 게임
  
  지능과 현실감을 나타내 보다 나은 경험 제공.

- 자율주행
  
  전통적인 방법은 복잡한 시나리오를 효과적으로 지각하고 해석하는데 어려움이 있었으나 LMA를 통해 진전.(ex. GPT-3.5기반의 GPT-driver를 모션 플래너로 적용)

- 시각 추론
  
  LMA의 주요 초점은 멀티모달 분석은 QA작업으로 이루어져 합리적인 응답을 목표로함.

- 이미지 생성, 비디오, 오디오 등

> ## Ⅰ. DALL-E 2

OpenAI에 의해 '21년도에 소개된 DALL-E의 개발된 버전으로 더 현실적이고 정확한 이미지를 선명한 해상도로 생성해냅니다.

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/d991bedd-43e2-4225-90e8-11f423f5cb52)
_Figure 1 : 1024x1024 크기로 생성된 이미지 예제._

> ### ⅰ. Method

Contrastive model로 CLIP은 이미지의 견고한 표현과 semantic과 style을 잘 수집하는 것을 보여주었습니다. 이러한 CLIP을 이미지 생성으로 옮겨 아래와 같이 두 단계의 모델을 제안했습니다.

![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/eee39683-1965-430a-9285-8b2d892cb000)
_Figure 2 : unCLIP의 overview, (점섬 위) CLIP 학습 절차를 나타냄. 텍스트와 이미지의 결합 표현 공간을 학습 (점선 아래) text-to-image 생성 절차_

아래와 같이 두가지 요소를 적용해 캡션에서 이미지를 생성하는 stack 구조를 디자인했습니다.
1. 텍스트 캡션이 주어진 CLIP 이미지 임베딩을 사전에 생성
2. 위의 임베딩을 조건으로 이미지 생성 디코더 구현.

이미지 표현을 생성하는 것은 이미지의 photorealism과 캡션 유사도에서 최소한의 loss로 다양성의 향상을 보여주었으며 다양한 이미지에서 이미지 표현에 기반한 디코더는 semantic과 style을 잘 보존했습니다.

해당 방법은 텍스트(y)에서 이미지(x)를 뽑는 절차를 아래와 같이 표현가능합니다.

$$P(x\vert y)=P(x,z_i \vert y) = P(x\vert z_i, y)P(z_i\vert y)$$

$$z_i$$ : image embedding, $$z_t$$ : text embedding 으로 $$z_i$$는 x에 의해 결정되며 chain rule을 통해 위 수식이 성립하게 됩니다.

- Decoder

    $$P(x\vert z_i, y)$$ : CLIP 이미지 임베딩을 입력으로 이미지 x를 생성

    Diffusion모델로 GLIDE[^1] 아키텍처를 사용해 CLIP 텍스트 임베딩이 이미지 인코더를 통해 이미지 임베딩을 생성하고, diffusion 디코더의 입력으로 들어가게됩니다. Figure 2의 점선처럼 CLIP 모델은 prior와 디코더가 학습하는 동안 멈춰$$_{freeze}$$있습니다.

    추가적으로 단계적인 업샘플링 모델(가우시안 블러, DSR)들이 사용되어 고화질의 이미지를 생설할 수 있게 해줍니다

[^1]: GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusuion Models

- Prior

    $$P(z_i \vert y)$$ : 캡션 y를 입력으로 이미지 임배딩을 생성

    캡션 y로부터 이미지 임베딩 $$z_i$$를 생성하는 모델로 아래와 같이 두가지 모델을 사용했습니다.

    1. Autoregressive prior : 이미지 임베딩이 이산$$_{discrete}$$ 코드로 변환되어 자동회귀적 예측
    2. Diffusion prior : 가우시안 디퓨전 모델로 연속적인 벡터 z 생성

> ### ⅱ. Image manipulations

위의 방법을 통해 CLIP의 이미지를 설명하는 잠재 $$z_i$$와 디코더가 x를 설명하는 잠재 $$x_T$$를 bipartite 잠재표현 $$(z_i, x_T)$$로 표현이 인코딩이 가능합니다. 

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/820ab181-1d60-4790-ae83-803393e4e3f0)
_Figure 1 : (왼쪽) 입력이미지의 다양한 변형으로 의미론적인 정보와 스타일 요소를 보존하면서 중요하지 않은 디테일을 변형시킴 (가운데) 두 이미지간의 CLIP 임베딩 사이를 채워서 diffusion 모델로 디코드한 변형 (오른쪽) 두 텍스트와 CLIP 이미지 임베딩 사이를 채워 텍스트의 차이를 이미지에 적용_

해당 이분 표현은 아래와 같은 세 종류의 조작이 가능하게 합니다.

- Variations

    Figure 1의 왼쪽 그림과 같이 DDIM을 사용하여 샘플링(η > 0)하면서 이분 표현에 디코더를 적용해 주어진 이미지를 생성해 나갈 것입니다. η가 커질수록 η=0인 원본 이미지 x를 중심으로 변형됩니다.

- Interpolations

    두 이미지의 임베딩 사이를 $$z_i_θ = slerp(z_i_1, z_i_2, θ)$$의 구 모양$$_{spherical}$$으로  탐색할 수 있습니다.

- Text diffs

    CLIP은 이미지와 텍스트를 같은 잠재공간으로 임베딩할 수 있는 장점이 있습니다. 이를 통해 새로운 텍스트 y를 반영하도록 이미지를 수정할 수 있는데, 본 캡션과 새로운 캡션 각각 CLIP 텍스트 임베딩을 구해 두 임베딩의 방향과 차이를 나타내는 잠재 벡터를 만들어 Interpolations 처럼 회전시켜 이미지를 수정할 수 있습니다.

> ### ⅲ. limit

한계로는 이미지에서 세부 디테일에 관해 어려움을 겪고, 속성과 객체를 혼동(ex. "a red cube on top of a blue cube”)하는 경우가 있다고 합니다.








> ## Ⅱ. WebQA

WebQA는 Carnegie Mellon 대학과 Microsoft에서 같이 만들어진 웹 기반 질의응답(Question Answering, QA) 시스템으로 멀티모달 멀티홉(여러 과정) 추론을 위한 새로운 벤치마크를 소개했습니다. 이러한 시스템은 멀티모달리티에서 관련된 정보를 인식하고 결합하여 질문에 대한 답을 추론합니다. 

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/bbf21104-2961-4525-9232-d208b90851d2)
_Figure 1 : WebQa 데이터셋 파이프라인 예쩨. 질문이 두 연관된 소스에 대해 reasoning하고 찾는것 그리고 discarding distractor 옳은 자연어 답을_

WebQA 시스템의 목적은 대량의 정보 속에서 질문에 관련된 정확한 답변을 하는것입니다. 이러한 과정은 인간에게는 쉽지만 기계에게는 어려운 과제입니다. 따라서, 사람이 웹을 사용하는 방식을 모방해 (1)질문을 하고, (2)소스를 찾고, (3)응답을 생성합니다. 

이러한 VQA 작업을 웹 검색 및 다중 홉에 적용하려면 시각적 표현 학습, 지식 집계 및 언어 생성이 필요합니다. 즉, 텍스트와 이미지를 통합하고, 관련 지식을 찾아 수치적인 추론을 통해 정보를 통합해 자연어로 답변을 생성하는 멀티모달 추론 및 텍스트 생성 모델을 나타냅니다.

> ### ⅰ. Task Formulation

질문 집합 Q와 긍정 소스 $${s_q, ..., s_m}$$ (green), distractor의 집합 $${s_{m+1},..., s_n}$$(red)가 주어지면, 모델은 정답이 어디에서 파생됐는지 소스를 인식하고 Context C로서 소스를 선택해 정답 A를 생성합니다.

해당 과정을 통해 여섯개의 이미지 들을 제시하고 각 질문에 대해 세개의 QA 쌍을 생성하도록 진행했습니다. VQA와 유사하지만 이미지가 질문에 대한 추가적인 정보로 사용되지 않고, 추론을 위해 사용되었습니다.

> ### ⅱ. Metric

응답 퀄리티는 Fluency(BARTScore)로 Accuracy(keywords overlap)으로 소스 retrieval은 F1 score로 평가 됨.

![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/d690fe51-33d1-4d36-a473-b65b35bb781d)
_Figure 2 : 관련 소스에 "제한"을 둔 경우와 "전체"를 사용한 경우를 모두 제시함._

Figure 2에서 보여졌듯이 LMA에서도 좋은 성능을 보이지 않아 아직은 사람 수준의 성능은 불가능해보입니다. 그러나 이러한 결과로 실세계 응용에 도움이 되는 모델의 개선 관점을 나타냅니다.

















> ## Ⅲ. GPT-4

OpenAI에서 만든 GPT계열의 최신 모델로 넓은 상식과 문제해결능력을 지녀 높은 정확도로 어려운 문제를 해결할 수 있습니다. 이는 전보다 더 창의적이고, 시각적 입력과 더 긴 텍스트를 다룰 수 있습니다.

![1](https://github.com/eastk1te/P.T/assets/77319450/e7983a7d-389e-48ca-bfe1-d1720cc82857)
_Figure 1 : GPT4의 시각적인 입력 능력을 보여주는 예시_

이전 GPT 모델과 같이 모델의 "환각"이나 오류를 발생해 신뢰성의 부재가 발생하는 문제들이 있습니다. 


![2](https://github.com/eastk1te/P.T/assets/77319450/43dd51a5-1b49-4d44-bac5-93a521f6e40e)
_Figure 2 : 차트 정보를 입력으로 한 GPT4_

![3](https://github.com/eastk1te/P.T/assets/77319450/b9cb7565-cc94-46ef-ac6c-1152d5593b4a)
_Figure 2 : 다른 언어로 작성된 다이어그램을 입력으로 한 GPT4_

![4](https://github.com/eastk1te/P.T/assets/77319450/92b2b2f3-d06d-4599-a361-24f8c83e0b10)
_Figure 2 : GPT4의 이미지 이해 능력_


> ### ⅰ. GPT-4V

GPT-4V(ision)은 GPT4 이후 안전성 문제를 다루어진 이후 만들어진 모델입니다.

![10](https://github.com/eastk1te/P.T/assets/77319450/69d675b1-445c-4563-8461-984877626247)
_Figure 10 : 근거 없는 추론과 편향에 대한 모델 비교_

해당 GPT-4 with Vision(a.k.a GPT-4V)의 System Card에서 GPT-4에 기반으로 시각적인 능력과 안정성을 개선하는 작업에 대해 소개되었습니다. GPT4와 동일하게 학습 과정은 인터넷의 방대한 데이터를 학습하여 사람에 의한 피드백을 통해 강화 학습 알고리즘(RLHF)으로 미세조정되었습니다. 















> ## Ⅳ. Naver deview

NAVER DEVIEW 2023에서 진행한 "이제는 AI가 읽고(Language), 보고(Vision), 생성하는 Large-scale Multimodal의 시대입니다" 라는 트랙을 이전에 접하고 '한국의 구글'이라 불리는 네이버에서 멀티모달을 어떻게 다루는지에 대해 소개하겠습니다.

> ### ⅰ. VLM Foundation Modeling

- 패션 상품 속성 검색 모델.

  ![1](https://github.com/eastk1te/P.T/assets/77319450/e70acc77-2a6e-4c64-a2df-326ec28535d5)
  _Figure 1 : CLIP기반의 domain-specific 모델_

  Clip 모델 기반 contrastive 학습 기반으로 특정 도메인에 사용하는 구조로 분류 구조에서 상품 속성을 가져와 이미지, 카테고리 등의 데이터들을 각 인코더에 넣어 활용한 케이스입니다.

  아래와 같이 "신발"이라는 상품에서 "이 상품의 [토스타일]은 [라운드 토]입니다"와 같이 상품 속성이 나오게 합니다.

  ![2](https://github.com/eastk1te/P.T/assets/77319450/cc3d5f03-17d8-47be-b9ea-65244f6514c3)
  _Figure 2 : 상품 속성 검색 예시_

- prompt-Coca model

  기존의 CoCa모델을 기반으로 데이터의 속성에 맞추어 Prompt 토큰을 더해 학습한 케이스입니다.

  ![3](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/fb82f58e-0323-47e0-9d67-1c1d3987aea9)
  _Figure 3-1 : CoCa: Contrastive Captioners are Image-Text Foundation Models, 2022_

  영어와 다르게 한국어에서는 문서 제목, 퀀리, 캡션 등 각 데이터마다 속성이 너무 달라 [PROMPT], [QUERY], [SHOPPING] 등 각 스타일에 맞는 Prompt 토큰을 추가적으로 넣어서 학습을 진행했습니다.

  ![3](https://github.com/eastk1te/P.T/assets/77319450/5217d359-d7b0-4b61-b867-644863b01e45)
  _Figure 3-2 : Image-to-Text 생성 예시_

- Modality-agnostic Model

  ![4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/1f9de831-bc39-4214-8105-2f896f48145c)
  _Figure 3-3 : 모델 아키텍쳐와 성능 비교_

  하나의 모델로 각 모달리티에 대해 좋은 표현을 학습하고 동일한 방식(weight share)으로 활용하도록 하는 것을 목표로 논문[^1]이 게제되었습니다.

  Moco v3 의 Contrastive Self-supervised Learning framework를 사용하여 CTR(image, text)가 모달리티 특성에 따른 차이 때문에 어려워 CTR(image+text, image+text)로 학습을 진행하는 방식으로 각 모달리티를 동일한 공간에 표현하여 사용합니다.

> ### ⅱ. Korean Text-to-Image Generation

기존 stable diffusion의 경우 한글이 적용되지 않아 아래와 같이 knowledge distillation을 활용해 영어 잠재 공간에 한국어 임베딩도 활용할 수 있도록 학습하여 영어와 한글 모두 같은 잠재공간에서 표현이 가능하게 되었습니다.

![4](https://github.com/eastk1te/P.T/assets/77319450/68442455-29ac-402c-a2e8-ed2c4dd13769)
_Figure 4 : 텍스트 인코더 학습 절차_

동일한 데이터에 대해 영어 임베딩과 한글 임베딩 사이의 차이가 존재했으나 위의 학습 절차를 통해 같은 공간에 위치하게 되었고, 해당 텍스트 인코더만 변경해 간단한 적용이 가능했습니다. 즉, 한글로도 Stable diffusion 모델에서 자연스러운 이미지 생성이 가능하게 되었습니다.

> ### ⅲ. Multimodal Document Search(MDS) 서비스

VLM을 활용하여 멀티모달 질의에 대한 문서 검색 및 랭킹 서비스를 적용한 사례에 대해 이야기 해줍니다.

![7](https://github.com/eastk1te/P.T/assets/77319450/99cf0683-9e49-4d5c-b8ba-dcfae3eb96b4)
_Figure 7 : VLM 을 통한 문서 검색 서비스, 이미지와 텍스트에 대한 feature 분석을 통해 랭킹 서비스를 제공함._

아래와 같은 기술들이 해당 MDS에 포함되어 있습니다.


- 스마트 썸네일

  ![8](https://github.com/eastk1te/P.T/assets/77319450/6866e227-07a0-42a2-9d56-cf7087f96bc6)
  _Figure 8 : 질의와 이미지를 기반으로 score를 측정하여 좋은 썸네일을 선택할 수 있게함_

- 멀티모달 기반 패션 상품 검색

  ![9](https://github.com/eastk1te/P.T/assets/77319450/d59dbcd0-a063-4389-a731-e0345faa9f6d)
  _Figure 9: 멀티모달 기반으로 상품읠 추천할 수 있게 함._

- 의류 검색을 위한 색상/패턴 분류기

  ![10](https://github.com/eastk1te/P.T/assets/77319450/aee63351-b49e-456b-acb1-fd326b8c93dc)
  _Figure 10: 명확한 상품 이름이 없는 경우 VLM 기반으로 색상과 패턴을 분류해 메타데이터로 태깅_

- Future Works
  
  "Adding Conditional Control to Text-to-Image Diffusion Models, 2023" 해당 논문을 참고해 아래와 같이 새로운 상품에 대한 상품을 검색할 수 있는 모델을 구상중이라고 합니다.

  ![11](https://github.com/eastk1te/P.T/assets/77319450/bd2326e3-db26-40f1-8331-366f3458c7af)
  _Figure 11: 파란색 신발 이미지와 '초록색'이라는 질의를 받아 유사한 제품을 멀티모달 상품검색 시스템과 융합하여 사용 가능_

  또한, 명소라는 주제로 옮겨 멀티모달 기반의 검색 서비스도 생각중이라고 합니다.

  ![12](https://github.com/eastk1te/P.T/assets/77319450/59080854-0687-4fa9-a879-3230480b5f71)
  _Figure 12: 앞서 설명한 패션이라는 주제에서 명소라는 주제로 옮겨 멀티모달 검색서비스를 도입_













> ## Ⅴ. MGL, 2023

이전에 그래프를 공부했다보니 그래프와 관련되어 멀티모달 그래프에 대한 내용을 조금 다루어보려고 합니다. 

![1](https://github.com/eastk1te/P.T/assets/77319450/5826f6d9-f358-4f10-8a45-c5fcb9256690)
_Figure 1 : 그래프 중심의 멀티모달 학습으로 멀티모달 그래프 구조를 통합하는 프레임워크_

> ### ⅰ. GNN for Multimodal-learning

기존의 멀티모달 모델들은 인코더-디코더 프레임워크에 기반하여 좋은 성능을 입증하였습니다. 그러나 모달리티 간의 복잡한 관계가 형성될때 모달리티간 상호의존성을 활용한 전략으로 그래프 신경망이 좋은 방법일 수 있다고 합니다.

GNNs을 사용한 멀티모달 학습은 다른 데이터 유형간의 상호 작용을 모델링할 수 있어 좋은 방법이 될 수 있지만 그래프  학습을 통한 데이터 융합은 그래프 위에 네트워크 토폴로지와 추론 알고리즘의 적용이 필요합니다. 따라서, 주어진 멀티모달 입력 데이터에서 downstream까지 아래와 같은 방법론을 제시했습니다.

![2](https://github.com/eastk1te/P.T/assets/77319450/df155583-fc73-479e-aea9-d0710633b2ac)
_Figure 2 : (a) 단일 모달에 특화된 아키텍쳐들의 결합 (b) 멀티모달을 다루는 아키텍쳐 (c) MGL의 상세 개요로 개체 식별, 토폴로지 발견, 정보 전파, 표현 결합으로 음영된 부분 처럼 두가지 단계로 나뉘어짐._

해당 방법론은 구조 학습(개체, 토폴로지 식별)과 학습 구조(정보 전달, 표현 결합)의 두 단계로 구성됩니다.
- 개체 식별 : 다양한 모달리티에서 관련 개체를 식별(ex. 슈퍼픽셀 등)하여 공유된 네임스페이르소 전달합니다.
- 토폴로지 식별 : 노드 간의 상호 작용과 유형을 식별(ex. 인접 노드 등)합니다.
- 정보 전달 : 그래프 인접성을 기반으로 메시지 패싱을 사용합니다.
- 표현 결합 : 하향식 작업에 따라 학습된 노드 수준의 표현을 변환합니다.

> ### ⅱ. mgl for images

Image-intensive graphs(IIGs)는 노드가 노드가 시각적 특징을 대표하고, 엣지가 공간적 연결을 나타냅니다. 이러한 그래프는 이미지 분할, 복원, 인간-객체 상호작용 등의 작업에서 사용됩니다.

![3](https://github.com/eastk1te/P.T/assets/77319450/01300c75-d6f6-4e1d-a3bc-074447cf2e08)
_Figure 3 : 이미지에 적용한 멀티모달 그래프 (a) 이미지 이해를 위한 모달리티 식별(슈퍼픽셀) (b) 이미지 노이즈 제거를 위한 토폴로지 (c) 인간-객체 상호작용에서의 토폴로지 식별을 통해 두 개의 그래프가 생성 (d) 인간-객체 상호작용에서의 정보 전달은 객체탐지의 가장자리를 포함하여 전달_

> ### ⅲ. mgl for language

언어 모델은 문맥적인 언어 임베딩을 생성해 광범위하게 사용되고 있습니다. 그러나 단어를 넘어 언어 내의 구조는 문장, 문단, 문서에서 존재해 이러한 구조를 포착하지 못합니다. 지배적인 트랜스포머 모델은 이러한 구조를 잘 수집하나 엄격한 계산 및 데이터 요구 사항을 가지고 있습니다. 언어 구조를 그래프에 포함시켜 노드가 언어 의존성에 의해 연결된 의미적 특징을 나타낼 수 있게 합니다.

![4](https://github.com/eastk1te/P.T/assets/77319450/83092cc2-572a-4cc9-acfb-b4e4227d8744)
_Figure 4 : 텍스트에 적용한 멀티모달 그래프 (a) 문장, 문서에서의 다양한 맥락 수준 식별 (b) Text-intensive graph에서 토폴로지 식별 (c, d) 다운 스트림을 적용한 예시_

> ### ⅳ. mgl for natural sciences

knowledge-intensive graphs(KIGs)들은 구조에서의 특정 작업이나 과학적 지식의 인코드의 유도편향을 포함합니다.

![5](https://github.com/eastk1te/P.T/assets/77319450/4bad4603-47d0-459a-b8af-a556505d3fd7)
_Figure 5 : 자연 과학에 적용한 멀티모달 그래프 (a) 물리적 상호작용에서의 정보 전달 (b) 분자 추론에서의 정보 전달 (c) 단백질 모델링에서의 토폴로지 식별_










> ## Ⅵ. MGL Survey, 2024


멀티 모달 학습의 특별한 유형인 멀티모달 그래프 학습(MGL) 은멀티모달 그래프의 보급으로 주요한 주제가 되었으며 그래프 형식의 멀티 모달데이터는 의료, 소셜, 교통등 다양하게 존재한다고 합니다.

MGL은 MGs의 관계적인 표현을 활용하여 멀티모달 데이터 간의 상호 관계들을 완전히 탐색하는 것을 목표로 합니다. 이는 데이터가 명확하고 일관된 구조를 갖는 다른 유형의 멀티모달 학습과 매우 다르다고 합니다. 즉, 데이터 간의 상호작용을 보다 잘 표현하는게 가능합니다.

먼저 그래프에 대해 이야기하자면 세가지 수준으로 구분되며 멀티모달 그래프는 아래와 같이 구분될 수 있다고 합니다.

![6](https://github.com/eastk1te/P.T/assets/77319450/a378bbda-2444-492b-afdc-4a22e643b23c)
_Figure 6 : Graph 특성에 따른 Multimodal Graph types_

- feature-level MGs : 각 노드가 멀티모달 특성을 가져 상호연결된 집합을 위해 사용함 
- Node-level MGs : 각 노드가 단일 모달 특성을 carrie하여 특성 모달리티는 노드들이 vary across modes. 엔티티들 사이의 연결을 표현하기위해 구현됨 다른 모달리티로 묘사된.
- Graph-level MGs : 여러개의 부분그래프로 이루어져있으며 sole 모달리티의 특징을 가지고있음. 같은 엔티티 집합으로 상호연결됨.


> ### ⅰ. Taxonomy of MGL

- `Multimodal Graph Convolution Network`

  GCN은 컨볼루션을 기반으로 한 그래프 모델로 노드 간의 관계를 탐색하는데 있어서 서로 다른 모달리티 간의 관계를 잘 나타낸다고 합니다.

- `Multimodal Graph Attention Network`

  GAT(Graph Attention neTwork)는 어텐션 매커니즘을 활용하여 노드의 중요성을 동적으로 측정할 수 있어 멀티모달 정보 융합에 사용된다고 합니다.


- Multimodal Graph Contrastive Learning

  GCL은 서로 다른 노드 표현을 학습하는데 중점을 두어 모달 간의 차이 추출에 사용된다고 합니다.

> ### ⅱ. MGL applications

- `Multimodal Knowledge Graphs`

  멀티모달 지식그래프는 다양한 모달리티의 정보를 통합하여 복잡한 지식을 포괄적으로 설명해 대화, 추천 및 정보 검색 등 다양하게 사용되며 멀티모달을 통해 엔티티 인식, 연결 예측, 관계 추출, 지식 추론 등 다양한 분야에서 지식 그래프 임베딩으로 사용된다고 합니다.

- `Multimodal Biomedical Graphs`

  멀티모달 생물의학 그래프는 분자 및 세포 수준에서 특성과 관계를 모델링하는 복잡한 생물학적 시스템에 적용된다고 합니다.

- `Multimodal Brain Graphs`

  다양한 신경 이미지 기술들(EGG, MRI, PET, DTI 등)이 개발되어뇌 분석을 위해 사용되어 뇌의 기능, 구조, 구조-기능 그래프로서 뇌 내의 관계를 탐색하는데 사용된다고 합니다.


> ## Ⅶ. Financial time series forecasting with MGNN

주가 예측 모델로 멀티모달 그래프를 활용한 논문이 있어 간략히 소개드립니다.

![8](https://github.com/eastk1te/P.T/assets/77319450/819a2d9c-b45f-4f78-b28e-51ca2da71ef4)
_Figure 8 : 멀티모달리티 입력과 헤테로지니어스 그래프._

Figure 8처럼 헤테로지니어스 그래프에서 멀티모달 데이터를 입력으로 받아 모달리티 내의 그래프 어텐션과 모달리티 간의 소스 어텐션 층을 통해 예측을 하는 구조가 이루어집니다.

![7](https://github.com/eastk1te/P.T/assets/77319450/ae65f25a-71bf-4e71-bdba-196b8f1c5c8b)
_Figure 7 : Overview_




> ## Ⅷ. REFERENCES

1. [J. Xie, "Large Multimodal Agents: A Survey", 2024](https://arxiv.org/pdf/2402.15116.pdf)
2. [OpenAI, "DALL-E : Zero-Shot Text-to-Image Generation", 2021](https://arxiv.org/abs/2102.12092)
3. [OpenAI, "DALL-E 2 : Hierarchical Text-Conditional Image Generation with CLIP Latents", 2022](https://arxiv.org/abs/2204.06125)
4. [Microsoft, "WebQA: Multihop and Multimodal QA", 2-22](https://arxiv.org/abs/2109.00590)
5. [OpenAI, "GPT-4V(ision) System Card", 2023](https://cdn.openai.com/papers/GPTV_System_Card.pdf), 
6. [Y. Ektefaie at el, "Multimodal learning with graphs", 2023](https://arxiv.org/pdf/2209.03299.pdf)
7. [C. Peng ,"Learning on Multimodal Graphs: A Survey", 2024](https://arxiv.org/pdf/2402.05322.pdf)

[^1]: Unifying Vision-Language Representation Space with Single-tower Transformer, AAAI 2023