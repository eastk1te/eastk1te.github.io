---
title: '[Paper]Self-Supervisd Learning'
author: east
date: 2023-08-10 00:00:00 +09:00
categories: [Paper, Self Supervised Learning]
tags: [Paper, Self Supervised Learning]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

Transformer에서 self-supervised learning으로 어텐션 매커니즘이 활용되었습니다. 이러한 방법은 토큰(데이터)에 포함된 구조와 패턴을 이용하여 모델을 학습시키는 것으로 최근에 트랜스포머 기반의 다양한 모델들이 나왔습니다. 

따라서, Meta Research의 발행된 내용을 참고하여 해당 내용에 대한 더 자세한 이해를 돕고자 합니다.

> ## Ⅰ. SSL : Self-Supervised Learning


지도 학습은 많은 양의 데이터에 label이 필요하기 떄문에 한계에 부딪혀있습니다.

우리는 경험을 통해 세상이 작동하는 방식을 배웁니다. 관찰과 경험을 통해 일반적인 이해를 하여 새로운 상황에서도 예측이 가능합니다. 이러한 common sense는 사람이 새로운 기술을 배우는데 많은 양의 학습이 필요없이 됩니다. 그러나 common sense는 기계에 적용시키기란 어려운 일입니다. 예를 들어, 아이에게 소 그림을 몇장 보여주면 다른 소를 보고도 인식이 가능하지만, 기계는 해변에 누워있는 소와 같은 새로운 상황에서는 이해하기 어려울 것입니다. 이러한 차이는 세상이 어떻게 작동하는지에 대한 배경 지식에 의해 발생합니다.

그러면 어떻게 기계에 적용할 수 있을까?

자기 지도학습이 배경 지식과 common sense에 가까운 방법 중 하나일 것입니다. SSL을 통해 훨씬 더 많은 양의 데이터로 부터 학습할 수 있하고, 세상의 더 감지하기 힘들고 덜 흔한 표현을 인삭하는데 중요합니다. 

이러한 SSL은 일종의 예측 학습으로 데이터 자체에서 학습됩니다. NLP 분야에서는 지도 학습보다 이러한 방법이 더 좋은 성능을 보여주었습니다. NLP 분야에서 마스킹을 통해 나머지 단어나 다음 단어들의 잠재 데이터를 예측할 수 있어 데이터 자체 구조를 활용하기에 레이블에 의존하지 않아 상호 발생하는 모달리티나 대규모 데이터 셋에 걸쳐 다양한 감독 신호를 활용할 수 있게됩니다. 즉, 비지도 학습보다는 "Self-Supervised"가 더 올바른 표현입니다.

위에서 언급했듯이 NLP 분야에서 BERT와 같은 레이블이 없는 대규모 텍스트 데이터에서 사전 학습되는 형태의 모델들은 SSL 단계에서 가려진 텍스트를 예측하여 텍스트의 의미를 학습하니다. 그러나 컴퓨터 비전 분야에서는 SSL이 NLP 만큼 발전을 이루지는 못했었습니다.

![4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/75b19086-094e-4914-9890-8140896efa3b){: w="500"}
_Figure 1 : Modeling the uncertainty in prediction_

이 문제를 더 잘 이해하기 위해서는 예측 불확실성에 대해 알아야하는데 NLP에서 단어를 예측하려면 어휘의 가능한 모든 단어에 대한 예측 점수를 계산하는데 이 과정은 불확실성이 수반되긴하지만 어휘목록을 만드는건 가능합니다. 반면 CV에서는 누락된 프레임 이나 패치 또는 음성에서 누락된 세그먼트를 예측하는 것은 더 고차원 적인 연속 개체에 대한 예측으로 그 가지수가 무한합니다.


> ### ⅰ. Energy based Model

따라서, 이러한 예측 불확실성을 해결하기 위해 에너지 기반 모델(EBM)의 프레임워크로 SSL을 생각해야합니다. EBM은 두 입력의 에너지(즉, 호환성)를 측정하는 방법으로 사용됩니다. 아래와 같이 x와 y의 비호환성을 나타내는 에너지 함수(energy_func)를 통해 단일 값을 생성합니다. 에너지가 낮으면 호환 가능하고, 높을수록 호환되지 않는 것을 의미합니다.

$$minimize(energy_func(x,y))$$


> ### ⅱ. Joint embedding, Siamese networks

위의 에너지 기반 모델을 수행하기 위한 딥러닝 아키텍처로는 "결합 임베딩"을 사용할 수 있습니다. 결합 임베딩 아키텍처는 동일하거나 유사한 네트워크($$enc(\cdot)$$)를 사용하여 두 입력 x와 y를 처리하고 각각의 임베딩 벡터$$z_x = enc(x)$$를 생성합니다. 이후, 두 임베딩 벡터 사이의 거리$$energy_func(\cdot)$$를 계산하여 에너지로 활용하는 세 번째 모듈이 이를 결합합니다. 주요 과제는 x와 y가 서로 다른 이미지인 경우 높은 에너지를 생성하는 임베딩 벡터를 생성하는 것입니다.

$$minimize(energy_func(enc(x), enc(y)))$$

만약 적절한 메커니즘이 없다면, 두 네트워크는 입력을 무시하고 항상 동일한 출력 임베딩을 생성할 위험이 있으며 이를 "붕괴"라고 합니다. 이 경우, 서로 다른 이미지임에도 불구하고 낮은 에너지를 갖게 됩니다.

> ### ⅲ. Contrastive energy-based SSL

Contrastive 방법은 호환되지 않는 x와 y의 쌍을 사용하여 모델을 조정해 에너지를 증가시킵니다. 예를들어 NLP에서 일부 단어를 마스킹하여 원래의 텍스트를 재현하도록 하는 것이 대조적 방법의 한 예 입니다. 이떄 마스킹되지 않은 텍스트는 낮은 에너지로 마스킹된 텍스트는 높은 에너지로 재구성합니다. 이러한 방식은 "Denoising AutoEncdoer"와 유사한 기술에서 사용됩니다.

$$ minimize(energy_func(x, corrupt(x)))$$

잠재 변수 모델은 관찰되지 않는 추가 입력 변수 z를 포함하고 있기 때문에 "잠재 변수"로 알려져 있습니다. 이러한 모델은 대조적 방법을 사용하여 학습될 수 있으며, 예를 들어 GAN에서 판별자($$energy_func(\cdot)$$)는 입력과 생성 사이의 에너지를 계산하는 것으로 볼 수 있습니다. 그러나 이러한 대조적 방법에서는 주어진 이미지와 모든 측면에서 다를 수 있는 대조 이미지를 만드는 것은 거의 불가능하기에 학습이 매우 비효율적입니다.

> ### ⅳ. Non-contrastive energy-based SSL

그러나 같지 않은 쌍의 에너지를 명시적으로 증가시키지 않아도 높다는 것을 확인할 수 있다면 어떨까요? 이러한 방법은 현재 시각적 자기 지도 학습에서 가장 인기 있는 주제 중 하나 입니다.

$$minimize(energy_func(E(x), E(\tilde{x})))$$

위와 같이 형태는 원본 입력 데이터와 변형된 버전에 대한 에너치 값의 차이를 줄이는 방식으로 학습되어 잠재 변수의 표현력을 줄이는 방식으로 모델의 일반화 성능을 높이는 것입니다. 


> ## Ⅱ. Reference.

1. [Meta Research : self-supervised-learning](https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)



<br><br>
---