---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

<!-- align, equation, matrix, array, theorem, proof -->


<!-- $$
\begin{align}
z_uz_v^T \approx & \text{graph의 u,v 에서 rankdom walk가} \\
&\text{함께 나타날 확률}
\end{align}
$$ -->

<!-- 
$$
\begin{align*}
\text{수식의 왼쪽 항} &= \text{오른쪽 항의 첫 번째 줄} \\
&= \text{오른쪽 항의 두 번째 줄}
\end{align*}
$$ 
\partial
$$\max\limits_{f}$$
-->

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅





## Graph as Matrix 

Page Rank (a.k.a Google Algorithm)

Web as a directed graph이다. 
웹 링크가 navigational
모든 웹페이지는 not equally "important"
따라서 아래와 같은 방법으로 웹 페이지에 순위를 메길것이다.

Link Anlysis Algorithms
PageRank
Personalized PageRank PPR
Random Walk with Restarts


Link as Votes
In-coming links, Out-goint links
모든 in links는 동등할까?중요한 페이지는 연결된게 더 많을 것이다.
중요한 페이지에서 "vote"sms ej wnddygkek.

각 링크는 원문 페이지에서 비례하는 중요도를 투표한다.

_Figure 4-1_

따라서 페이지의 중요도는 다른 중요한 페이지에서 가리켜 지면 $$_{pointed}$$ 중요하다.

$$r_j = \sum_{i \rightarrow j}\frac{r_i}{d_u}, d_i\text{ : 노드 i의 out-degree}$$

확률적 인접 행렬 M : j에서의 out-link $$d_j$$ 일때 $$M_{ij}=\frac{1}{d_j}$$이고 M의 열 합은 1이 되는 확률 행렬이 된다.
RAnk vector r $$r_i$$ : 페이지당 들어오는 i 페이지의 중요도 점수로 그 합은 1이다.

$$r=M \cdot r; r_j = \sum_{i \rightarrow j}\frac{r_i}{d_i}$$이다.

무작위 보행 연결

$$p(t)$$는 페이지의 확률분포이다.
$$p(t+1) = M \dcot p(t)$$
$$p(t+1) = M \dcot p(t) = p(t)$$가 되는 상태에 도달하고 해당 p(t)를 무작위 보행의 stationary distribution이라고 한다.

행렬의 eigenvector
eigenvetor centrality
$$\lambda c = Ac; c:\text{ eigenvector}; \lambda : \text{ eigenvalue}$$

$$1\cdot r = M \cdot r$$로 정의할 수 있고, rank vector r은 M의 eigenvector가 된다.\

어떤 벡터 u에서 시작하여 $$M(M(...M(Mu)))$$를 surfer의 장기 분포로 제한할 수 있다.
PageRank = Limiting distribution=principal eigenvector of M
r은 eigenvalue를 1로 가지는 M의 principal eigenvector가 된다.
이제 우리는 r을 효과적으로 풀 수 ㅂ있고 이러한 방법을 "Power iteration"이라고 부른다.

## PageRank : How to solve?

그래픵 n 노드가 주어졌을때 반복 절차를 사용할 수 있다.
$$\sum_i|r_i^{t+1}-r_i^t| < \epsilon$$으로 수렴할 때까지 반복한다.
그러면 각 노드의 page rank를 계산할 수 있따$$r_j^{t+1}=\sum_{i \rightarrow j}\frac{r_i^{(t)}}{d_i}$$

_Figure 4-2_
$$|x|_1 = \sum^N_1|x_1|$$은 L1 norm으로 유클리디안 이나 다른 vector norm으로 변경할 수 있다.

이러한 방법을 통해 세가지 의문점이 발생한다.
이것은 수렴하는가?
우리가 원하게 수렴하는가?
이 결과는 믿을만한가?

1. 몇 페이지는 dead end이다, out-link가 없다. 이러한 페이지는 "leak out"의 주요한 원인이 된다.
2. Spider traps : 모든 out-link가 그룹안에 있다.

수렴하는가는 (2)번 문제로 인해 문제가 되고, 원하게 수렴하는가는 (1)번 문제에서 찾아야하낟.

(2)번 문제를 해겨ㅑㄹ하기위해 확률 $$\Beta$$의 확률로 연결을 따르는 거고 $$1-\Beta$$의 확률로 다른 무작위 페이지로 넘어가는 것이다.
spider trap을 몇번의 단계로 teleport out으로 탐함한다.
(1)번 문제는 dead-end의 상황에서 무조건 무작위 teleport 하도록 한다.(각 확률은 1/N)

왜 이러한 텔레포트 전략이 문제를 풀 수 있는가?
Spider-traps은 문제가 아니지만 우리가 원하는 PageRank 점수가 나오지 않는다. 따라서 spider trap에 갖히지 않게 텔레포트를 하는 것이다.
Dead-end는 문제가 되는데, 행렬의 확률이 항상 무작위로 텔레포트 하는 것으로 풀 수 있다.

PageRank equation
$$r_j = \sum_{i \rightarrow j}\Beta\frac{r_i}{d_i}+(1-\Beta)\frac{1}{N}$$
Google Matrix G : $$G = \Beta M + (1-\Beta)[\frac{1}{N}]_{N \times N}$$
이렇게하면 Power metgod는 여전히 작동한다. ($$\Beta$$는 보통 0.8, 0.9로 설정한다.)

_Fgirue 4-3_

## Random Walk with Restarts and Personalized PageRank

user와 item으로 이루어진 이분으로 구성된$$_{bipartite}$$ graph가 주어졌을때 그래프의 Proximity를 계사한하는 것으로
아이템 Q, P가 유사한 유저들로 연결되어있다면 유저가 Q를 골랐을때 P를 추천하게끔한다.


PageRank : 노드들을 중요도로 순위를 메기는것
Personalized PageRank : S노드로 teleport하는 노드의 근접성을 Rank한다.
Proximity on graphs : 아이템 Q와 매우 관련된 아이템은 무엇인가? 
   - Random Walks with Restats : $$S={Q}$$ 시작 노드로 돌아가서 이동한다.

무작위 보행
query node Q가 주어졌을때 무작위 보행을 시뮬레이트한다.

_Figure 4-4_
쿼리 노드에서 시작하여 연결된 유저들이 가진 item의 방문 count를 1씩 늘리는 작업을 반복한다.

왜 이게 좋은 방법일까? 왜냐하면 유사도를 고려하기 때문이다.

- PageRank:
  - Teleports to any node
  - Nodes can have the same probability of the surfer landing:
    S = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]
- Topic-Specific PageRank aka Personalized PageRank:
  - Teleports to a specific set of nodes
  - Nodes can have different probabilities of the surfer landing there: 
    S = [0.1, 0, 0, 0.2, 0, 0, 0.5, 0, 0, 0.2]
- Random Walk with Restarts:
  - Topic-Specific PageRank where teleport is always to the same node:
    S = [0, 0, 0, 0, �, 0, 0, 0, 0, 0, 0]

## Matrix Factorization and Node Embeddings

가장 간단한 노드 유사도는 edge로 연결되어있으면 u,v는 유사하는 것이다.
이것은 $$z_v^Tz_u = A_{u,v}$$를 뜻하고 인접행렬 A의 구성이다. 그리므로 $$Z^TZ =A$$가 된다.

행렬 분해.
정확히 $$Z^TZ =A$$는 일반적으로 불가능하다.
그러나 Z를 근사하도록 학습은 가능하다.
$$\min\limits_{Z}||A-Z^TZ||_2$$
엣지들의 연결성을 노드의 유사도로 정의한 내적 곱 decoder는 A의 행렬분해와 동일하다.


## Matrix Factorization and Node Embeddings

![3-10](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/969257ba-0dcd-42ed-9f8c-c2eed291f60c)
_Figure 3-10_


노드 쌍 (u,v)에서 유사성인 $$z_v^Tz_u$$를 최대화하는 방법이 목적함수이다.

가장 간단한 노드 유사성은 edge로 연결이 되어있으면 노드 u,v를 유사하다고 가정하는 것인데
이것은 $$z_v^Tz_u=A_{u,v}$$로 (u,v) 는 그래프의 인접행렬 A의 항목이다.

그러므로 $$Z^TZ=A$$가 된다.


### MAtrix Factorization

임베딩 차원 d 노드의 수 n보다 매우작은 수 이다.

Exact factorization $$A = Z^TZ$$는 일반적으로 불가능하다.
그러나, Z 근사는 학습이 가능하다.

목적함수 : $$\min\limits{Z}||A-Z^TZ||_2$$
  - $$A-Z^TZ$$의 L2 norm Frobenius norm을 최소화하는 방식으로 Z를 최적화할 수 있다.
  - softmax 대신 L2를 사용하는데 $$A$$를 $$Z^TZ$$로 근사하는 목표는 같다.

edge의 연결성으로 정의된 노드의 유사도를 내적곱decoder는 A를 matrix factorization하는 것과 동일하다.

DeepWalk와 node2vec은 무작위 보행에 기반한 더 복잡한 노드 유사도를 가진다.
DeepWalk는 아래와 같은 행렬표현으로 matrix factorizaation된ㄷ,

![3-11](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/dc4dd61d-6ed2-4ff8-99c0-637463823217)
_Figure 3-11_


node2vec은 더 복잡한 행렬 일지라도 행렬 분해로 형식화할 수 있는데, 아래 논문에서 자세한 사항을 알아 볼 수있다/
[Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec, WSDM 18](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf)




Limiatation 1.

행렬 분해와 무작위 보행으로 얻은 노드 임베딩의 제한 사항으로
훈련 집합에 없는 노드의 임베딩은 얻을 수 없다.

Limiatation 2.
구조적인 유사도를 수집할 수 없다. 구조적으로 유사한 노드일지라도 매우 다른 임베딩을 얻게 된다.
Deepwalk와  node2vec은 구조적 유사도를 수집할 수 없다.


Limiatation 3.

node, edge 그리고 graph의 특징을 활용할 수 없다.

Feature vector : 단백질간 상호작용 그래프에서 단백질의 성질 등
DeepWalk,. node2vec 임베딩은 이러한 노드 특징을 포함하지않는다.

이러한 제한의 해결방법은 깊은 표현 학습과 GNN이다.




<br><br>
---




































lkadjsgkljd;sarhglkjdjlfkgjlkdjfglk;djsfl;kgfjlsdk;fjglk;sjd;lfkgjsl;dkjf