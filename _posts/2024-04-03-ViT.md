---
title: '[Paper]ViT : Vision Transformer'
author: east
date: 2024-04-03 00:00:00 +09:00
categories: [Paper, CV]
tags: [Paper, CV, ViT]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---
 
ViT는 기존의 CNN을 대체하여 이미지에 트랜스포머 아키텍처를 적용하는 milestone이기에 관련된 내용을 한번 다루어 보겠습니다.

트랜스포머 아키텍처가 사실상 NLP 작업의 표준이 되는 동안, CV에서는 제한적인 적용을 보였습니다. 이미지의 전체적인 구조를 유지하면서 어텐션 매커니즘을 컨볼루션 네트워크와 결합하거나 대체하는 방식으로 CNN에 대한 의존성이 줄어들었습니다. 이미지 패치 시퀀스의 적용된 기본적인 트랜스포머 구조는 이미지 분류 작업에서도 매우 좋은 성능을 보였습니다.

> ## Ⅰ. Introdiction



기존의 트랜스포머를 이미지에 적용하기 위해 이미지를 패치로 분할하고, 이 패치들의 선형 임베딩을 시퀀스로 만들어 입력으로 사용합니다. 이미지 패치는 NLP에서의 토큰(단어)처럼 다뤄집니다. 

ImageNet과 같이 중간 규모의 데이터셋에서 강한 규제 없이 학습했을 때는 ResNet보다 낮은 성능을 보여주었습니다. 이는 트랜스포머가 CNN이 가지고 있는 Inductive bias인 equivariance와 locality를 일반화하지 않기 때문에 데이터가 충분하지 않은 경우 이러한 결과가 나올 수 있다고 합니다.

그러나 트랜스포머의 efficient와 scalability 덕분에 이러한 상황을 극복할 수 있는데, 큰 규모의 데이터셋에서 사전학습을 통해 효과적으로 학습하고 전이 학습할 때 좋은 성능을 보인다고 합니다.

> ## Ⅱ. Method

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/c962152e-e3a1-4be6-be0d-6dd4779706ab)
_Figure 1 : ViT 모델 개요_

기존의 트랜스포머는 1D의 토큰 임베딩 시퀀스를 입력으로 받습니다. 그러나 2D의 이미지를 다루기 위해 이미지 x를  $$x \in \mathbb{R}^{H \times W \tims C}$$에서 $$x \in \mathbb{R}^{H \times (P^2 \dot C)}, C : channels, (P,P) : Patches$$로 변경했습니다.

BERT와 유사하게 [class] 토큰을 활용하여 `학습 가능한 임베딩`($$z^0_0 = x_{class}$$)을 패치 시퀀스 앞에 추가했습니다. 즉, 트랜스포머 인코더의 출력 상태($$z_L^0$$)가 이미지 표현 y로 사용될 수 있도록 합니다.이러한 분류헤드$$_{classification-haed}$$는 사전학습과 미세 조정동안 $$z_L^0$$에 연결됩니다.

추가적으로 `1D의 포지션 임베딩이 연결`되어 위치 정보를 제공합니다.

![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/56022819-a31f-4b96-a208-2fed6a852217)
_Figure 2 : 트랜스포머 모델을 수식으로 나타낸 형태입니다._

`유도 편향`$$_{inductive-bias}$$가 CNN에 비해 ViT에서 적다고 이야기했습니다. CNN은 지역성과 2D 지역 구조, 변환 동등성$$_{translation-equivariance}$$의 성질들을 각 층에서 포함하고 있습니다. 그러나 ViT에서는 MLP 레이어들이 지역적이지 않으며 셀프어텐션은 전역적입니다. 즉, 2D 이웃 구조는 매우 제한적으로 사용됩니다.

translation equivariance
: 입력이 변환되었을때, 출력도 동일하게 변환되는 성질

따라서, 위의 문제를 해결하기 위해 `이미지를 패치로 분할`하고, 미세 조정 시에 다른 해상도의 이미지에 대한 위치 임베딩이 조정됩니다. 그리고 2차원 위치 임베딩을 자체적으로 사용하지않고, 상대적인 공간 관계를 학습합니다.

`하이브리드 구조`로 원본 이미지의 패치 대신 CNN의 feature map으로부터 형성된 입력시퀀스를 대신사용하는 방법입니다. 패치 임베딩 투영 E는 CNN의 feature map으로부터 추출된 패치에 적용됩니다. 특별한 경우, 해당 패치들은 1x1 공간 크기를 가지며 이는 입력 시퀀스가 단순히 feature map의 공간 차원을 펼침으로써 얻을 수 있고, 트랜스포머 차원으로 투영될 수 있음을 의미합니다.

> ## Ⅲ. Experiments


![4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/ffe4c68e-af54-493f-b79b-cfce06efb6a3)
_Figure 4 : (좌) 선형 임베딩 필터 (중앙) 위치 임베딩 유사도 (우) 헤드와 네트워크 깊이 별 수용역역 크기_

Figure 4의 가운데 그림을 보면 가까운 위치에 있는 패치들은 서로 비슷한 위치 임베딩 값을 가지고, 같은 행-열에 존재하는 패치들이 유사한 임베딩을 확인할 수 있습니다.

셀프어텐션은 각 토큰들 간의 유사도를 계산하는 매커니즘으로 어텐션 가중치를 통해 해당 픽셀이 얼마나 멀리있는 픽셀까지 고려하는 지를 나타내는 가를 나타내는 평균 어텐션 거리를 계산합니다. 

Figure 4의 우측 그림을 보면 이러한 어텐션 거리는 CNN에서 수용영역$$_{receptive-field}$$의 크기와 유사합니다. 그리고 거리가 높은 헤드들을 보아 정보를 전역적으로 통합하는 능력을 사용하는 것을 보이며 거리가 작은 헤드들의 수는 하이브리드 모델에서 감소해 앞서 말한 CNN의 컨볼루션 층과 유사하다는 것을 알 수 있다고 했습니다.

![3](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/3cd6cd4d-7211-4f7f-89eb-a82ec0a96a14)
_Figure 3 : 입력 이미지에서 출력된 어텐션의 표현 예시_

BERT에서 사용되는 MLM처럼 masked patch prediction을 self-supervision을위해 수행했는데 supervised pre-training보다는 4% 성능이 뒤떨어졌다고 나타냈습니다.

> ## Ⅳ. Conclusion

이미지에 트랜스포머 인코더를 적용함으로써 큰 규모의 데이터셋에서 사전학습이 가능해져 저렴한 비용으로 좋은 성능을 내는 길이 열리게 되었습니다.

ViT가 소개된 이후 컴퓨터 비전영역에서 기존 모델들을 뛰어넘는 성과를 보여주었고 CLIP과 같은 멀티모달 영역에서도 다양하게 사용되었습니다.

> ## Ⅴ. Code

간단하게 psuedo 코드를 작성해보았습니다.

```python
class ViT:
    images # [Batches, Channels, Height, Width]
    # CNN과 결합한 하이브리드 모델
    image_embed = conv2d(x) # [B, E, H/P, W/P]
    image_flatten_embed = flatten(image_embed) # [B, E, N] -> [B, N, E]
    input_embed = image_flatten_embed + position_embed # [B, N, E]
    encoder_embed = transformer_encoder(input_embed) # [B, N, E]

```

> ## Ⅵ. REFERENCES

1. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)

<br><br>
---