---
title: '[Study]Chapter 2. Understanding Multimodal Data'
author: east
date: 2024-03-24 11:00:00 +09:00
categories: [Study, Multimodal]
tags: [Study, Multimodal]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

멀티모달에서는 아래와 같이 두 가지 주요 과제가 있습니다.

1. 입력을 수적 표현을 어떻게 하는가
2. 머달리티를 어떻게 결합하는가

그 중 첫번째 과제를 다루기 위해 일반적인 딥러닝에서는 이미지나 텍스트를 잠재공간의 벡터로 임베딩하여 수치적으로 표현합니다. 이렇게 수치적인 표현을 가능하게 하려면 데이터셋이 필요하고, 언어모델과는 다르게 Visual-Language 작업에서는 고품질 이미지의 설명을 요구하고 상대적으로 극히 드뭅니다. 

따라서, 해당 데이터셋에 대한 내용을 알아보도록 하겠습니다.

> ## Ⅰ. Introduction

VLM(Visual-Language Model)은 텍스트와 이미지 쌍을 학습하기에 이미지에 해당하는 다수의 텍스트를 포함합니다. 다른 모달리티에 비해 규모가 작고, 대기업들은 여전히 비공개로 유지하려고 하고 있고 이는 재현성을 방해합니다. 심지어 CommonCrwal을 사용하여 이미지와 대체 텍스트를 추출 해도 대체 텍스트는 종종 이미지 내용을 충분히 설명하지 못합니다. 

그럼에도 불구하고 널리 사용되는 데이터셋으로 Microsoft COCO, Visual Genome(VG), Conceptual Captiosn(CC), Filckr30k, LAION-400M,5b, WebImageText(WIT) 등의 데이터셋이 존재한다고 합니다.

또한, VLM이 성능측정을 위한 벤치마크로 Flickr30k Entities, COCO Captions, VCR, VQA, GQA(General Question ANswering), NLVR(Natural Language for Visual Reasoning) 등이 있다고 합니다.

> ## Ⅱ. Dataset

> ### ⅰ. COCO

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/cb94119c-5e38-4c14-b8e2-d43b8ee70e37){: .w="500"}
_Figure 1 : MS COCO 데이터셋에서 주석이 달린 이미지 샘플_

Microsoft에서 scene understanding(장면 이해)의 세가지 주요 문제인 (1) 객체가 중앙 (2) 여러 객체를 포함하는 다양한 상황 (3) 모든 객체를 정확히 분리하여 라벨링 들을 해결하기 위한 새로운 대규모의 object detaction, segmentaion, captioning 데이터셋을 만들었습니다.

작업자들은 91개의 카테고리와 11개의 슈퍼카테고리 목록을 받아 계층적인 분류를해 카테고리 분류의 시간을 줄이고, 이후에는 작업자들이 인스턴스 세분화도 해야 했으며 마지막으로 데이터셋의 각 이미지에는 다섯 개의 캡션을 추가해 COCO(Microsoft Common Objects in Context)라고 불렀습니다. 특히, COCO는 이미지-텍스트 쌍 덕분에 CV뿐만 아니라 멀티모달 모델에서도 사용할 수 있는 데이터셋이라고 합니다.

> ### ⅱ. Conceptual Captions

![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/5007acdb-4422-4115-9a8c-aed729ba476b){: .w="500"}
_Figure 2 : CC 데이터셋에있는 이미지와 이미지 설명 예제. Alt-text에서 전처리를 자동화하여 뽑아낸 CC text._

위의 COCO 데이터셋보다 더 많은 이미지 규모의 이미지 캡션 데이터셋을 소개합니다. 해당 데이터셋은 여러 웹페이지에서 이미지 캡션을 추출하고 필터링하여 구축되었습니다.

MS-COCO 데이터셋에는 그림은 없고 사진만 있어 다양성이 떨어져 높은 상관관계를 가지게 되어 잘못된 결과가 발생하고 큰 규모의 모델들이 사용하기에는 데이터 수가 부족 하다는 단점들이 존재했습니다.

따라서, Conceptual Captions이라는 약 330만 개의 이미지와 캡션으로 구성된 데이터셋을 구축하게 되었으며 웹에서 수집되어 다양한 스타일을 지니고, 이미지-텍스트 쌍을 전처리하는 파이프라인을 구성해 깨끗함, 정보성, 유창성, 학습 가능성의 균형을 달성했다고 합니다.

> ### ⅲ. Visual Genome

![4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/6346e2f8-9b42-42d4-9185-cc0dc3267629){: .w="500"}
_Figure 3 : Visual Genome 데이터셋은 각 이미지의 특정 부분을 설명하는 영역이 포함되어 있습니다. 각 영역은 객체, 속성, 관계의 지역 그래프 표현으로 변환되어 위와 같이 장면 그래프로 결합됩니다._

Visual Genome은 mutli-choice setting에서 VQA 데이터를 포함하고 있습니다. 다른 VQA 데이터셋에 비해 Visual Genome은 6가지 질문 유형(What, Where, When, Who, Why, How)에 대해 더 균형 잡힌 분포를 나타내고, 이미지의 특정 부분을 설명하는 영역 설명이 포함되어 있습니다.

> ### ⅳ. Flickr30k

![3](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/c6ed0005-5280-45db-a89d-4d81a417991c){: .w="500"}
_Figure 3 : 두개의 이미지에 해당하는 다섯가지의 cations_

해당 데이터셋은 한 장의 이미지가 있을 때, 그 이미지를 설명하는 문장을들을 포함하는 "문장 단위 이미지 설명(sentence-based image description)" 데이터셋의 대표적인 예시입니다.

Flicker는 야후의 온라인 사진 공유 커뮤니티 사이트로 Flickr에서 수집된 31,000개의 이미지와 FIgure 3처럼 5개의 문장을 포함하고 있습니다.


> ### ⅴ. LAION-400M,5B


![5](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/023a337b-f46c-40c6-8080-9924328c9e14){: .w="500"}
_Figure 5 : CLIP embedding을 사용한 LAION-5B 예제로 LAION-5B 예제입니다. CLIP 임베딩을 사용하여 LAION-5B에서 가장 가까운 이웃 검색의 샘플 이미지입니다. 이미지와 캡션(C)은 쿼리(Q)에 대한 첫 번째 결과입니다._

LAION-400M은 4억 개의 이미지-텍스트 쌍으로 구성되어 있습니다. Common Crawl을 사용해 모든 HTML IMG 태그에서 alt-text 속성을 파싱했는데, 때때로 매우 정보가 부족할 수 있어 이미지와 alt-text의 임베딩을 CLIP으로 계산하고 유사도가 0.3 이하인 모든 샘플을 제거했습니다.

2022년 3월 말, LAION 팀은 LAION-400M보다 큰 LAION-5B 데이터셋을 출시했으며 현재 가장 큰 공개적으로 접근 가능한 이미지-텍스트 데이터셋이라고 합니다.

COCO와 비교하여 규모는 비교가 안되지만 COCO의 텍스트 데이터는 고품질로 수집되어 여전히 사용되고 있다고 합니다. 

해당 데이터셋을 구축하는 과정에서 CLIP 모델이 사용되었기 떄문에 잠재적으로 CLIP과 관련된 편향을 지닐 수 있다고 합니다. 또한, LAION-400M, 5B 데이터셋이 민감한 데이터들도 포함하고 있어 유의해야 한다고 합니다.

> ### ⅵ. Wikipedia Image Text

![6](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/215caa95-5b82-410a-b031-739ff49d01f1){: .w="500"}
_Figure 6 : 위키미디어 커먼즈를 통해 하프 돔, 요세미티, 캘리포니아의 위키피디아 페이지가 WIT에서 추출되어 제공된 다른 필드의 예제입니다._

Wikipedia Image Text (WIT) 데이터셋으로 대부분의 데이터셋은 영어로만 제공되어 multilingual 멀티모달 연구를 방해합니다. 따라서, 이를 해결하고자 위키백과 기사와 위키미디어 이미지 링크를 사용해 이미지와 관련된 여러 다른 텍스트들을 추출했습니다. 추가로, 고품질 이미지-텍스트 연관성을 유지하기 위한 엄격한 필터링이 사용되었다.

위키백과의 multimodal 커버리지로 인해, 그들은 108개 언어 각각에서 12K 예제 이상과 53개 언어에서 100K 이미지-텍스트 쌍 이상을 포함하는 독특한 다언어 커버리지를 제공한다.

또한 위키백과의 편집, 검증 및 수정 메커니즘을 활용하여 고품질 기준을 보장할 수 있습니다. 마지막으로 사람이 WIT 데이터셋의 품질을 검증해 무작위로 샘플링된 이미지-텍스트 연관성을 98.5%가 긍정적으로 평가되었다고 합니다.

> ## Ⅲ. Benchmarks for multimodal

다양한 데이터의 형식과 넓은 작업영역을 위하 정량적인 평가가 필요하기 떄문에 멀티모달의 핵심 과제 들을 공정하게 평가하기 위한 벤치마크들을 몇가지 소개하겠습니다.

> ### ⅰ. COCO Captions

![1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/0a4f705a-a01d-449a-acb0-9041b425ea80){: .w="500"}
_Figure 1 : COCO Caption_

COCO Captions는 COCO 데이터셋의 일부로 이미지 데이터에 캡션을 추가한 것입니다. 해당 데이터셋은 이미지 캡셔닝, 텍스트 생성 등의 벤치마크로 사용되며 CoCa 등의 모델에 사용되었다고 합니다.


> ### ⅱ. Flickr30k Entities


![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/ded34afd-a8c9-4baf-a874-2d322a67bea7){: .w="500"}
_Figure 2 : 각 이미지에서 개체와 바운디 박스가 같은 색으로 표시 됨._

Flickr30k는 이미지-텍스트의 쌍으로 이루어진 데이터이며 캡션에 나온 객체와 이미지의 바운딩 박스가 추가적으로 설정되어 있습니다. 이는 이미지 내에서 텍스트로 언급된 객체를 지역화하는 새로운 벤치마크로 사용된다고 합니다. VisualBERT 등에 사용되었다고 합니다.


> ### ⅲ. NLVR(Natural Language for Visual Reasoning)

![3](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/daa40f89-279c-466c-bf61-2a4eb71c6fbc){: .w="500"}
_Figure 3 : NLVR2 데이터셋의 한 예로 각 캡션이 두 이미지와 짝을 이룸._

컴퓨터가 이미지에 대한 자연어 문장을 해석하고, 해당 문장이 이미지의 내용을 정확하게 묘사하는지 여부를 판단할 수 있는 능력을 평가하는 시각 추론에 대한 벤치마크입니다. 즉, 모델이 이미지를 이해하고 해당 캡션에 대한 참,거짓 분류 성능을 평가합니다. VisualBERT나 CoCa 같은 모델에 사용되었다고 합니다.


> ### ⅳ. VQA

![Untitled (8)](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/f3c9145d-137d-417e-b2d9-45ffd894c3ac){: .w="500"}
_Figure 4 : VQA 데이터셋의 예로 추상적인 질문을 포함한다._

VQA는 이미지와 질문이 주어졌을때, 해당 질문에 맞는 올바른 답변을 생성하는 작업입니다. 이러한 작업은 이미지 캡셔닝보다 더 사람과 유사한 "AI-complete" 작업입니다.

해당 데이터셋은 COCO에서의 이미지와 추상적인 장면을 포함하는 대규모 데이터셋입니다. 데이터셋의 구성은 각 이미지나 장면마다 세 개의 질문과 여러명의 참가자가 작성한 답변으로 이루어져있습니다.

위와 같은 개방형 질문에 답을 하기 위해서는 세부적 인식, 객체 감지, 활동 인식, 지식 베이스 추론, 상식 추론 등의 능력이 필요하며 고차원적인 추론을 위해 이미지 분석 필요성을 제거하는 추상 장면$$_{scene}$$을 추가하였습니다. 해당 질문은 단순한 질문보다는 상식을 필요로 하면서도 상식만으로는 답변 할 수 없는 질문들을 선호했습니다. ex. “사진의 저 동물은 어떤 소리를 낼 것 같은가?”

> ### ⅴ. GQA(General Question ANswering)

![4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/e826bb2c-a773-45f0-876b-4181b2521fc6){: .w="500"}
_Figure 5 : Examples of questions from the GQA dataset, 이미지와 질의, 응답에 이어 semantic 그래프(객체,속성,관계)가 추가적으로 주어짐._

VQA는 알고있는 정보로만 문제를 풀어 실제 이미지를 이해하는 능력을 기르지 못한다는 단점이 존재합니다. 이를 해결하고자 만들어진 데이터셋으로 Visual Genome의 장면 그래프 구조를 활용하여 이미지의 객체, 속성, 관계에 대한 정보를 제공해 데이터의 정보를 제공합니다. KaKao Brain과 같은 모델에 사용되었습니다.



> ### ⅵ. VCR

![Untitled (7)](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/894aeb72-b294-46fb-97d6-3c32c7837d60){: .w="500"}
_Figure 6 : VCR dataset의 한 예로 이미지에서 각 객체에 대한 정보와 다중 질문과 답변으로 그러한 이유도 포함되어있습니다._

시각 이해 작업은 시각 상식 추론(Visual Commonsense Reasoning, VCR)으로 형식화될 수 있고 이에 관한 데이터셋을 소개합니다. 즉, 이미지에서 객체를 인식하는 것을 넘어(COCO Captions 등)이미지 내의 사건이나 상황에 대한 깊은 이해와 추론 능력이 필요합니다. 

해당 데이터셋은 영화 장면에서 유래한 다중 선택형 QA 문제로 구성되어 이미지 객체 정보, 다중 QA 및 해당하는 근거(R)로 구성되어 있습니다 . 이러한 벤치마크는 VisualBERT, GPT4RoI 등에 사용되었습니다.

> ## Ⅳ. Additional

위의 데이터셋들은 대부분 VLM을 위한 텍스트-이미지 데이터셋임으로 다른 형태의 데이터 구조를 간략하게 소개하겠습니다.

> ### ⅰ. IMAGEBIND

해당 모델은 이전 Facebook 이었던 Meta에서 발표현 논문으로 Image, Text, Audio, Depth, Thermal, IMU 데이터를 묶어 사용한 모델을 소개합니다. 

![9](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/ea16e9cd-e856-4fb7-9516-96210dd14b74){: .w="500"}
_Figure 7 : IMAGEBIND overview_

여러가지 다른 모달리티를 같은 공간으로 표현하여 유사한 내용을 포함하는 모달리티를 가깝게 표현하는 것이 목표입니다.

> ### ⅱ. VisualCOMET

![8](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/5073e222-2067-412b-87f2-8916ac2a4e1d){: .w="500"}
_Figure 8 : Task Overview_

이미지 내 현재 사건에 관한 설명과 배경을 통해 사건에 대한 과거, 현재, 미래에 대한 상식 추론을 생성하기 위해 구축된 프레임워크입니다.



> ## Ⅴ. REFERENCES

1. [Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)
2. [Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](https://aclanthology.org/P18-1238.pdf)
3. [Flickr30k : From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions](https://shannon.cs.illinois.edu/DenotationGraph/TACLDenotationGraph.pdf)
4. [Visual Genome : Connecting Language and Vision Using Crowdsourced Dense Image Annotations](https://arxiv.org/pdf/1602.07332.pdf)
5. [LAION-5B: An open large-scale dataset for training next generation image-text models](https://arxiv.org/pdf/2210.08402.pdf)
6. [Wikipedia-based Image Text Dataset](https://github.com/google-research-datasets/wit)
7. [Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models](https://arxiv.org/pdf/1505.04870.pdf)
8. [Microsoft COCO Captions: Data Collection and Evaluation Server](https://arxiv.org/abs/1504.00325)
9.  [NLVR : A Corpus for Reasoning About Natural Language Grounded in Photographs](https://arxiv.org/abs/1811.00491)
10. [VQA](https://visualqa.org/)
11. [GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](https://arxiv.org/abs/1902.09506)
12. [VCR : Visual Commonsense Reasoning](https://visualcommonsense.com/)
13. [IMAGEBIND: One Embedding Space To Bind Them All](https://arxiv.org/pdf/2305.05665.pdf)
14. [VisualCOMET: Reasoning about the Dynamic Context of a Still Image](https://arxiv.org/abs/2004.10796)
