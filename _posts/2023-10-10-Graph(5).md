---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)


## Message Passing and Node Classification

네트워크에서 몇몇의 노드에 label이 달려있으면 다른 노드들에게는 label을 어떻게 할당해야하는가?

_Fugyre 5-1 : 좌측 그림과 같이 label이 지정된 몇몇의 노드가 주어졌을때, label이 없는 노드들의 label을 예측한다. 이러한 분류를 semi-supervised 라고 부른다._

네트워크 안에서 연관관계가 존재한다. 다른말로 유사한 노드들은 연결되어있다. 주된 concept는 단체의 분류로 같은 네트워크의 모든 노드에 label을 부여하는것.

세가지 기술을 볼 것이다. relational classification, iterative classification, belief propagation

네트워크안에서 개개인의 행동은 상관되어있다.
상관 : 근처에있는 노드들은 같은 색상을 가지고 있다.

동종 선호$$_{Homophily}$$는 각각의 유사한 것들끼리 연관되고 묶이도록 되는 경향이 있다.
ex) 같은 취미를 가진 사람들은 동종 선호로 인해 더 가까운 연결 관계를 가진다. 

영향$$_{Influence}$$는 소셜 연결은 사람의 개인의 성질에 영향을 준다.
ex) 같은 장르를 좋아하는 사람에게 어떤 영화를 추천해준다

우리는 네트워크에서 상관관계가 어떻게 영향을 주는게 노드 라벨을 예측하는데 도움을 주는가?

motivation 1.
Guilt-by-association : X노드와 연결되어있따면 X가 가지고있는 것들을 좋아한다.
Motivation 2.
v의 특징, v 이웃들의 라벨, v 이웃들의 특징에 의존하여 라벨을 분류해야한다.
semi-supervised learning
그래프와 라벨이 달린 몇개의 노드가 주어졌을때 라벨이 달리지 않은 노드들의 라벨을 찾아야한다. 그리고 동종 선호가 네트워크에 존재한다고 가정한다.
목표로 라벨이 달리지않ㄴ은 노드들을 얼마나 각 class같은지 예측하는 것이다.

Approach
Collective classification
문서 분류, 연설 태깅 부분, 연결 예측, 광학문자인식, 이미지 데이터 segmentaion, 센서 네트워크 entity resolution, 스팸 또는 fraud 탐지

상관관계를 이용하여 상호 연결된 노드들을 동시에 분류하는 것.
확률 framework
마코브 가정 $$P(Y_v) = P(Y_v|N_v)$$ v의 라벨은 노드 v의 이웃들에 의존한다.

 Collective classification은 아래와 같은 3단계를 포함한다.
Local classifier
    초기 라벨을 배정 : 노드의 속성과 특징을 가지고 라벨을 예측, 기존의 분류 작업, 네트워크 정보는 필요 없음
relational classifier
    노드들간의 상관관계 수집 : 분류기를 해당 노드의 이웃들의 속성이나 라벨을 가지고 학습 시킴. 여기에서 네트워크 정보가 사용됨
collective inference
    네트워크를 통해 상관관계를 전달 : 각노드에 관계된 분류기를 반복적으로 적용 시킴, 이웃 라벨이 일관성 없음이 최소화되도록 반복한다. 네트워크 구조가 마지막 예측에 영향을 미친다.

문제 설정.
네트워크와 모든 특징이 주어졌을때 $$P(Y_v)$$를 찾아야한다.


### Relational classifiers

Basic Idea
- Node v의 class probability $$P(Y_v)$$는 이웃 노드들의 class probability의 가중 평균과 같음.
  - 인접 행렬

_Figure 5-2_
$$A_{u,v}$$는 v와 u사이 edge의 가중치가 됨.


Step
label이 지정되어 있지 않는 노드들의 확률 값을 0.5로 초기화함.
회색 노드들을 하나씩 위 수식을 기반으로 업데이트함.
_Figure 5-3_

2번 업데이트 과정을 노드가 수렴할떄까지 반복진행
_Figure 5-4_

문제점 
이와 같은 과정은 주변 노드의 구조적 정보와 라벨을 활용하고 있지만, 노드의 특징은 활용하지 못한다는 단점이 있음, 즉 주어진 정보를 충분히 활용하지는 못하는 방법론
모델이 노드의 feature 정보를 사용할 수 없음, 위 수식은 수렴이 보장되지 않음

### Iterative classification 

Main Idea
- 이웃 노드들의 labels $z_v$와 feature $f_v$를 input으로 사용하여 node v의 label을 예측
-  Node feature를 고려하지 않는 relational classifiers의 한계를 극복

How to work? : 2개의 classifier를 학습
1. $\phi_1(f_v)$ : node feature vector $f_v$ 기반 node label 예측 (local classifier)
2. $\phi_2(f_v,z_v)$ : 이웃 노드들의 label 정보가 있는 summary $z_v$와 node feature vector $f_v$를 이용하여 node label을 예측
    - 이때 summary $z_v$는 벡터표현으로 주변 노드들의 label 정보를 포함하는 많은 표현 방법들이 활용 가능함 (ex. most common label, number of different labels, … )

### Steps
Phase 1.
- **Train :** classifier는 SVM, NN, linear 등 무엇이든 가능함
    1. 먼저, training dataset에 주어진 feature vector $f_v$를 이용하여 $Y_v$를 예측하는  $\phi_1(f_v)$를 학습시킴
    2. 그리고 training dataset 기반 $f_v$와 summary $z_v$를 이용하여 $Y_v$를 예측하는 $\phi_2(f_v,z_v)$를 학습함
    

_Figure 5-5_
Phase 2.
- **Test**
    
    _Figure 5-6_
    
    1. $\phi_1(f_v)$를 이용하여 initial label을 예측함
        - feature vector는 변하지 않기 때문에 초기 한 번만 사용
    2. 모든 node에 label이 부여되었음으로, summary $z_v$를 계산함
    3. $\phi_2(f_v,z_v)$를 이용하여 $Y_v$를 예측함
    4. 2번과 3번 과정을 수렴하거나 특정 횟수만큼 반복함


### Loopy Belief Propagation

신뢰 전달$$_{Belief-Propagation}$$은 동적 프로그래밍 접근법으로 그래프에서 확률적 쿼리를 대답한다.

<aside>

💡 **Main Idea**
- “I (node v) believe you (node u) belong to class 1 with likelihood … “ → **belief**
- 각자의 이웃 노드로부터 정보를 받다보면, 먼 노드까지 정보를 전달할 수 있음 (passing messages)
- 의견이 도달하면 마지막 신뢰를 계산할 수 있따.

_Figure 5-7_

메시지 패싱을 트리 구조 그래프로 변환하여 수행할 수도 있다. 메시지를 잎에서 root로 전달하는 순서로 정의할 수 있다.

</aside>



### Notation



- $\psi$ : Label-label potential matrix로 node 와 이웃 node 사이의 dependency matrix임
    - node i가 state $Y_i$에 있을 때, node j가 state(class) $Y_j$에 속할 확률과 비례한 i&j 의 correlation matrix라고 볼 수도 있을 것 같음
- $\phi(Y_i)$ : Prior belief, node i가 state $Y_i$에 속할 확률
- $m_{i \to j}(Y_j)$ : node j의 state $Y_j$에 대한 node i의 message
- $L$ : 모든 classes/labels의 집합

_Figure 5-8_


### Steps

1. 모든 messages를 1로 초기화 함
2.  각각의 노드에 대해 messages를 업데이트를 반복
    
    __Figure 5-9__
    
3. 2번 과정을 통해 messages가 수렴하면 아래와 같이 특정 노드에서 어떤 label을 가질 belief를 계산할 수 있음
    
    _Figure 5-10_
    
만약 그래프에 cycle이 존재한다면? 다른 서브 그래프에 있는 메시지는 더이상 독립적이지 않다 그러나 BP를 실행할 수 있지만 메시지는 루프 안에서 계속 전달 될것이다. Belief가 수렴하지 않을것이다.
그러나 실제로 Loopy BP는 여전히 많은 가지를 포함한 복잡한 그래프에서 heuristic에 좋다.
_Figure5-11_

메세지는 계속 돌고 돌것이다. 헤딩 variables가 T라고 확신하면서 계속할 것이다.
BP가 이 메시지를 variable이 T라는 분리된 증거로 잘못다룬다.
이러한 두 메시지를 독립적인 것처럼 곱할 것이다.
그러나 그래프에서 독립적인 부분에서 오는 것이 아니다.
하나는 다른 하나에서 영향을 받는다(cycle을 통해서)

$$

| Advantages | Challenges |
| --- | --- |
| - 구현하기 쉬움 <br> - 어떤 graph model에도 적용할 수 있음 | - 수렴이 보장되지 않음 <br> - 특히 closed loops가 많을 때 수렴이 어려움 |




<br><br>
---