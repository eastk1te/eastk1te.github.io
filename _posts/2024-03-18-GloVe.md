---
title: '[Paper]GloVe'
author: east
date: 2024-03-18 18:00:00 +09:00
categories: [Paper, NLP]
tags: [Paper, NLP]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

2014년에 나온 GloVe[^2](Global Vectors for Word Representation)이라는 논문을 통해 단어의 의미를 벡터 공간에 임베딩 하여 단어 간의 유사성과 관계를 표현할 수 있는 Word2Vec과 같은 기존의 기법과 마찬가지로 최근 자연어 처리 분야에서 큰 성공을 거두고 있습니다.

단어를 벡터 공간의 표현으로 변환하는 방법은 단어의 fine-grained semantic 정보를 수집하고, 벡터 연산을 통해 문법적으로 표현 가능하나 이해하기는 어려울 수 있습니다.

> ## Ⅰ. Introduction

단어 벡터 표현 방법은 벡터 사이의 거리나 각에 의존하여 본질적인 품질$$_{intrinsic quality}$$을 평가하는 주된 방법입니다.

이러한 방법은 특정 의미가 있는 차원을 표현하는 모델을 선호합니다. 따라서, 아래의 주된 두가지 모델은 단어 벡터를 학습하는것에 친숙하다.

1. global matrix factorization(ex. LSA)
2. local context window(ex. skip-gram)

LSA는 효율적으로 확률적인 정보를 활용하지만 단어 유추$$_{analogy}$$에서는 낮은 성능을 보입니다. 이는 LSA가 단어 벡터 공간 구조를 sub-optimal하게 인식하기 때문입니다.

Skip-gram 같은 경우는 유추 작업에서의 성능은 더 낫지만 별도의 지역적인 범위의 문맥만 학습하기 떄문에 전체적인 말뭉치의 정보를 활용하지 못합니다. 

따라서, 해당 논문에서는 새로운 global log-bilinear 회귀 모델로 위의 두 방법의 이점을 결합합니다. 오히려 큰 말뭉치에서는 전체적인 희소행렬 또는 독립적인 context window보다 효과적으로 단어간의 상호발생 행렬에서 non-zero 요소만을 학습하여 통계적인 정보를 활용합니다. 

> ## Ⅱ. Related Work

> ### Matrix Factorization Methods

저차원의 단어 표현을 생성하는 행렬 분해 방법은 LSA와 같은 뿌리를 가집니다. 이러한 방법은 낮은 차원의 근사치를 활용합니다. 큰 행렬로부터 말뭉치에 대한 통계적 정보를 수집하여 분해하는데, 여기서 수집되는 정보는 행렬에 따라 달라집니다. LSA의 경우 term-dcoument 행렬이고, HAL(HyperSpace Analogue to Language)의 경우는 term-term의 형태입니다.

> ### Shallow Window-based method

해당 방법은 지역 문맥 범위(window) 이내의 예측을 돕는 단어 표현을 학습하는 것입니다. 최근 CBOW와 Skip-gram가 그 예시입니다.

행렬 분해 방법과 다르게 해당 방법은 말뭉치의 상호발생 행렬을 직접적으로 활용하지 않지만 전체 말뭉치에 걸쳐 문맥 창을 살핍니다. 그러나 이는 방대한 데이터에서는 반복의 한계가 존재합니다.

> ## Ⅲ. The GloVe Model

global corpus 통계를 모델을 통해 직접적으로 수집하기 때문에 Global vector 즉, Glove라 지었습니다.

$$\begin{equation}
X_i = \sum_kX_{ik}, P_{ij} = P(j\|i) = \frac{X_{ij}}{X_i}
\end{equation}$$

단어간 상호발생 행렬을 X로 두고, word j의 발생확률은 $$P_{ij}$$가 됩니다.

![1](https://github.com/eastk1te/P.T/assets/77319450/bb7b3c76-17f1-45a4-88f0-fb7ae646de07)
_Figure 1 : 얼음과 증기가 주어졌을때, k라는 단어가 동시에 발생할 확률을 나타낸 표입니다. 아래의 ratio가 작을 수록 steam과 관련된 속성이고 클수록 ice와 관련된 속성입니다._

Figure 1에서 나온 ratio는 단어 벡터 학습에서 확률 그자체보다 적절한 시작점으로 제안합니다.

$$\begin{equation}
F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}} \tag{1}
\end{equation}$$

위 식의 확률은 세개의 단어 i,j,k에 의존하는 형태로 표현됩니다. 여기서 w는 단어벡터이고 $$\tilde{w}$$는 별도의 문맥 단어 벡터입니다.

벡터 공간은 본질적으로 선형 구조이므로, 벡터의 차이를 사용하여 식 (1)을 아래와 같이 수정합니다.

$$\begin{equation}
F(w_i - w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}} \tag{2}
\end{equation}$$

F를 복잡한 함수로 만들면 수집하려고 하는 선형 구조를 애매하게 만들기 떄문에 이를 피하기 위해 내적을 활용해 F가 원하지 않는 방향의 벡터 차원으로 뒤섞이는것을 방지합니다.

$$\begin{equation}
F((w_i - w_j)^T \tilde{w}_k) = \frac{P_{ik}}{P_{jk}} \tag{3}
\end{equation}$$

단어간 상호발생 행렬에서 대상 단어와 문맥 단어 간의 역할을 임의로 지정할 수 있으며, 두 역할은 서로 바뀔 수 있습니다. 이렇게 하기위해 w와, $$\tilde{w}$$뿐만 아니라 $$X$$와 $$X^T$$를 교환해야합니다.

마지막 모델은 label 변경에 대해 불변해야하지만 (3)식은 그렇지 않습니다. 따라서, 위 식의 대칭성은 아래의 과정을 통해 성립하게 됩니다.

F가 $$(\mathbb{R},+), (\mathbb{R},\times)$$ 사이에서 동형사상$$_{homomorphism}$$이라는 것은 가정합니다. 즉, 덧셈연산이 곱셈연산으로 매핑된다는 의미로 아래와 매핑이 됩니다.

$$\begin{equation}
F((w_i - w_j)^T \tilde{w}_k) = \frac{F(w^T_i\tilde{w}_k)}{F(w^T_j\tilde{w}_k)} \tag{4}
\end{equation}$$

(3)과 (4)를 통해 아래와 같이 (5)가 나오게 됩니다.

$$\begin{equation}
F(w^T_i\tilde{w}_k) = P_{ik} = \frac{X_{ik}}{X_i} \tag{5}
\end{equation}$$

위의 (4)와 (5)를 통해 모든 조건을 만족하는 F = exp 함수로 정의하면 다음과 같이 나타내고, 아래와 같은 식이 나오게 됩니다.

$$\begin{equation}
w^T_i\tilde{w}_k = log(P_{ik}) = log(X_{ik}) - log(X_i) \tag{6}
\end{equation}$$

식 (6)은 $$log(X_i)$$가 우변에 없으면 교환대칭성이 성립합니다. 그러나 이 항은 k에 독립적이므로 $$w_i$$의 편향 $$b_i$$로 넣을 수 있습니다. 마지막으로 추가적인 편향 $$\tilde{w}$$의 $$\tilde{b}$$를 대칭성을 회복하기위해 넣어줍니다.

$$\begin{equation}
w^T_i\tilde{w}_k + b_i + \tilde{b}_k = log(X_{ik}) \tag{7}
\end{equation}$$

식 (7)은 (1)번 식을 극단적으로 간단화한 형태지만 log의 인수가 0 일때, 무한대로 발산하는 문제 때문에 특정 조건에서 정의되지 않는 $$\mathbb{i.l.l}$$ 의존적입니다. 따라서, 추가적인 항을 넣어 해결합니다.

$$log(X_{ik}) \rightarrow log(1+X_{ik})$$

이는 X의 발산을 막고 상호발생의 log 행렬이 LSA와 연관되어 있습니다. 그리고 단점으로 모든 상호발생 가중치가 동일해 sparse한 상호행렬이 빈번한 정보보다 정보를 덜 옮깁니다. 그러므로 새로운 가중치 최소 제곱 회귀 모델을 이 문제에 적용합니다.

$$\begin{equation}
J = \sum^V_{i,j=1} f(X_{ij})(w^T_i\tilde{w}_k + b_i + \tilde{b}_k -  log(X_{ik}))^2 \tag{7}
\end{equation}$$

V는 vocabulary의 크기이고 이 가중치 함수는 아래와 같은 속성을 지닙니다.

1. f(0)=0
2. f(x)는 증가함
3. f(x)는 x의 값이 클 때 상대적으로 작아야함.


$$\begin{equation}
f(x) = \begin{cases}
    (x /x_{\text{max}})^\alpha & \text{if } x < x_{\text{max}} \\
    1 & \text{otherwise}
\end{cases} \tag{8}
\end{equation}$$


> ## Ⅳ. Experiments

다음과 같은 세가지 Task에 관해서 실험을 하였고 좋은 성능을 보여주었습니다.

![2](https://github.com/eastk1te/P.T/assets/77319450/41837288-b94e-4b33-bf09-331fd503327c)
_Figure 2 : (좌) 단어 유사성 작업에 대한 스피어만 순위 상관 관계 (우) 개체명 인식(NER)에서의 F1 점수_



- Word analogies
  
  “a is to b as c is to _?”와 같은 질문의 연속입니다.

  - semantic question
    
   사람 또는 장소에 대해 비유됩니다. (ex. “Athens is to Greece as Berlin is to _?”)

  - syntactic
    
    동사 또는 형용사의 형태로 비유됩니다. (ex. “dance is to dancing as fly is to _?”)
    
  질문에 정확한 대답을 하려면 모델은 빈 항을 인식해야합니다. 즉, “a is to b as c is to _?”라는 질문에서 단어 d를 표현하는 $$w_d$$ 는 $$w_b - w_a + w_c$$ 와 가까워야 하며 관계는 코사인 유사도를 사용해 측정합니다.

- Word similarity

    벡터 공간의 부분 구조를 찾는 비유 작업과 함께 다양한 단어 유사도 작업에서의 다양성 또한 평가합니다.

- Named entity recognition

    텍스트에서 엔티티를 식별하고 분류하는 과정으로 5개의 이산 특성과 50차원의 벡터를 활용하여 평가하였습니다.

> ## Ⅴ. results

많은 파라미터가 성능에 영향을 많이 미치기 때문에 Word2Vec과 비교할때 vector length, window size, corpus, vocabulary size, training time 들을 제어하였고 그 중 학습 시간이 매우 중요한 변수로 작용했습니다. 

![3](https://github.com/eastk1te/P.T/assets/77319450/3f3b91f2-5b46-452e-84d3-143078738bd9)
_Figure 3 : word analogy task에서 CBOW와 Skip-Gram과의 성능 비교._

동일한 corpus, vocabulary, window size, training time에서 GloVe는 연속적으로 word2vec을 압도하는 성능을 보여주었고 더 빠르고 속도에 무관하게 더 좋은 결과를 달성했습니다.


[^2]: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)