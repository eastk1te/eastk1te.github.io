---
title: 'Hadoop vs Spark'
author: east
date: 2023-05-16 00:00:00 +09:00
categories: [Open Source, Framwork]
tags: [Hadoop, Spark, Aparch]
math: true
mermaid: true
---

하둡(Hadoop)과 스파크(Spark)는 모두 대용량 데이터 처리를 위한 인기 있는 프레임워크이지만, 각각 다른 아키텍처와 목적을 가지고 있습니다. 하둡과 스파크의 주요한 차이점은 다음과 같습니다.

> ## Ⅰ. Hadoop.

대규모 데이터 세트를 처리하기 위한 분산 데이터 처리 및 저장 시스템입니다. 주로 대용량의 데이터를 효율적으로 분산 처리하고 저장하기 위해 사용됩니다. Hadoop은 아파치 소프트웨어 재단에서 개발되었으며, 여러 개의 서버 또는 노드에서 작업을 분산하여 병렬 처리를 수행합니다.

> ### ⅰ.HDFS

HDFS(Hadoop Distributed File System)는 대용량 데이터를 저장하기 위한 분산 파일 시스템입니다. 데이터는 여러 개의 노드에 분산되어 저장되며, 고장 내성 및 확장성을 제공합니다. 주로 배치 작업(Batch Processing)을 위한 데이터 처리에 사용되며, 대규모 데이터 처리 및 분석을 수행하는데 많이 활용됩니다.

- `블록 단위 저장:` HDFS는 대용량의 파일을 여러 개의 블록(Block)으로 분할하여 저장합니다. 기본적으로 128MB의 블록 크기를 가지며, 블록은 여러 개의 노드에 분산 저장됩니다. 이러한 분산 저장 방식은 데이터의 안정성과 고가용성을 제공합니다.

- `분산 데이터 저장:` HDFS는 여러 개의 데이터 노드(Data Node)로 구성된 클러스터에서 데이터를 저장합니다. 각 데이터 노드는 로컬 디스크에 데이터 블록을 저장하며, 데이터의 복제를 통해 데이터의 안정성과 내고장성을 보장합니다. 기본적으로 각 블록은 세 개의 복제본을 가지며, 이러한 복제본은 서로 다른 데이터 노드에 저장됩니다.

- `마스터-슬레이브 아키텍처:` HDFS는 네임노드(Name Node)와 데이터노드(Data Node)라는 두 가지 유형의 서버로 구성됩니다. 네임노드는 파일 시스템의 메타데이터를 관리하고, 데이터노드는 실제 데이터 블록을 저장합니다. 네임노드는 클러스터의 전체 파일 시스템 구조와 각 데이터 블록의 위치 정보를 유지하며, 데이터노드는 블록의 저장 및 복제를 담당합니다.
  ![image](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/1a3235f6-be92-4605-b6c2-3470ad75fd0d)
  _한기철, K-ICT 빅데이터 교육교재 중 발췌_

- `데이터 복제와 안정성:` HDFS는 데이터의 안정성과 내고장성을 보장하기 위해 데이터의 복제를 사용합니다. 각 블록은 여러 개의 데이터 노드에 복제되어 저장되므로, 하나의 데이터 노드가 고장나더라도 다른 복제본을 사용하여 데이터의 손실을 방지할 수 있습니다.

- `고확장성과 높은 처리량:` HDFS는 대규모 데이터 세트를 처리하기 위해 설계되었습니다. 여러 개의 데이터 노드를 통해 데이터를 병렬로 처리하고, 네임노드는 메타데이터 관리를 위해 메모리에 유지하여 빠른 응답 속도와 높은 처리량을 제공합니다.
  

> ### ⅱ.YARN

YARN(Yet Another Resource Negotiator)은 클러스터에서 작업을 관리하고 리소스를 효율적으로 할당하는 클러스터 리소스 관리 시스템입니다. YARN은 작업 스케줄링, 리소스 할당, 모니터링 등을 담당합니다.

- 구성요소

  - `리소스 매니저(Resource Manager):` 리소스 매니저는 클러스터 전체의 리소스 할당과 관리를 담당합니다. 클러스터의 전체 용량과 가용 리소스를 파악하고, 어플리케이션의 리소스 요청을 조율하여 적절한 할당을 수행합니다.

  - `노드 매니저(Node Manager):` 노드 매니저는 각각의 노드에서 실행되며, 해당 노드의 리소스를 관리합니다. 리소스 매니저와 통신하여 할당된 리소스를 관리하고, 작업 실행을 감독하며 상태를 보고합니다.

  - `애플리케이션 마스터(Application Master)`: 애플리케이션 마스터는 특정 작업을 수행하는 애플리케이션의 실행과 관리를 담당합니다. 각각의 애플리케이션은 자체적으로 애플리케이션 마스터를 가지며, 리소스 매니저와 통신하여 리소스 할당을 요청하고 작업의 진행 상황을 보고합니다.

- 주요 기능

  - `다양한 작업 지원:` YARN은 기존의 MapReduce 작업 뿐만 아니라 다양한 작업을 수행할 수 있도록 설계되었습니다. 다양한 프레임워크와 언어를 지원하며, 사용자는 자신의 작업에 맞는 프레임워크를 선택하여 실행할 수 있습니다.

  - `용량과 성능 최적화`: YARN은 클러스터의 리소스를 효율적으로 관리하여 용량과 성능을 최적화합니다. 다양한 애플리케이션에 리소스를 공정하게 할당하고, 리소스의 활용도를 높이는 기능을 제공합니다.

  - `확장성:` YARN은 수천 대의 노드로 구성된 대규모 클러스터에서도 확장성을 제공합니다. 클러스터의 크기에 상관없이 안정적으로 작업을 처리하고, 필요에 따라 리소스를 동적으로 할당하고 확장할 수 있습니다.


> ### ⅲ.MapReduce

MapReduce는 Hadoop의 데이터 처리 모델로, 대규모 데이터 세트를 작은 블록으로 분할하고, 각 블록을 병렬로 처리한 후 결과를 결합하는 방식으로 작동합니다. MapReduce는 분산된 환경에서 데이터 처리 작업을 수행하여 데이터 병렬 처리를 가능하게 합니다.

MapReduce 모델은 다음과 같은 두 단계로 구성됩니다:

- `Map 단계:` 입력 데이터를 읽고, 주어진 함수를 적용하여 중간 결과를 생성합니다. 이 단계에서는 데이터를 키-값 쌍으로 매핑합니다. 보통 입력 데이터의 각 레코드에 대해 동일한 연산을 적용하여 여러 개의 중간 키-값 쌍을 생성합니다. 예를 들어, 로그 데이터에서 각 로그 레코드를 읽고 특정 단어가 나타나는 빈도를 계산하여 중간 키-값 쌍으로 출력할 수 있습니다.

- `Reduce 단계`: Map 단계에서 생성된 중간 결과를 받아 특정 작업을 수행하여 최종 결과를 생성합니다. 이 단계에서는 중간 결과를 특정 키에 대해 그룹화하고, 그룹 내에서 원하는 작업을 수행하여 최종 결과를 생성합니다. 예를 들어, 중간 키-값 쌍을 특정 키에 대해 그룹화하고, 그룹 내에서 각 키에 대한 총합을 계산하여 최종 결과로 출력할 수 있습니다.

> ## Ⅱ. Spark.

Spark는 대규모 데이터 처리 및 분석을 위한 빠르고 일관된 분산 컴퓨팅 시스템입니다. Apache Spark는 Hadoop의 MapReduce보다 훨씬 빠른 데이터 처리를 제공하며, 다양한 작업 유형을 지원하고 복잡한 분석 및 처리 과정을 간단하게 만듭니다. Spark는 다양한 언어로 작성된 애플리케이션을 지원하며, Java, Scala, Python, R 등을 사용하여 개발할 수 있습니다.

- `빠른 처리 속도:` Spark는 인메모리 데이터 처리를 지원하여 데이터를 디스크에서 메모리로 로드하여 처리 속도를 향상시킵니다. 이를 통해 실시간 및 대화식 분석을 수행할 수 있습니다.

- `유연한 데이터 모델:` Spark는 다양한 데이터 소스와 호환되는 유연한 데이터 모델을 제공합니다. Hadoop의 HDFS뿐만 아니라 Cassandra, HBase, S3 등과 같은 다양한 데이터 저장소에서 데이터를 읽고 쓸 수 있습니다.

- `다양한 작업 유형 지원:` Spark는 배치 처리, 반복 처리, 실시간 스트리밍 및 대화식 쿼리와 같은 다양한 작업 유형을 지원합니다. 이는 다양한 분석 및 처리 요구에 맞춰 유연하게 사용할 수 있음을 의미합니다.

- `확장성과 고가용성:` Spark는 클러스터 환경에서 확장성과 고가용성을 제공합니다. 데이터 처리 작업을 클러스터의 여러 노드에 분산하여 처리하며, 단일 노드 장애에 대비한 자체적인 장애 복구 기능을 제공합니다.

- `풍부한 라이브러리:` Spark는 다양한 기능을 제공하는 풍부한 라이브러리를 보유하고 있습니다. 예를 들어, Spark SQL, MLlib, GraphX 등을 통해 SQL 쿼리 처리, 머신러닝, 그래프 분석 등을 수행할 수 있습니다.

> ## Ⅲ. Ecosystem

![image](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/63134871-a64d-4727-9fff-cb085096e495)
_출처 : https://wikidocs.net/31411_

- 데이터 수집

  - `Flume   `: 실시간으로 대량의 로그 데이터를 수집하여 하둡 클러스터로 이동합니다.           
  - `Sqoop   `: RDBMS(관계형 데이터베이스)와 하둡 클러스터 간에 데이터를 이동하고, 관계형 데이터를 하둡에 적재합니다.

- 데이터 저장
  - `HDFS    `: 대용량의 데이터를 저장하고 관리하기 위한 분산 파일 시스템입니다.              
  - `HBase   `: 대규모의 비정형 데이터를 실시간으로 읽고 쓸 수 있는 분산형 NoSQL 데이터베이스입니다. 
- 데이터 처리
  - `MapReduce     `:분산 데이터 처리를 위한 프레임워크로, 대용량 데이터를 분산 환경에서 처리하고 분석합니다.
  - `Spark   `: 데이터 처리 및 분석을 위한 빠르고 범용적인 클러스터 컴퓨팅 시스템입니다.        
  - `Pig     `: 구조화되지 않은 데이터를 처리하기 위한 스크립트 언어와 데이터 플로우 실행 프레임워크입니다.
  - `Hive    `: 데이터 웨어하우스 인프라로, SQL 기반의 데이터 쿼리 및 분석을 제공합니다.    
  - `Mahout  `: 대규모 데이터 세트에서 머신러닝 및 데이터 마이닝 알고리즘을 실행하기 위한 라이브러리와 프레임워크입니다.
- 데이터 가공 및 분석
  - `Pig     `: 구조화되지 않은 데이터를 처리하기 위한 스크립트 언어와 데이터 플로우 실행 프레임워크입니다.
  - `Hive    `: 데이터 웨어하우스 인프라로, SQL 기반의 데이터 쿼리 및 분석을 제공합니다.    
  - `Impala  `: 실시간 대화식 분석을 위한 고성능 분산 SQL 엔진입니다.                      
  - `Spark SQL     `:Spark에서 SQL 쿼리를 실행하고 대화식 분석을 수행하기 위한 모듈입니다.     
  - `Flink   `: 실시간 스트림 처리 및 배치 데이터 처리를 위한 분산 데이터 처리 엔진입니다.
  - `Drill   `: 다양한 데이터 소스에 대한 실시간 대화식 쿼리 처리를 위한 분산 SQL 쿼리 엔진입니다.
  - `Kylin   `: 대용량 데이터에 대한 뛰어난 쿼리 성능을 제공하기 위해 Hadoop 기반의 OLAP 큐브를 생성하는 오픈 소스 프로젝트입니다.
- 데이터 시각화
  - `Zeppelin`: 대화식 데이터 분석을 위한 웹 기반 노트북 인터페이스입니다.                  
  - `Hue     `: Hadoop 에코시스템의 다양한 구성 요소와 상호 작용하기 위한 웹 기반 사용자 인터페이스입니다.
  - `Superset`: 데이터 시각화 및 비즈니스 지능을 위한 웹 기반 대시보드 및 데이터 시각화 도구입니다.
- 클러스터 관리
  - `YARN    `: 하둡 클러스터의 리소스 관리와 작업 스케줄링을 담당합니다.                     
  - `Hadoop Common `:하둡 클러스터의 공통 라이브러리와 유틸리티를 제공합니다.                      
  - `Ambari  `: Hadoop 클러스터의 설치, 설정, 관리를 위한 웹 기반 관리 도구입니다.           
- 보안
  - `Ranger  `: Hadoop 클러스터의 보안 정책을 관리하고 강제하는 프레임워크입니다.            
  - `Knox    `: Hadoop 클러스터의 보안된 웹 게이트웨이를 제공합니다.                         



> ## Ⅳ. CONCLUSTION.

결론적으로 대용량 배치 작업에는 하둡이 적합하고, 인메모리 처리를 통해 빠른 데이터 처리는 Spark가 유용하다.

> ## Ⅴ. REFERENCES

[1] : [virtual box linux [ubuntu 18.04]에 하둡 설치,다운로드 3.ubuntu 에 hadoop(하둡) 다운로드,설치](https://spidyweb.tistory.com/214)  
[2] : [하둡 기초 정리](https://han-py.tistory.com/361)

<br><br>
---