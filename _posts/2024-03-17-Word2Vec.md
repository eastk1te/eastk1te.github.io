---
title: '[Paper]Word2Vec'
author: east
date: 2024-03-17 09:00:00 +09:00
categories: [Paper, NLP]
tags: [Paper, NLP]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

NLP의 초기 발전 중 하나는 단어 내부의 표현을 학습하는 것입니다. 이전에 기계학습 알고리즘은 구조화되고 잘 정의된 고정된 길이의 입력을 선호하고, 텍스트보다 숫자가 더 잘 작동합니다. 따라서 기본적인 방법으로 원핫 인코딩이나  bag-of-words로 text를 숫자의 equivalent vector로 정보 손실없이 변환하였습니다.

![1](https://github.com/eastk1te/P.T/assets/77319450/c35212d7-31cd-4c8b-bee9-13bdca322bd0){: w="500"}
_Figure 1:Example of one-hod encoding, 다항 non-zero values들은 bag-of-words를 사용_

위처럼 단어를 원자 단위로 취급하는것은 robustness와 간결성과 같은 타당한 이유들이 있습니다. 심지어 방대한 양의 데이터에서 간단한 모델은 적은데이터에서의 복잡한 모델보다 성능이 좋다고 논의되었지만 많은 작업에서 문제가 있습니다.

기존의 단어 표현의 단점은 차원의 저주와 일반화 문제로 묘사됩니다. 전자는 단어간의 유사성을 키워 특징의 크기를 증가시켜 희소하고 고차원의 벡터들이 발생하고 후자는 단어 간 유사도를 수집할 수 없어서 발생합니다.

이러한 한계점을 극복하기 위해 Word2Vec[^1]이 연구되었습니다.

> ## Ⅰ. Introduction

큰 데이터 집합에서 연속적인 벡터 표현으로 계산하는 두가지 새로운 모델 아키텍쳐를 제안하였습니다.

NLP 작업에서 단어를 매우 작은 단위로 사용하는데 단어들간의 유사도는 표현하지 못하고 인덱스로만 표현했습니다. 그래도 단순성과 robustness, 적은 데이터로도 수행가는 하다는 점에서 N-gram 같은 유명한 통계적 언어 모델링이 사용되었습니다.

그러나 여러 작업에 제한이 있었고 양질의 데이터와 표현 가능한 언어의 부족으로 단순하게 크기만 키우는 방법은 더 나아가지 못했습니다.

ML 방법 중 가장 강력한 개념은 단어의 분포 표현을 사용하는 것입니다. 따라서, 큰 데이터 셋으로부터 고품질의 단어 벡터를 학습하는 것이 주된 목표라고 합니다. 이는 유사한 단어들이 서로 가까울 뿐만 아니라 유사도의 다차원을 지니고 있어 단순한 syntactic 규칙을 넘어서고 word offset 방법을 사용해 단순한 연산이 단어 벡터에서 작동합니다.예를들어 V("King) - V("Man") + V("Woman")의 결과는 V("Queen")과 유사하게 됩니다.

이렇게 단어를 연속적인 벡터로 표현하는 것은 오랫동안 진행되어 왔습니다. FNNLM이 유명한 모델 아키텍쳐였고, 선형적인 projection layer와 비선형적인 hidden layer는 단어 벡터 표현과 통계적 언어 모델을 결합하여 학습할 수 있었습니다. 이후 소개된 연구로 단어 벡터는 단일 hidden layer를 가지는 신경망을 사용하여 처음으로 학습이 가능하고 여기서는 해당 아키텍처를 확장하여 간단한 모델을 가지고 단어 벡터를 학습하는 것입니다.

> ## Ⅱ. Model Architectures

해당 모델은 LSA나보다 단어 사이의 선형적 관계를 유지하면서 LDA보다 계산적 비용이 덜 들어 더 효율적인 성능을 발휘합니다. 연산 복잡도는 아래와 같습니다.

$$O = E \times T \times Q$$

E: epochs, T: N(words), Q: 모델에 따라 다르게 형성됩니다.

> ### ⅰ. FNN Language Model

$$Q=N\times D + N \times D \times H + H \times V$$

N은 입력 단어 개수(1-of-V), V는 총 단어의 개수, D:input 차원, H:hidden 차원, V: 출력 차원 이며 입력 층은 projection layer P를 통해 N X D 차원으로 투영됩니다. 

계층적인 softmax를 사용하여 어휘들은 Huffman binary tree으로 표현된됩니다. 이를 통해 결과를 log2(V) 줄일 수 있지만 NxDxH 항 때문에 결정적인 속도의 향상을 이루지는 못합니다. 나중에 제안될 모델은 은닉 층을 가지지 않아 효과적인 softmax 정규화가 가능합니다.

Huffman binary tree란?
: 데이터 압축 알고리즘 중 하나로 가장 빈번하게 나타나는 문자에 짧은 이진 코드를 넣고, 드물게 나타나는 문제에는 긴 이진 코드를 할당해 압축합니다.


> ### ⅱ. RNN Language Model

RNN은는 효율적으로 더 복잡한 패턴을 표현할 수 있습니다. 투영 층을 가지지 않고 입력, 은닉, 출력 층만 지녀 위의 FNN의 단점을 극복합니다. RNN은 재귀적 행렬이 time-delayed 연결을 사용하여 은닉 층을 스스로 연결합니다. 이는 재귀적인 구조로써 현재 입력과 이전 시간의 은닉층에 기반해 현재 은닉층을 업데이트합니다. 이러한 결과로 단기 메모리와 같은 형태를 형성하고, 시간적인 의존성을 학습할 수 있게 합니다.

$$Q=H \times H + H \times V$$

위에서 마찬가지로 V는 log2(V)로 변환이 가능합니다.

> ## Ⅲ. New Log-linear Models

단어의 분산된 표현을 학습하고 계산 복잡도를 최소화하는 두가지 아키텍처를 소개합니다. 이러한 아키텍처는 아래와 같은 두 단계로 구성됩니다.

1. 연속적인 단어 벡터는 단순한 모델을 사용하여 학습
2. N-gram NN Language Model은 위의 학습된 분산 표현위에서 학습

![1](https://github.com/eastk1te/P.T/assets/77319450/e1e48342-58b6-42ca-9194-6534c0006f9b){: w="500"}
_Figure 1 : CBOW와 Skip-gram의 두 가지 아키텍처._

> ### ⅰ. Continuous Bag-of-words model

CBOW는 NNLM과 유사한 방식으로 비선형적인 은닉층이 제거되고 투영 층이 모든 단어에서 공유됩니다.

$$Q=N\times D + D \times \log_2(V)$$

> ### ⅱ. Continuous Skip-gram Model

CBOW와 유사하지만 맥락에 기반해 현재 단어를 예측하는 것에 반해 주어진 문장에서 각 단어의 분류를 예측하고 최대화하기 위해 학습됩니다. 각 현재 단어는 연속적인 투영 층의 입력으로 사용된 후 log-linear 분류기의 입력으로 사용됩니다.

$$Q=C\times(D+D\times log_2(V))$$

C는 단어의 최대 거리를 나타내며, 각 단어를 학습하는 데 사용할 이웃 단어의 개수를 나타내는 R을 1:C까지의 범위에서 랜덤하게 선택합니다. 선택된 R값에 따라 현재 단어를 중심으로 R 거리만큼의 이전과 이후 단어들을 올바른 레이블로 지정합니다. 이는 $$R \times 2$$의 단어 분류가 필요합니다.

고차원의 단어 벡터를 데이터에서 학습하면 벡터는 단어 사이의 미묘한 의미 관계를 파악할 수 있습니다. 단어 벡터는 semantic 관계를 캡처하고, 이는 현존하는 자연어 처리 작업에서 성능을 향상시킬 수 있습니다.

[^1]: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)