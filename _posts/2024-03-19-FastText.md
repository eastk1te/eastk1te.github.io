---
title: '[Paper]FastText'
author: east
date: 2024-03-19 00:00:00 +09:00
categories: [Paper, NLP]
tags: [Paper, NLP]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

2016년에 Facebook에서 발표된 논문으로 단어 벡터를 풍부하게 만드는 새로운 임베딩 방법인 FastText[^1]를 발표했습니다.

큰 규모의 라벨링 되지 않은 데이터에서 학습되는 단어 표현 벡터는 많은 NLP 작업에서 유용합니다.

유명한 모델들(GloVe, Word2Vec)은 각 단어로 구별하는 표현 벡터로 형태(?)$$_{morphology}$$를 무시하며 학습합니다. 이는 특히 큰 규모와 드문 어휘들에서 제한이 있습니다. 따라서, 해당 논문에서 skip-gram 모델에 기반한 각 단어를 특징화된 bag인 n-grams으로 하는 새로운 방법을 제시합니다.

벡터 표현은 각 단어의 n-gram 연관되어 있으며 해당 표현의 합으로서 표현됩니다. 이러한 방법은 큰 규모의 말뭉치에서 빠르게 학습되고 학습 단어에 나오지 않는 단어 표현도 계산가능합니다. 

> ## Ⅰ. Introduction

이전의 단어 표현 방법들의 대부분은 어휘의 각 단어들을 구별된 벡터로 표현합니다. 특히, 이러한 방법들은 단어의 내부 구조를 무시해 형태론적으로 다양한 언어에서 큰 제약이 있습니다. 

형태론적으로 다양한 언어는 많은 단어의 형태를 형성하고 있고, 말뭉치를 학습시 자주 발생해 좋은 단어 표현을 학습하는데 방해합니다. 왜냐하면 많은 단어 형성 과정에서 특정 수준의 정보를 사용함으로써 형태론적으로 다양한 언어가 벡터 표현을 향상시키기 때문입니다.

해당 논문에서 `단어를 단어의 n-gram 벡터들의 합으로 표현하는 방식을 제안`합니다.

> ## Ⅱ. Model

> ### ⅰ. Subword model

간단한 skip-gram 모델에 대한 내용은 [Word2Vec](../Word2Vec)에서 확인하시기 바랍니다.

skip-gram 모델은 단어의 내부 구조 정보를 무시하기에 해당 정보를 얻기위한 다른 계산 함수 $$s$$를 제안합니다. 각 단어 w는 bag-of-특징의 n-gram으로 표현됩니다. 

여기에 다른 글자 시퀀스와 구별하는 특별한 경계 <>를 prefix와 suffix의 시작과 끝에 넣습니다. 또한, 각 단어 w를 n-gram 집합 안에 포함시킵니다. 각 단어의 표현을 학습하기위해 글자의 n-gram을 추가합니다. 

$$<wh,whe,her,ere,re> + <where>$$

"where"이라는 단어를 n=3으로 설정한 예로 이는 아래와 같은 character n-grams로 표현됩니다. 여기에서의 her은 단어 <her>과 다른 방식으로 상호작용합니다.

이는 매우 단순한 접근법으로 n-gram의 다른 집합에서 모든 prefix와 suffix를 고려할 수 있습니다.

G 크기의 n-gram 사전이 주어졌다고 가정했을때, n-gram의 집합은 w로 각 n-gram $$g$$를 벡터표현 $$z_g$$와 연계할 수 있습니다. 

이렇게 단어를 n-gram들의 벡터표현의 집합으로 표현할 수 있습니다.

$$s(w,c) = \sum_{g\in G_w}z_g^Tv_c$$

이러한 간단한 모델은 단어를 넘어서는 표현을 공유하고 드문 단어를 위해 신뢰할만한 표현을 학습합니다.

> ## Ⅲ. Experimental setup

최적화 문제를 확률적 경하하강법로 음의 로그 우도를 손실 함수로 사용하여 해결하였고, skip-gram의 기본을 따라갔습니다. 

> ## Ⅳ. Results

해당 논문에서 단어 유사도 평가, 단어 유추, SOTA와의 비교, 학습데이터 크기, 문자 n-gram 크기의 다섯 가지 실험을 통해 모델을 평가하였습니다.

이러한 결과로 subword 정보로 훈련되는 단어 표현을 사용하는 것이 일반적인 skip-gram 모델보다 성능이 우수한 것을 확인했습니다.

이는 subword 정보의 중요성이 언어 모델링 작업과 형태학적으로 풍부한 언어 벡터의 유용함을 관찰 가능합니다.

> ## Ⅴ. Conclusion

해당 논문에서 단어 표현을 학습하는데 subword 정보를 넣는 간단한 방법을 연구했습니다. 이러한 접근법은 문자의 n-grams를 skip-gram model로 편입시키는 방법입니다. 해당 모델의 단순성 덕분에 빠른 학습과 전처리나 지도학습이 필요하지 않습니다. 해당 모델의 성능이 subword 정보를 다루지않는 baseline(skip-gram)보다 우수함을 보여주었습니다.


[^1]: [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)