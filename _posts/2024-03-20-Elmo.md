---
title: '[Paper]Elmo'
author: east
date: 2024-03-20 00:00:00 +09:00
categories: [Paper, NLP]
tags: [Paper, NLP]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---


2018년에 context를 고려한 단어 표현을 학습할 수 있는 ELMo(Embeddings from Language Models)가 발표되었습니다.

이는 이전의 Word2Vec, GloVe의 context-independent한 단일 표현과 다르게 맥락을 고려하는 단어 표현을 임베딩한다는 것에 큰 의미가 있었습니다.

> ## Ⅰ. Introduction

단어 표현은 단어를 사용하는 복잡한 특징과 문맥을 넘어 어떻게 사용하는 지에 대한 문제가 존재했습니다. 이러한 두 문제를 다루는 새로운 타입의 맥락있는 단어 표현을 소개합니다.

양방향 LSTM에서 파생된 벡터를 사용하여 큰 규모의 텍스트 말뭉치에서 수정된 LM의 Objective로 사전 학습하게 됩니다. 이러한 ELMo의 표현력은 더 깊어졌으며 biLM의 내부 기능을 감지합니다. 이는 현존하는 모델에 쉽게 추가가 가능하며 각 케이스에서 20%까지의 성능향상을 이루어냈습니다.

Word2Vec, Glove 같은 단어 벡터 학습 접근법은 맥락과 상관없는 각 단어의 단일 표현만을 얻는 것이었습니다. 이를 극복하기 위해 특징 컨볼루션(문장에서의 특징, 패턴 등)을 사용하여 subword 단위에서의 이점(morphology)을 활용하고 다양한 의미를 가진 정보를 포함하게 됩니다.

양방향 언어 모델의 목적을 수정함으로써 다른 층의 정보를 효율적으로 사용할 수 있으며 다운스트림 작업에 있어 좋은 성능을 보였습니다.

> ## Ⅱ. ELMo

ELMo의 단어 표현은 전체 입력 문장을 반영하는 기능을 합니다. 이러한 표현은 biLM의 두 층으로 계산되며 컨볼루션 특징을 가지는 내부 상태의 선형 조합으로 구성됩니다.

이러한 방법은 준지도 학습을 가능하게 하며 biLM은 큰 규모로 학습이 되어 여러 NLP 아키텍쳐와 쉽게 결합될 수 있습니다.

> ### ⅰ. Bidirectional language models

N개의 토큰들의 시퀀스 (t_1, ..., t_N)에서 언어 모델은 아래와 같이 이전의 토큰들이 주어졌을때, 다음과 같은 토큰의 확률을 생성합니다.

$$
\begin{equation}
p(t_1, ..., t_N) = \Pi_{k=1}^Np(t_k\vert t_1,...,t_{k-1})
\end{equation}
$$

biLM은 위의 순방향 모델과 그에 반대방향인 역뱡항 LM을 결합한 형태로 아래와 같이 구성됩니다.

$$
\begin{equation}
\begin{array}{ll}
\sum_{k=1}^N& (\log p(k_l \vert t_1,...,t_{k-1};\theta_x, \overrightarrow{\theta}_{LSTM}, \theta_s) \\
& + \log p(k_l \vert t_{k+1},...,t_N;\theta_x, \overleftarrow{\theta}_{LSTM}, \theta_s))  
\end{array}
\end{equation}
$$

```python
lstm = torch.nn.LSTM(..., bidirectional=True)

# lstm = (sequence, batch, 2 * hidden)
lstm_out, _ = lstm(embeddings)
forward_out, backward_out = lstm_out[:,:,:hidden], lstm_out[:,:,hidden:]
```


토큰 표현 $$\theta_x$$와 softmax $$\theta_s$$ 파라미터들은 LSTM에서 순방향과 역방향 각각에서 구분되어 유지하며 같이 사용됩니다.

이러한 공식은 모델이 순방향과 역방향으로 정보를 학습하여 독립적으로 사용하는 것 대신 두 방향 사이에서 가중치를 공유해서 사용합니다.

> ### ⅱ. ELMo

biLM에서 각 토큰 $$t_k$$에 대해 L-layer의 biLM은 2L의 임베딩과 원래 임베딩 1개의 표현을 계산하여 아래와 같이 "2L+1"개의 표현을 얻습니다.

$$\begin{array}{ll}
    R_k & = \{x_k^{LM}, \overrightarrow{h}^{LM}_{k,j}, \overleftarrow{h}^{LM}_{k,j} \vert j=1,...,L \} \\
     & = \{h^{LM}_{k,j} \vert j =0,...,L\}
\end{array}$$

```python
R_k = [batch_size, sequence_length, 2L + 1, hidden]
```

이러한 표현을 적용하기 위해 아래와 같이 Task에 맞춰 단일 벡터로 임베딩시킵니다.

$$\begin{array}{ll}
ELMo_k^{task} &= E(R_k;\theta^{task}) \\
&=  \gamma^{task}\sum_{j=0}^Ls_j^{task}h^{LM}_{k,j}
\end{array}$$

$$s^{task}$$는 소프트 맥스 정규화 가중치이고, 상수 파라미터 $$\gamma^{task}$$는 모델이 전체 ELMo 벡터를 조정합니다. 각 biLM 층의 활성화는 다른 분포를 가지며 이전에 정규화를 하는 것이 도움이 됩니다.

종합적으로 양방향 언어 모델을 통해 높은 품질의 맥락을 반영하는 표현 방법을 소개했습니다. 이러한 방법은 ELMo의 큰 성능 향상을 보였고, biLM에서 다른 층들이 서로 다른 타입의 syntactic, semantic 정보를 효과적으로 표현하는 것을 확인했습니다. 

> ## Ⅲ. REFERENCES

1. [Deep contextualized word representations](https://arxiv.org/abs/1802.05365)

<br><br>
---