---
title: "[논문 읽기]GAN"
author: east
date: 2023-03-11 14:35:00 +09:00
categories: [PaperReview, GenerationModel]
tags: [GAN]
math: true
mermaid: true
---



> ## Introduction

> # Generation Model(생성모델)

우선 GAN을 정리하기에 앞서 생성모델을 이해해야합니다. 기계 학습 모델은 모델의 역할에 따라 크게 분류 모델과 생성 모델로 나뉠 수 있다고 합니다.
여기서 분류 모델(판별자 모델, Discrimnative Model)은 `데이터 X가 주어졌을 때 나타나는 레이블 Y가 나타날 조건부확률 P(Y|X)를 직접적으로 반환하는 모델`입니다.(주로 고차원의 데이터에서 낮은 차원의 레이블 데이터로 변환하는 작업)

이와 다르게 생성 모델(Generative Model)은 `데이터의 분포를 추정하는 방법`으로 아래와 같이 두가지로 나뉠 수 있습니다.(주로 분류 모델과는 반대로 낮은 차원에서 높은 차원의 데이터를 만들어내는 작업)
1. 지도적 생성모델 : Label이 있는 데이터에 대해 확률분포를 추정한 후 계산하는 방법으로 대표적인 예시로는 선형판별분석법(LDA)[^1], 이차판별분석법(QDA)[^2]가 있다.
1. 비지도적 생성모델 : Label이 없는 데이터에 대한 분포를 학습하여 모분포를 추정하는 방법입니다.
  - 통계적 생성 모델 : 관측된 데이터들의 분포로 원래 변수의 확률 분포를 추정하는 밀도 추정 모델이라 볼 수 있다. 대표적인 예시로는 커널 밀도 추정(KDE)[^3], 가우시안 밀도 추정 모델[^4] 등
  - 딥러닝을 이용 생성 모델 : 

    <img src=https://danbi-ncsoft.github.io/assets/works/generator/%EA%B7%B8%EB%A6%BC7.png width=60%>

    해당 그림을 보면 처음으로 Explicit density와 Implicit density 2가지로 나뉘어 진다.

    - Explicit density(명시적) : 확률 변수 p(x)를 정의하여 사용하는 것.
      - Tractable density : 데이터를 가지고 확률 분포를 '직접'구하는 방법.
      - Approximate density : 데이터를 가지고 확률 분포를 '추정'하는 방법.(VAE[^5], Variational Auto-Encoder)
    - Implicit density(암시적) : 확률 변수 p(x)에 대한 정의 없이 p(x)를 샘플링하여 사용. 대표적으로 GAN[^6] 이 있음.

VAE, GAN은 Flow based 모델
Diffusion model에 기반을둔 DALL-E, NovelAI, Stable Diffusion 등은 

[^1]: LDA
[^2]: QDA
[^3]: KDE
[^4]: 가우시안 밀도 추정 모델
[^5]: VAE
[^6]: GAN

> ## REFERENCES.
https://danbi-ncsoft.github.io/works/2021/10/01/Generator.html
https://blog.mnc.ai/33
https://blog.mnc.ai/48
https://devkihyun.github.io/study/Flow-based-Generative-Models-1-Normalizing-Flow/
https://lilianweng.github.io/posts/2021-07-11-diffusion-models/
https://medium.com/@sunwoopark/slow-paper-glow-generative-flow-with-invertible-1x1-convolutions-837710116939


> ## GAN(Generative Adversarial Networks, 적대적 생성 신경망)

> ## Introduction

<center>
<img src="https://paperswithcode.com/media/methods/gan.jpeg" width=70%> 

<sub>[이미지 출처] https://paperswithcode.com/method/gan</sub>
</center>

$$x \approx Genrator(Dicriminator(x))$$

- 위조범과 판별자간의 관계로 서로 각각 모방과 판별의 능력을 향상 시켜나가는 관계를 가짐.

- GAN의 네트워크
  - 생성자 네트워크(generator network)
    - 랜덤 백터(잠재 공간의 무작위한 포인트)를 입력으로 받아 이를 합성된 이미지로 디코딩
  - 판별자 네트워크(discriminator network)
    - 이미지를 랜덤으로 입력으로 받아 훈련 세트에서 온 이미지인지 생성자가 만든 이미지인지 판별

>> GAN은 매 단계가 조금씩 전체 공간을 바꾸기 때문에 최적화 과정으로 최솟값을 찾는 것이 어려움.   
>> 즉, 적절한 파라미터를 찾고 조정해야함.

<img src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FL7afg%2Fbtre2eX38nW%2FYfepfjkmUZ4pP73XarTymk%2Fimg.png" width=70%>


generator의 분포 $p_g$

사전에 noise 변수($p_z(z)$) 지정 후 

$G(z;\theta_g)$
다층 신경망(multilayer perceptron, output=scalar)

$D(x;\theta_d)$
다층 신경망(multilayer perceptron, output=scalar)

parameters $\theta_g$

목적함수(value function, $V(G,D)$)를 최대화 해야함.

$D(G(x))$는 생성된 가짜 이미지


$D(x;\theta_d)$

$$\min\limits_{G}\max\limits_{D}V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_{z}(z)}[1-D(G(x))] \space \cdots \space (1)$$
즉, 목적함수가 최대값이 되게 하려면 D(x)=1, D(G(x))=0이 되게 만들어야 합니다.

> ## REFERENCES.
https://roytravel.tistory.com/109
https://arxiv.org/pdf/1406.2661.pdf


<!-- [1] : https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html -->