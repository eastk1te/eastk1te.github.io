---
title: '[Paper]GPT3'
author: east
date: 2024-03-24 00:00:00 +09:00
categories: [Paper, NLP]
tags: [Paper, NLP]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

2018년 BERT의 성공은 NLP 분야에서 "Gold rush"를 일으켰으며 OpenAI의 세 번쨰 GPT 아키텍처인 GPT-3가 큰 주목을 받았습니다. 

원래의 GPT 아키텍처는 12개의 레이어로 구성된 Transformer 디코더로 구성되어 있습니다. 이러한 트랜스포머 언어 모델은 사전 학습된 모델 위에서 softmax 분류기를 초기화하고 미세조정을 통해 기울기를 전달하는 전이학습을 하기에 알맞습니다. 이전의 가장 큰 모델이었던 T5는 110억 개의 파라미터였지만, GPT-3에서 1750억 개의 파라미터로 매우 커졌으며 학습 데이터 역시 거대해졌습니다. 

LLM의 시대를 열기 시작한 "Language Models are Few-Shot Learners, OpenAI, 2020" 논문을 정리해보았습니다.

> ## Ⅰ. Introduction

최근 사전학습된 표현이 주를 이루었지만 이러한 접근법은 task-agnostic, large dataset, 미세조정이 필요하다는 단점들이 존재합니다.

이러한 단점들은 첫째로 현실적으로 "large labeled dataset"을 매번 새로운 작업을 위해 만드는 것은 어렵습니다. 두번째로 모델의 표현력과 학습 분포가 좁아 학습 데이터에서 잘못된 상관관계를 사용할 가능성이 있습니다. 셋째로 대부분의 언어 작업에서 사람은 많은 양의 설명이 필요하지 않습니다.

![1](https://github.com/eastk1te/P.T/assets/77319450/3a7abf0e-2eee-4cf3-8b69-e9a7b925fffe){: w="500"}
_Figure 1 : 비지도된 사전학습 동안 언어모델이 넓은 지식과 패턴인식 능력을 개발하여 추론에 적용합니다. "in-context learning term"은 inner loop를 설명하기 위해 각 시퀀스에서 발생하고, 해당 시퀀스는 모델이 사전학습동안 데이터의 표현을 하는것이 아닌 단일 시퀀스에서 반복되는 sub-task를 포함되어있습니다._

이러한 문제를 해결하는 잠재적인 방법은 meta-learning입니다. 모델은 여러 sub-task에 대한 패턴과 특징을 학습하고, 추론에 빠르게 적용하는 것입니다.

![2](https://github.com/eastk1te/P.T/assets/77319450/34f1872b-e4d8-45fc-b8b5-d22a8aa24ef1){: w="500"}
_Figure 2: 큰 모델일수록 in-context 정보를 효율적으로 만듭니다._

FIgure 2 에서 few-shot learning은 모델이 단어에서 관계없는 기호를 제거하도록 간단한 작업을 진행했습니다. 모델의 성능은 작업에 대한 설명을 추가를 하며 이루어졌고, 모델 크기에 드라마틱하게 향상되었습니다.

이러한 결과를 통해 few-shot setting은 여러 작업에 관한 데이터를 수집해 학습시킨 것을 의미합니다.

> ## Ⅱ. Approach

![3](https://github.com/eastk1te/P.T/assets/77319450/41426cf4-fc79-467d-b319-fb989911149d){: w="500"}
_Figure 3: 언어 모델을 가지고 수행한 4가지 방법들_

- `Fine-Tunning`

    기존의 접근법으로 많은 벤치마크에서 강한 성능을 보였지만 매 작업마다 큰 규모의 새로운 데이터셋이 필요하고 일반적인 out-of-distribution에 대한 약점이나 학습데이터의 오염된 특징을 사용할 가능성이 있습니다.

    그러나 GPT3에서는 특정작업에 국한되지않는 성능에 집중하여 미세조정을 사용하지 않습니다.

- `Few-Shot`

    모델이 추론할때 몇개의 작업 설명이 조건적으로 제공되지만 가중치 업데이트는 진행하지 않습니다. K개의 예제를 활용하여 모델이 문맥을 이해하고 완성하도록 합니다. few-shot의 주된 이점은 특정 작업에 국한되지 않아 데이터의 필요성이 줄어들어 특정 작업의 패턴에 과적합되는 위험을 줄이는 것입니다. 그러나 미세조정된 모델보다 성능이 낮을 수 있습니다.
    
- `One-Shot`

    위의 few-shot과 유사하지만 한가지의 작업 설명만을 사용합니다. 이러한 one-,few-,zero-를 구분하는 이유ㅜ는 사람이 작업을 하는 방식과 유사하기 때문이라고 합니다.


- `Zero-Shot`

    One-shot과 같고 설명이 허락되지 않는다는점이 다릅니다. 즉, 모델은 작업에 대한 설명만 주어집니다. 이러한 방법은 편리함을 극대화시켜 robustness하고, 데이터의 오염에 강합니다.

> ### ⅰ. Model and Architectures

GPT2의 아키텍처와 유사하며 몇 가지 수정사항으로 pre-normalization, reversible tokenization, SparseTransformer에서 사용되는 sparse attention을 사용했습니다.

> ### ⅱ. Training Dataset

Common Crawl dataset으로 기존의 GPT-2의 데이터셋인 WebText2 보다 급격히 커진 것이 특징입니다.

![4](https://github.com/eastk1te/P.T/assets/77319450/4882c35b-f5d3-4769-94ce-c7ebb1a62052){: w="500"}
_Figure 4:GPT3에 사용된 데이터셋으로 "Weight in training mix"는 학습 중 데이터셋에서 추출된 예제의 비율을 나타냄._

> ## Ⅲ. Conclusion

사람은 새로운 언어 작업에서 몇 가지 예제나 단순한 설명만으로도 일반적으로 수행가능합니다. 규모가 커진 언어 모델들은 불가지론적인 작업과 few-shot 설정에서 향상되어왔습니다. GPT-3는 이전의 non-sparse 언어 모델보다 10배는 더 많은 175 billion 파라미터를 지니고 있고,  미세 조정이나 가중치 업데이트 없이 모델과의 텍스트 상호작용을 통해 few-shot 설명만 가지고 모든 작업이 가능했습니다.


> ## Ⅳ. REFERENCES

1. [GPT-3 : Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
2. [hugginface GPT-2 github](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py)

<br><br>
---