---
title: '[Paper]RBM(Restricted Boltzmann Machine)'
author: east
date: 2023-06-26 00:00:00 +09:00
categories: [Paper, NN]
tags: [Paper, NN]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

Restricted Boltzmann Machine(이하 RBM)은 심층 확률 모형을 구축하는 데 가장 흔이 사용되는 요소이며 ANN, DNN, CNN, RNN 등과 같은 타겟과 가설 간의 차이를 줄여서 오차를 줄이는 것이 목표인 결정적$$_{Deterministic}$$ 모델들과는 다르게 확률 밀도 함수를 모델링 하는 것이 목표이다.

> ## Ⅰ. RBM(Restricted Boltzmann Machine)

확률 그래픽$$_{Probability \space Graphic}$$ 모델링을 위한 기초적인 모델 중 하나로, 잠재된 패턴을 자동으로 학습하여 더 높은 차원의 데이터를 분석하는 데 사용됩니다. 특히, 필터링, 특징값 학습, 차원 축소 등의 분야에서 활용되어 효과적인 성능을 발휘합니다.

> ### ⅰ. Graphical Model

Graphcial Model이란?
: 변수들간의 상호 의존 관계를 표현한 확률 모델로 변수간 의존관계를 설명하는 모델입니다.

![graphic](https://github.com/eastk1te/P.T/assets/77319450/9595d71a-2668-465a-95b0-9c0972f923b3)
_출처 : https://medium.com/@chullino/graphical-mode이란-무엇인가요-2d34980e6d1f_

그래픽 모델링은 변수 간의 상호 작용과 불확실성을 효과적으로 표현하기 위해 조건부확률과 체인룰을 이용합니다. 조건부확률을 통해 한변수가 다른 변수에 영향을 미치는 정도를 정량화하면, 변수 간의 종속성을 파악할 수 있으며 추론에도 도움이 됩니다. 또한, 체인룰을 활용하여 확률 분포들을 세부적으로 나누고 다양한 조건부확률들의 곱으로 분해해 계산을 단순화하여 계산 비용을 줄일 수 있게 도와줍니다. 즉, 변수간의 관계를 정량적으로 표현하고 계산량을 줄이는 방법입니다.

> #### Graphical Model의 분류

1. Directed Graphical Model 
   
    방향 그래프로 "Batesian Network", "Belief Network"라고도 불리며 화살표로 방향성 있는 엣지$$_{edge}$$를 표현합니다. 방향 그래프에서는 조건부 확률 분포를 기반으로 변수들 간의 관계를 모델링합니다.

2. Undirected Graphical Model
   
    무방향 그래프로 "Markov Random Field", "Markov Network"라고도 불리며 방향성이 없는 엣지로 표현합니다.무방향 그래프에서는 함께 등장하는 변수들의 결합 분포를 기반으로 모델링합니다.

3. Chordal Graphical Model
   
   순환 그래프가 없는 특수한 종류의 그래픽 모델로 각 사이클에 최소 하나의 "줄$$_{chord}$$"이 있는 그래프 입니다. 해당 그래프의 특징은 모든 순환 그래프 내에서 한 가지 이상의 동시 발생 관계를 가지는 것으로 복잡성을 줄이는데 도움이 되며, 각 변수들 간의 의존성을 포착하려는 응용 분야에서 사용됩니다.


> ### ⅱ. Restricted Boltzmann Machine(RBM)

![다운로드](https://github.com/eastk1te/P.T/assets/77319450/9e0712bf-02d6-49bd-ae92-03afebae4b76)
_Figure 1 : 좌 : 일반적인 볼츠만 머신으로 위의 layer는 이산형 값을 가지는 hidden feature이고 아래 layer는 동일하게 이산형 값을 가지는 visible feature임. , 우 : 제한된 볼츠만 머신으로 hidden, visible layer 각 계층 내의 연결이 존재하지 않음._

Figure 1에서 좌측의 일반적인 볼츠만 머신은 신경망 기반의 확률적 에너지 모델로 입력 데이터의 고차원 평가 공간을 낮은 차원으로 투영시키고, 데이터로 부터 숨겨진 특징$$_{feature}$$인 확률 밀도 함수를 학습하기 위한 머신 설계입니다. 해당 모델은 완전연결그래프$$_{Fully-connedted-graph}$$로 표현되며 에너지 함수를 최소화하도록 가중치를 조정하는 과정을 통해 학습됩니다.

해당 모델에서는 이산형 값$$_{binaray-state}$$만을 가지는 Hidden unit과 visible unit들이 존재하는데, hidden unit은 우리가 보지 못하는 어떤 특성이 존재함을 암시하고, 보이지 않는 factor들까지도 학습할 수 있다면 좀 더 정확한 확률분포를 학습할 수 있다는 것을 전제합니다.

Figure 1에서 좌측의 BM과 다르게 우측의 RBM에서는 동일한 layer의 node 간의 내부적인 연결이 없어진 것은 사건 간의 독립성을 가정함으로써 확률분포의 결합을 쉽게 표현하기 위해서입니다.

> ### ⅲ. Energy

RBM이 학습 피드백을 수치적으로 판단하기 위해서는 여타 model들을 구성할 때와 마찬가지로 cost function을 정의해주어야 한다.

RBM의 cost function은 독특하게 “Energy”라는 개념을 이용해 정의되게 될텐데, 이 energy라는 개념은 물리학에서 차용한 것이라고 한다. 물리에서 예시를 들자면 어떤 기체가 그 공간에 있을 확률은 온도(에너지)와 반비례한다고 한다.

- #### Energy func 정의
    ![Energy](https://github.com/eastk1te/P.T/assets/77319450/eff66286-11d8-424e-8be3-fc74d660071d)
    _Figure 2 : 입력 x가 나올 확률은 에너지 $E(x)$에 반비례함._

- #### Energy func 기반의 모델 정의
    ![Energy based](https://github.com/eastk1te/P.T/assets/77319450/21d2c515-8264-4469-8603-31749c8050c6)
    _Figure 3 : Boltzmann distribution_

- #### 확률 분포 함수 정의
    ![Probability](https://github.com/eastk1te/P.T/assets/77319450/fc032a1c-4988-415f-b0c5-d761f5f450e8)
    _Figure 4 : parameter $\theta$가 주여졌을때 visible, hidden의 확률 분포 모델._

- #### 각 조건부 확률분포 정의
    ![conditional distribution](https://github.com/eastk1te/P.T/assets/77319450/12e60b5e-db57-4ab3-b85a-961fe1e1bbd1)
    _Figure 5 : 위의 Figure 4를 결합확률분포를 통해 각 조건부확률분포를 확인 가능함_

- #### Maximum Likelihood Estimation(MLE) 정의
  
    ![MLE](https://github.com/eastk1te/P.T/assets/77319450/18e56300-0a5b-4c30-8d29-fd10a6cfbc7d)
    _Figure 6 : 최대우도추정법을 활용하여 visible을 최대로하는 parameter $\theta$를 찾는다._

    Figure 6에 나온 postive phase와 negative phase를 보자.

    ![image](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/9b27a7fd-a2e9-41bd-8c97-8e2bd00d8bf7)
    _출처 : https://www.edwith.org/deeplearningchoi/lecture/15304?isDesc=false_

  - Positive Phase(visible_to_hidden)
    
     히든이 주어졌을때 에너지로 visible이 주어졌을떄 hidden 노드를 모두 summation하는 작업이 필요. 즉, 확률분포를 통한 샘플 생성을 의미함.

  - Negative Phase(hidden_to_visible)
    
    hidden과 visible 모두 주어졌을때 에너지로 모든 가능한 visible과 hidden 노드들의 summation하는 작업이 필요함. 즉, 주어진 데이터와 sampling을 통해 얻은 visible layer의 데이터가 원래 데이터와 거의 같아야 함.

  즉, Positive Phase는 visible-to-hidden 과정이고, Negative Phase는 hidden-to-visible 과정입니다.
    
  여기서 2번인 Negative Phase는 계산 복잡성이 너무 켜져서 Contrastive Divergence(이하 CD)를 활용합니다.


- #### Loss 정의
  
    $$Loss = F(v) - F(v^1)$$

    visible layer에 데이터를 주고 hidden layer의 node 값을 샘플링, 그런 다음 다시 visible layer의 데이터를 다시 예측하도록 하는 과정에서 처음 주어진 visible layer의 데이터와 다시 획득한 visible layer의 데이터가 얼마나 차이가 있는지를 말하는 것이다.

    즉, 원래의 데이터로부터 얻은 visible layer의 energy와 생성된 데이터로부터 얻은 visible layer의 free energy 간의 차이가 적을 수록 학습이 잘 된 것이라고 할 수 있습니다. 따라서 해당 loss를 역전파를 통해 가중치들을 업데이트 해가며 학습해나가는 방식입니다.


> ### ⅳ. Contrastive Divergence

Geoffrey Hinton이 제안한 방법으로 `매개변수를 학습하는 데 사용되는 근사화된 최대우도 추정 방법`입니다. 일반적인 최대우도 학습의 경우, 각 단계에서 오차 기울기를 계산하려면 너무 많은 양의 계산이 필요합니다. CD는 이러한 계산적인 문제를 해겨하기 위해, MCMC 샘플링을 이용하여 근사화된 기울기 추정치를 얻는 방법입니다.

우선 상세한 내용을 설명하기 전에 기초가 되는 내용을 설명하겠습니다.

`몬테카를로(Monte Carlo Method)`이란?
: 랜덤 표본을 뽑아 함수의 값을 확률적으로 계산하는 알고리즘으로 값을 근사적으로 계산하려고 할때 쓰임. ex. 원주율 계산 - 무직위 점들을 뽑아 놓고, 해당 점들이 원안에 속하는 비율을 계산하면 대략적으로 원의 넓이를 구할 수 있음.

`마코프 연쇄(Markob Chain)`이란?
: 특정 시점의 상태 확률은 그 이전 상태에만 의존함. 즉, 이를 반복하다보면 직전 상태의 확률과 같아지는 수렴 상태가 나타납니다. 이렇게 평형 상태에 도달한 확률 분포를 정적분포(Stationary Distribution)이라고 함.

$$P(q_t|q_{t-1})$$

`MCMC(Monte Carlo Markov Chain, 마코프 연쇄 몬테카를로 방법)`이란?
: 복잡한 확률 분포로부터 샘플링을 수행하는데 사용되는 알고리즘으로 샘플을 얻으려고 하는 목표분포를 정적 분포를 활용하여 마코프 체인을 만들어 시뮬레이션을 가동하고 초기값에 영향을 받는 burn-in 기간을 지나고 나면 목표분포를 따르는 샘플이 만들어진다.

`깁스샘플링(Gibbson sampling)`이란?
: MCMC의 일종으로 모든 샘플이 독립인 몬테카를로 방법과 다르게, 마코프 연쇄에 기반한 MCMC는 이전 샘플의 영향을 받는다는 점이 다릅니다. 깁스 샘플링은 이전 샘플의 영향을 받는 점이 MCMC와 같지만, 나머지 변수는 그대로 두고 한 변수에만 변화를 주는것이 다릅니다. 이렇게 뽑은 표본들의 앞부분은 초기 상태(임의의 값)에 크게 의존하지만 충분한 반복이 이루어진 후에는 초기 상태에 관계 없이 확률 분포에 기반한 표본 수집이 가능하다고 함.


앞서 나온 Figure 6의 Negative Phase는 $\sum_{v,h}p(v,h)\frac{\partial E(v,h)}{\partial \theta}$로 모든 경우의 수를 계산해야하기 때문에 계산복잡성이 너무 커짐. 결과적으로 이 값이 우리가 원하는 값도 아니고 중간의 gradient를 위한 필요한 step에 불과한데 iteration 안에 exponetial complexity가 존재하면 문제를 야기한다. 따라서 깁슨 알고리즘을 통해 $p(v,h)$를 구하는 step을 converge 할때까지 줄이는게 아니라 한번만 돌려 근사치를 구하는 방법이다. 이렇게 나온 근사치로 Negative Phase를 계산하여 gradient를 구하는 방법이다. 여기서 `정확히 converge 한 결과와 중간에 멈추거나 대략의 방향성은 공유하니 한번의 update만으로도 충분할 것이라는 생각이 핵심`이다.

> ### ⅴ. DNN과 다른 점

결과적으로 RBM은 데이터의 확률 분포를 습득하는 데 특화된 비지도 학습 알고리즘입니다. 그러나 DNN$_{Deep-Neural-Network}$은 학습 데이터를 기반으로 특징을 추출하고 복잡한 함수를 근사하는데 초점을 두고 있습니다. 하지만 DNN에서도 확률분포를 측정할수있는 모델이 있는데 VAE$$_{Variational-Auto-Encoder}$$, GAN$$_{Generative-Adversarial-Network}$$과 같은 생성모델들은 DNN을 기반으로도 데이터의 확률분포를 추정하는데 활용되고 있습니다.

> ## Ⅱ. DBN(Deep Belief net)

DBN(Deep Belief Network)은 여러개의 제한된 볼츠만 머신(Restricted Boltzmann Machine, RBM) 계층을 순차적으로 쌓아서 구성된 심층학습 기반의 비지도 신경망입니다. 각 레이어에서 데이터의 복잡한 패턴을 잘 추출하여 더 높은 수준의 표현을 얻을 수 있게 해줍니다. DBN의 핵심 목적은 높은 수준의 표현(representation)을 학습하여 복잡한 데이터를 효과적으로 처리하는 를 얻는 것입니다.

![DBN](https://github.com/eastk1te/P.T/assets/77319450/6543d9a3-93e4-40a5-bae2-b1e83f2397e8)
_Figure 1 : 최상층에 있는 2개의 히든 레이어가 방향성이 없는 연관 메모리를 형성하고 [그림 1], 나머지 히든 레이어들이 연고나 메모리의 표현을 이미지의 픽셀과 같은 관찰 가능한 변수로 변환하는 방향성 비순환 그래프(하이브리드 모델)를 제안한다._

여기서 마지막 label units layer는 결합 확률분포$$_{joint-probability}$$를 의미하고, 나머지 layer들은 모두 조건부확률분포$$_{conditional-probability}$$로 표현된다. 결합확률분포는 전체 변수들의 상호 작용을 모델링하고, 조건부확률분포는 각 변수가 이전 계층을 기반으로 어떻게 변화하는지 모델링합니다.참고로 전체를 jointly하게 표현하는 모델을 Deep Boltzmann Machine (DBM) 이라고 하는데, 이 모델의 경우 RBM update를 하는 알고리즘과 비슷한 알고리즘으로 전체 모델을 update하게 된다. 그러나 이 논문이 발표될 당시에는 DBN이 훨씬 간단하고 computational cost가 적기 때문에 DBN이라는 모델을 제안했습니다.

> ### ⅰ. Motivation

전통적인 인공신경망에서는 많은 수의 은닉계층을 포함하는 깊은 네트워크를 학습시키기 어렵고 초래하는 문제로 인해 수렴 속도가 느렸습니다. 이러한 문제를 해결하고자, 깊은 학습에 효과적인 DBN(Deep Belief Network)이 개발되었습니다. 이렇게 함으로써 깊은 네트워크를 빠르고 효율적으로 학습시키며, 데이터의 복잡한 구조를 효과적으로 학습할 수 있게 되었습니다. 

> ### ⅱ. Complementary priors

![2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/e962a0b5-dc46-4252-bd19-c0015732a2fb)
_Figure 2 : 지진과 트럭 노드가 모두 꺼져있을 때 들어오는 -20이 바로 "Complementary" 은닉층_

`"Explaining away"` 현상은 방향성 신경망에서 추론을 어렵게 만듭니다. 

Explaining away란?
: 확률 그래프 모델, 특히 방향 그래프에서 관찰할 수 있는 현상으로 두 개의 원인 집합이 주어진 결과로부터 상호 교환되어 설명이 가능할 때 발생합니다. 이 현상은 한 원인이 정보를 제공하게 되면, 다른 원인의 가능성이 감소하게 되는 것으로 `독립적인 원인들에서 의존 관계가 형성되는 현상`을 Explaining away 현상이라고 합니다. 해당 개념은 인과 관계의 이해와 모델릉을 통한 예측 및 추론에 도움을 줍니다.

Complementary priors는 두 확률 분포가 서로 보완적인 정보를 가진 사전 확률 분포를 의미하는 것으로 서로 다른 두 가지 사전 확률 분포가 보충적인 역할을 하여 함께 사용되면 모델 학습에 도움이 되는 경우를 지칭합니다. 즉, 계층간 가중치를 업데이트하는 과정에서 독립적인 사전 정보가 각 계층 사이에서 정보를 교환하고 적절히 업데이트 되게끔 돕습니다.



> ### ⅲ. Logistic belief net
  
$$p(s_i-1) = \frac{1}{1+exp(-b_i-\sum_js_jw_{ij})}$$

Logistic belief net이 생성 모델로 쓰일 경우,
노드 i가 켜질 확률은 위 수식처럼 부모 노드 j의 켜짐 여부와 연결 가중치를 이용한 함수로 표현된다.

만약 logistic belief net이 하나의 은닉층만을 가진다면 은닉 노드들이 서로 독립적으로 선택되기 때문에 prior distribution이 독립 분포로 표현이 가능하다.

하지만 posterior distribution은 비독립적인데, 이는 데이터로부터의 likelihood로 인해 발생합니다. 별도의 은닉층(Figure 2처럼)을 추가해 이 likelihood와 완전히 정반대의 상관관계를 갖는 complementary prior를 생성함으로써 첫번째 은닉층에서 발생하는 explaining away 효과를 제거할 수 있음을 보여줍니다.

이후 likelihood가 prior와 곱해지면 우리는 독립 분포로 표현되는 posterior를 구할 수 있다.

Complementary prior가 항상 존재한다고 확신할 수는 없지만,
다음 항목에서 소개하듯이 모든 은닉층에서 짝지어진 가중치들이 complementary prior를 생성하는 무한한 logistic belief net의 간단한 예시가 있다.

- #### An Infinite Directed Model with Tied Weights

![3](https://github.com/eastk1te/P.T/assets/77319450/e590c5c8-fd30-47c2-b7f9-52c77c6ab0f5)
_Figure 3_

이 모델은 무한히 깊은 은닉층에서 무작위 구성으로 시작해, 활성화된 부모 노드가 베르누이 분포를 따라 특정 자식 노드를 활성화시키는 top-down 방식으로 데이터를 생성해낸다.

하지만 이 모델은 다른 방향성 네트워크와는 다르게 가시층($V_0$) 데이터 벡터($v_i^0$)부터 시작해, 전치 가중치 행렬($W^T$ )을 이용해서 각 은닉층의 독립 분포를 차례대로 유추함으로써 모든 은닉층의 실제 posterior를 구할 수 있다.

이제 우리는 실제 posterior로부터 샘플을 만들어내 그것이 데이터와 얼마나 다른지를 계산할 수 있기 때문에 데이터 $v_0$ 로그 확률의 미분값을 계산할 수 있다.

먼저 $w_{ij}^{00}$(은닉층 $H_0$의 j 유닛과 가시층 $V_o$ 의 i 유닛 간의 가중치)에 대한 미분값을 구하면 다음과 같다.
​   

$$\frac{\partial logp(v^0)}{\partial w_{ij}^{00}} = <h_j^0(v^0_i - \hat{v}_i^0)>$$


$<...>$은 샘플링된 값들의 평균을 뜻하며 $\hat{v}_i^0$은 가시 벡터 $v_i^0$가 샘플링된 은닉층으로 부터 재생성되었을떄 유닛 i가 켜져 있을 확률을 뜻한다.

만약  $v_i^0$ 과 $v_i^0$ 가 같다면 샘플링된 은닉층으로부터 실제 가시 벡터를 완벽하게 재생성할 수 있다는 것을 뜻하며 이를 위한 방향으로 모델이 학습된다.

그런데 이때, 첫번째 은닉층 $H_0$ 로부터 $v_i^0$ 을 만들어내는 것은 첫번째 은닉층 $H_0$ 로부터 두번째 가시층 $V_1$ 을 통해 두번째 은닉층 $H_1$ 의 사후 분포를 계산하는 것과 완벽히 동일한 과정이다.

따라서 위의 수식은 아래와 같이 바뀔 수 있다.


$$\frac{\partial logp(v^0)}{\partial w_{ij}^{00}} = <h_j^0(v^0_i - v_i^1)>$$

위와 같은 수식의 변화에 있어 $h_j^0$ 에 대한 $v_i^1$ 의 비독립성은 문제가 되지 않는다.

그 이유는 $v_i^0$ 이 $h_j^0$ 에 전제를 둔 확률이기 때문이다.

같은 $w_{ij}$ 가 여러번 중복되어 사용되기 때문에 총 미분값은 아래와 같이 모든 층 간의 가중치 미분값들의 총합으로 표현된다.

$$\frac{\partial logp(v^0)}{\partial w_{ij}} = <h_j^0(v^0_i - v_i^1)> + <v_i^1(h^0_j - h_j^1)> + <h_j^1(v^1_i - v_i^2)> +  \ldots$$

가장 처음과 마지막 항을 제외하고 서로 상쇄되어 없어지고 이는 볼츠만 머신 수식과 같아진다.

즉, 서로 다른 계층의 가중치가 공유$_{tied-weights}$되어 데이터를 설명하고, 원인 간 반대의 상관관계$_{Complementary-priors}$를 만들어냅니다. 이 경쟁하는 원인들이 함께 작용하여 결과를 설명하는 데 더 신뢰할 수 있는 특징으로 전환되는 것에 중요한 역할을 합니다. 따라서 수식이 같아지는 RBM에서도 해당 효과를 볼 수 있는 주요한 결과를 보여줍니다.


> ### ⅳ. A Greedy learning Algorithm for Transforming Representations

복잡한 모델을 학습하는 효율적인 방법은 순차적으로 학습되는 간단한 모델 세트를 결합하는 것입니다. 순차적으로 학습되는 시퀀스의 각 모델이 이전 모델과 다른 것을 학습하도록 하기 위해 각 모델이 학습된 후 데이터가 어떤 방식으로든 수정된다. 모델은 입력 벡터에 대해 비선형 변환을 수행하고, 시퀀스의 다음 모델에 대한 입력으로 사용될 벡터를 출력으로 생성합니다.

Greedy Algorithm?
: 시퀀스의 각 모델이 데이터의 다른 표현을 받아낼 수 있도록 하는 것.


DBN의 학습은 탐욕 알고리즘을 활용한 계층별 학습을 진행합니다. 이 알고리즘은 각 단계에서 최적의 선택을 하여 전체 네트워크의 학습을 최적화합니다. 각 계층 학습이 끝날 때까지 반복하며, 최종 결과로 전체 네트워크가 구성됩니다. 계층별 학습을 통해 상위 계층으로 갈수록 저차원에서 고차원으로 표현이 변환됩니다. 이를 통해 네트워크가 데이터의 복잡한 구조를 포착하고 더 좋은 예측과 추론을 수행할 수 있습니다.

![figure5](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/f40d8399-e82d-4c56-bfd0-5af844839c08)
_Figure 5 : Hybrid network로 방향성이 있는 layer들을 쌓은 다음에 맨 상단의 두 layer는 무방향으로 생성._

Figure 5에서는 상위 2개 레이어가 무방향 연결(undirected connections)을 통해 상호 작용하며, 다른 모든 연결은 방향성이 있는 다층 생성 모델을 보여줍니다. 여기서 상단의 무방향 연결은 묶인 가중치가 있는 무한히 많은 상위 레이어를 갖는 것처럼 보입니다. 

상위 레이어 사이의 매개 변수가 $W_0$에 대한 complementary prior를 구성하는데 사용될 것이라고 가정하므로, 매개변수 $W_0$에 대한 합리적인 값을 학습하는 것이 가능합니다. 이 가정에 따르면, $W_0$를 학습하는 작업은 RBM을 학습하는 작업으로 단순화되는데, 이는 여전히 어렵지만 Contrastive Divergence를 최소화하여 꽤 좋은 근사값을 솔루션으로 얻을 수 있습니다. $W_0$를 학습한 후에는 $W^T_0$를 통해 데이터를 매핑해 첫 번째 은닉 레이어에서 더 높은 수준의 "데이터"를 생성할 수 있습니다.

> ### ⅴ. Back-Fitting with the Up-Down Algorithm

DBN의 마지막 학습 과정은 전체 네트워크의 가중치를 세밀하게 조정하는 데 사용하는 up-down 알고리즘입니다. 하위 계층에서 상위 계층으로 정보를 전달하고 다시 하위 계층으로 역전파하여 가중치를 업데이트합니다. 이 방법으로 전체 네트워크의 성능과 정확도를 높일 수 있습니다.

가중치 행렬을 한번에 한 계층씩 학습시키는 것은 효율적이지만 최적은 아닙니다. 또한, 상위 레이어의 가중치가 학습되면 가중치나 간단한 추론 절차가 하위 레이어에 적합하지 않습니다.

Greedy learning에 의해 각 단계별로 쇠선의 선택을 할 경우 전체 최적화에 이르지 못하는 Sub-Optimality문제는 부스팅이나 지도학습 같은 다른 학습 방법들에 비해 상대적으로 덜 치명적입니다.

현실에서는 레이블이 부족한 경우가 대부분이고, 각 레이블은 매개변수에 대해 몇 비트의 제약만 제공할 수 있으므로 일반적으로 과적합(overfitting)이 과소적합(underfitting)보다 더 큰 문제가 됩니다. 

하지만 비지도 학습은 레이블이 지정되지 않은 매우 큰 데이터셋을 사용할 수 있으며, 각 케이스는 매우 고차원적일 수 있으므로, 생성 모델에 많은 제약 조건을 제공한다. 그리고 과소적합은 먼저 학습된 가중치가 나중에 학습된 가중치에 더 잘 맞도록 수정되는 후속 단계의 back-fitting으로 완화할 수 있습니다.

> #### Down-pass


최상위 연관 메모리 상태에서 시작하여 하향식 생성 연결을 사용해 각 하위 레이어를 순차적이고 확률적으로 활성화합니다. 이 과정 동안 최상위 무방향 연결과 생성 방향 연결은 변경되지 않으며, 상향식 인식 가중치만 수정됩니다. 이 과정은 연관 메모리가 "down-pass"를 시작하기 전에 평형 분포에 정착되도록 하는 "wake-sleep" 알고리즘의 sleep 단계와 유사합니다. 하지만, 연관 메모리가 업 패스에 의해 초기화된 후 다운 패스를 시작하기 전까지 번갈아 가며 Gibbs 샘플링을 몇 번 반복하도록 합니다. 이렇게 하면, "대조적인(contrastive)" 형태의 "wake-sleep" 알고리즘이 형성되어 필요성을 제거할 수 있습니다. 

Wake-Sleep Algorithm
: 비지도 생성 모델을 학습하는 데 사용되는 알고리즘으로 1995년에 Hinton, Dayan, Frey, Neal에 의해 개발되었으며, 두 가지 단계인 "Wake" 단계와 "Sleep" 단계로 구성되어 있습니다. Wake 단계, 실제 데이터를 사용해 모델의 "인식(recognition)" 가중치를 업데이트합니다. Sleep 단계, 모델이 생성한 데이터를 사용해 모델의 "생성(generative)" 가중치를 업데이트합니다.

대조적인 형태는 sleep 단계의 문제를 해결하며, 실제 데이터에 사용된 표현을 학습하는 "인식(recognition)" 가중치를 확인하고 모드 평균화 문제를 제거하는 데 도움이 됩니다. 최상위 수준의 연관 메모리를 사용하면 wake 단계의 문제 역시 해결할 수 있지만, 독립적인 최상위 유닛은 가중치의 최상위 레이어에 대한 가변 불확정성 근사(variational approximation)가 부정확한 것처럼 나타납니다. 이는 약점으로 볼 수 있지만, 전체적으로 알고리즘이 여전히 효과적으로 작동하며 과소적합 및 과대적합 문제를 처리하는 데 도움이 됩니다.


> ### ⅵ. Conclusion

​Deep Belief Networks(DBNs)는 사전 학습(pre-training)과 미세 조정(fine-tuning) 과정을 기초로 하는 심층 학습 모델입니다. DBNs의 목적은 복잡한 패턴과 구조를 포착하는 데 도움이 되는 효과적인 특징을 학습하는 것입니다.

1. Greedy Layer-wise Pretraining
    
    DBN 내부의 각 계층에 있는 Restricted Boltzmann Machine (RBM)을 순차적으로 학습합니다. RBM은 가시 레이어와 숨겨진 레이어간의 양방향 뉴런 연결을 학습하며 원본 데이터의 특징을 추출합니다.
    학습 과정에서는 Contrastive Divergence (CD) 알고리즘을 사용하여 RBM의 가중치 및 편향을 업데이트합니다. 각 층의 숨겨진 레이어에서 얻어진 출력은 다음 층의 입력으로 사용되며, 이러한 방식으로 모든 층을 위에서 아래로 선행하는 방식으로 단계별로 학습이 이루어지는데 각 층에서의 특징 추출 과정은 속도가 빠르고 효율적이 됩니다.

2. Fine-tuning
   
    사전 훈련된 DBN 구조를 초기 신경망의 초기 가중치로 사용합니다. 역전파(backpropagation) 알고리즘이 사용되어 그래디언트 최적화를 통해 가중치와 편향을 업데이트하는 것으로 목표는 전체 네트워크에서 예측 오차를 최소화하기 위해 가중치 및 편향을 조정하는 것입니다.


DBN의 학습에서는 데이터 벡터를 주어진 숨겨진 활동들의 조건부 분포를 추론하는 것이 어렵습니다. 그 이유는 DBN이 빽빽하게 연결되어있고, 여러 개의 은닉층을 갖고 있는 구조 때문입니다. 이는 DBN이 여러 개의 RBM 층으로 구성되어 있기 떄문인데, 최상위 은닉층을 제외한 모든 은닉층이 다음 은닉층의 활성화 값에 의존하게 되어 역전파 알고리즘을 사용하기 어렵게 합니다. 특히, 이러한 구조는 결합 확률 분포에 대한 가시적인 속성과 은닉속성을 기반으로 추론을 수행하는 데 어려움이 따릅니다. 이러한 제약을 극복하기 위해 feed-ford 구조로 변환되며, 이를 fine-tuning을 통해 추가 훈련을 시키면 역전파를 사용하여 조건부 분포를 추론하는 것이 쉬워집니다. 따라서 DBN이 확률적 구조를 사용하기 떄문에 조건부 분포를 직접 추론하는 것은 어렵지만, 학습된 특징을 다양한 딥러닝 신경망 모델로 변환하고 미세 조정을 통해 상부 구조를 효과적으로 전파하면 추론을 용이하게 할 수 있습니다.

이 논문에서 모델이 의미있는 이유는 joint probability를 잘 표현하는 좋은 graphical model이어서가 아니라,이 모델로 deep network를 pre-training하고 backpropagation 알고리즘을 돌렸더니 overfitting 문제가 크게 일어나지 않고 좋은 성과를 거뒀기 때문이다.

성능 평가는 MNIST 손글씨 숫자 데이터셋과 TIMIT 음성 인식 데이터셋에 대한 실험에서 확인됩니다. 그 결과, DBN은 다양한 문제에서 기존 심층 신경망보다 더 나은 성능을 보여줍니다. 작업을 성공적으로 수행하는 능력이 향상되어, 사전 학습이라는 초기 교육의 도입이 신경망에 호응하여 딥러닝을 적용하기 적합하게 만들어 주었으며, 이것이 딥러닝의 발전 및 다양한 응용 분야에 영향과 기여를 하였습니다.


요약하면, DBM의 학습 과정은 사전 훈련 단계에서 초기 가중치를 설정하고, 파인 튜닝 단계에서 전체 네트워크를 최적화하는 것으로 구성되어 있습니다. 이 과정을 통해 DBM은 데이터의 특징을 효율적으로 포착하고 대규모 양자화 학습에 적용할 수 있습니다.


이렇게 DBN으로 unsupervised pre-training한 deep network 모델을 사용했을 때 MNIST 데이터 셋에서 그 동안 다른 모델들로 거뒀던 성능들보다 훨씬 우수한 결과를 얻을 수 있었고, 그때부터 deep learning이라는 것이 큰 주목을 받기 시작했다. 그러나 지금은 데이터가 충분히 많을 경우 이런 방식으로 weight를 initialization하는 것 보다 random initialization의 성능이 훨씬 우수하다는 것이 알려져있기 때문에 practical한 목적으로는 거의 사용하지 않는다.

> ## Ⅲ. Reducing the Dimensionality of Data with Neural Networks

![autoencoder](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/2a86f2b3-68a5-4821-b265-a2a6ec19dc2c)
_Fig1 : 사전학습은 RBM을 stack의 형태로 쌓아 올린후 형성된 deep autoencoder를 오차역전파를 활용하여 fine-tuned 시킨다._

오토인코더 신경망 구조에서 중간에 위치한 작은 은닉층인 small central layer를 통해 입력 데이터의 중요한 특성이 추출되어 고차원 입력 벡터를 재구성하는 다층 신경망을 학습시킴으로써, 고차원의 데이터를 저차원으로 변환할 수 있습니다. 

경사하강법은 오토인코더와 같은 구조의 가중치를 미세조정하는데 사용할 수 있습니다. 우리는 deep autoencoder 네트워크에서 저차원의 코드를 PCA와 같은 차원축소 도구보다 효과적으로 초기화하는 방법을 설명했습니다. 이렇게 생성된 차원축소의 결과물인 저차원 표현은 고차원 데이터의 분류, 시각화, 의사소통 및 저장을 가능하게 합니다.

흔히 사용되는 PCA(principal component analysis)와 같은 주성분 분석은 데이터셋의 큰 변수의 방향과 데이터들의 방향 좌표를 표현하는 방법입니다. 우리는 이러한 PCA의 비선형 일반화를 위해 오토인코더와 같은 적응적이고 다층 "인코더" 네트워크와 코드에서 데이터를 복구하는 유사한 "디코더" 네트워크를 사용하는 방법을 소개합니다.

먼저, 임의의 가중치로 구성된 두 개의 신경망을 원본 데이터와 재구축된 데이터 간의 차이를 최소화하는 방향으로 학습시킵니다. 필요한 그래디언트는 디코더에서 인코더까지 체인룰에 의한 오차 역전파를 통해 얻을 수 있습니다. 전체 시스템은 그림 1에 설명된 대로 autoencoder라고 불립니다. 

비선형 오토인코더의 다층 신경망에서 가중치를 최적화하는 것은 어렵습니다. 초기 가중치가 큰 오토인코더는 전형적으로 좋지 않은 지역 최소점을 찾을 수 있습니다. 초기 가중치가 작은 오토인코더는 많은 은닉층을 가지는 학습이 어려웠습니다. 만약 초기 가중치가 좋은 해결책에 근접하다면 경사하강법은 잘 작동할 수 있지만 그런 초기 가중치를 찾는 것은 각 레이어마다 매우 힘든 알고리즘이 될 것입니다. 따라서 우리는 이진 데이터에서 사전 학습이라는 절차를 소개합니다. 

이진 특성을 가진 단층 레이어는 이미지 구조를 가진 모델을 선택하는 최적의 방법이 아닙니다. 첫 번째 층에서의 결과물은 두 번째 층에서의 학습에서 visible unit이 됩니다. 이 layer-by-layer 학습은 원하는 만큼 진행할 수 있습니다. 이렇게 추가적인 레이어는 log 확률의 하한을 향상시켰으며, 각 레이어의 feature 결과물은 줄어들지 않고, 가중치도 정확하게 초기화되었습니다. 

이러한 경계선은 높은 레이어가 feature 결과물을 가지고 있으면 채택되지 않았습니다. 그러나 layer-by-layer 학습은 그럼에도 불구하고 deep autoencoder의 가중치를 사전 학습하는 데 매우 효과적인 방법이었습니다. 사전 학습이 완료된 각 레이어의 feature를 강하게 잡아내고, 레이어 유닛들 사이의 강한 상관관계를 보였습니다. 

사전 학습 이후 다층 신경망은 펼쳐지며$$_{Unfolded}$$(Fig1), 동일한 가중치를 사용하는 인코더와 디코더 네트워크로 전체 오토인코더를 수행합니다. 확률적인 활동은 결정론적인 실수 값 확률로 대체되며, 전체 오토인코더를 통해 역전파가 사용되어 최적 재구성을 위한 가중치를 미세 조정합니다

아래 이미지들은 여러 기법과 데이터 셋을 통해 비교한 내용을 시각화한 내용입니다.

![autoencoder1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/db99e954-1846-4464-8e5f-fdea7c1e92f4)
_Fig 3 : (A) PCA를 활용하여 2차원으로 시각화, (B) Autoencoder를 활용하여 2차원으로 시각화_

Fig3를 확인해보면 A보다 B가 조금더 분류하기 쉽게 나타냈다는 것을 확인이 가능한데, 이는 Autoencoder가 PCA보다 비선형의 더 복잡한 입력 데이터를 표현하는 능력이 있다는 것을 의미합니다.

![autoencoder2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/a8ad3092-a28e-4786-bd83-5bf357e53f4b)
_Fig 4 : (A) LSA와 Atoencoder의 성능을 비교한 그래프 (B) LSA를 활용하여 2차원으로 시각화 (C) Autoencoder를 활용하여 2차원으로 시각화_

마찬가지로 Fig4를 확인해보면 Autoencoder가 LSA보다 차원 축소의 능력이 뛰어난 것을 확인할 수 있습니다.

위의 내용들을 요약히자면, 이 논문은 고차원 데이터를 다루는 데 오토인코더가 어떻게 차원 축소 도구로 사용될 수 있는지에 대해 설명하고, 그 효과와 최적화를 위한 방법을 제시합니다. 

이 논문을 바탕으로 다양한 응용 분야에서 고차원 데이터의 차원 축소에 적용할 수 있는 연구가 이루어졌습니다. 이러한 기술은 labeled된 학습 데이터가 필요하지 않다는것을 알려주는데 큰 의의가 있습니다. 왜냐하면 우리는 충분히 큰 신경망 모델을 학습시키고 오버피팅을 방지하기 위해 레이블링 된 수많은 데이터를 가정할 수 없기때문입니다.


> ## Ⅳ. REFERENCES

1. ["Reducing the Dimensionality of Data with Neural Networks"](https://www.cs.toronto.edu/~hinton/absps/science.pdf)
2. ["A fast learning algorithm for deep belief nets"](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
3. [Restricted Boltzmann Machine](https://www.edwith.org/deeplearningchoi/lecture/15304?isDesc=false)
4. ["A Fast Learning Algorithm for Deep Belief Nets", Geoffrey E. Hinton. (2006)"](https://blog.naver.com/esemoon/222977153973)
5. ["Restricted Boltzmann Machines for Collaborative Filtering"](https://www.cs.toronto.edu/~amnih/papers/rbmcf.pdf)
6. [Deep Learning - RBM, DBN, CNN](https://sanghyukchun.github.io/75/)
7. [볼츠만 머신: 생성모형의 원리](https://horizon.kias.re.kr/18001/)


<br><br>
---