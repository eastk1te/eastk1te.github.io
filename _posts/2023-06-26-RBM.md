---
title: '[DL]RBM(Restricted Boltzmann Machine)'
author: east
date: 2023-06-26 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---


Restricted Boltzmann Machine(이하 RBM)은 심층 확률 모형을 구축하는 데 가장 흔이 사용되는 요소이며 ANN, DNN, CNN, RNN 등과 같은 타겟과 가설 간의 차이를 줄여서 오차를 줄이는 것이 목표인 결정적$$_{Deterministic}$$ 모델들과는 다르게 확률 밀도 함수를 모델링 하는 것이 목표이다.

> ## RBM(Restricted Boltzmann Machine)

확률 그래픽$$_{Probability \space Graphic}$$ 모델링을 위한 기초적인 모델 중 하나로, 잠재된 패턴을 자동으로 학습하여 더 높은 차원의 데이터를 분석하는 데 사용됩니다. 특히, 필터링, 특징값 학습, 차원 축소 등의 분야에서 활용되어 효과적인 성능을 발휘합니다.

> ### Graphical Model

Graphcial Model이란?
: 변수들간의 상호 의존 관계를 표현한 확률 모델로 변수간 의존관계를 설명하는 모델입니다.

![graphic](https://github.com/eastk1te/P.T/assets/77319450/9595d71a-2668-465a-95b0-9c0972f923b3)
_출처 : https://medium.com/@chullino/graphical-mode이란-무엇인가요-2d34980e6d1f_

그래픽 모델링은 변수 간의 상호 작용과 불확실성을 효과적으로 표현하기 위해 조건부확률과 체인룰을 이용합니다. 조건부확률을 통해 한변수가 다른 변수에 영향을 미치는 정도를 정량화하면, 변수 간의 종속성을 파악할 수 있으며 추론에도 도움이 됩니다. 또한, 체인룰을 활용하여 확률 분포들을 세부적으로 나누고 다양한 조건부확률들의 곱으로 분해해 계산을 단순화하여 계산 비용을 줄일 수 있게 도와줍니다. 즉, 변수간의 관계를 정량적으로 표현하고 계산량을 줄이는 방법입니다.

> #### Graphical Model의 분류

1. Directed Graphical Model 
   
    방향 그래프로 "Batesian Network", "Belief Network"라고도 불리며 화살표로 방향성 있는 엣지$$_{edge}$$를 표현합니다. 방향 그래프에서는 조건부 확률 분포를 기반으로 변수들 간의 관계를 모델링합니다.

2. Undirected Graphical Model
   
    무방향 그래프로 "Markov Random Field", "Markov Network"라고도 불리며 방향성이 없는 엣지로 표현합니다.무방향 그래프에서는 함께 등장하는 변수들의 결합 분포를 기반으로 모델링합니다.

3. Chordal Graphical Model
   
   순환 그래프가 없는 특수한 종류의 그래픽 모델로 각 사이클에 최소 하나의 "줄$$_{chord}$$"이 있는 그래프 입니다. 해당 그래프의 특징은 모든 순환 그래프 내에서 한 가지 이상의 동시 발생 관계를 가지는 것으로 복잡성을 줄이는데 도움이 되며, 각 변수들 간의 의존성을 포착하려는 응용 분야에서 사용됩니다.


> ### Restricted Boltzmann Machine(RBM)

![다운로드](https://github.com/eastk1te/P.T/assets/77319450/9e0712bf-02d6-49bd-ae92-03afebae4b76)
_Figure 1 : 좌 : 일반적인 볼츠만 머신으로 위의 layer는 이산형 값을 가지는 hidden feature이고 아래 layer는 동일하게 이산형 값을 가지는 visible feature임. , 우 : 제한된 볼츠만 머신으로 hidden, visible layer 각 계층 내의 연결이 존재하지 않음. _

Figure 1에서 좌측의 일반적인 볼츠만 머신은 신경망 기반의 확률적 에너지 모델로 입력 데이터의 고차원 평가 공간을 낮은 차원으로 투영시키고, 데이터로 부터 숨겨진 특징$$_{feature}$$인 확률 밀도 함수를 학습하기 위한 머신 설계입니다. 해당 모델은 완전연결그래프$$_{Fully-connedted-graph}$$로 표현되며 에너지 함수를 최소화하도록 가중치를 조정하는 과정을 통해 학습됩니다.

해당 모델에서는 이산형 값$$_{binaray-state}$$만을 가지는 Hidden unit과 visible unit들이 존재하는데, hidden unit은 우리가 보지 못하는 어떤 특성이 존재함을 암시하고, 보이지 않는 factor들까지도 학습할 수 있다면 좀 더 정확한 확률분포를 학습할 수 있다는 것을 전제합니다.

Figure 1에서 좌측의 BM과 다르게 우측의 RBM에서는 동일한 layer의 node 간의 내부적인 연결이 없어진 것은 사건 간의 독립성을 가정함으로써 확률분포의 결합을 쉽게 표현하기 위해서입니다.

> ### Energy

RBM이 학습을 잘 했다는 것을 수치적으로 판단하기 위해서는 여타 model들을 구성할 때와 마찬가지로 cost function을 정의해주어야 한다.

RBM의 cost function은 독특하게 “Energy”라는 개념을 이용해 정의되게 될텐데, 이 energy라는 개념은 물리학에서 차용한 것이라고 한다. 물리에서 예시를 들자면 어떤 기체가 그 공간에 있을 확률은 온도(에너지)와 반비례한다고 한다.

![Energy](https://github.com/eastk1te/P.T/assets/77319450/eff66286-11d8-424e-8be3-fc74d660071d)
_Figure 2 : 입력 x가 나올 확률은 어너지 $E(x)$에 반비례함._

![Energy based](https://github.com/eastk1te/P.T/assets/77319450/21d2c515-8264-4469-8603-31749c8050c6)
![Probability](https://github.com/eastk1te/P.T/assets/77319450/fc032a1c-4988-415f-b0c5-d761f5f450e8)

MLE
![MLE](https://github.com/eastk1te/P.T/assets/77319450/18e56300-0a5b-4c30-8d29-fd10a6cfbc7d)
1. 히든이 주어졌을때 visible, visible이 주어졌을떄 히든 노두 모두 summatu
2. 모두 주어졌을때 visivble 모든 가능한 visible과 hidden sumaation



여기서 2번은 계산량이 너무 많아짐 따라서 CD를 활용함

gibbson 샘플링을 진행 visible to hidden이 positive phase
그리고 hidden to visible


![conditional distribution](https://github.com/eastk1te/P.T/assets/77319450/12e60b5e-db57-4ab3-b85a-961fe1e1bbd1)


즉, $loss = F(v) - F(v^1)$
즉, 원래의 데이터로부터 얻은 visible layer의 free energy와 생성된 데이터로부터 얻은 visible layer의 free energy 간의 차이가 적을 수록 학습이 잘 된 것이라고 할 수 있으므로 loss는 다음과 같이 생각할 수 있다.

따라서 해달 loss를 역전파를 통해 가중치들을 업데이트 해가며 학습해나간다.

몬테카를로(Monte Carlo Method)이란?
: 랜덤 표본을 뽑아 함수의 값을 확률적으로 계산하는 알고리즘으로 값을 근사적으로 계산하려고 할때 쓰임. ex. 원주율 계산 - 무직위 점들을 뽑아 놓고, 해당 점들이 원안에 속하는 비율을 계산하면 대략적으로 원의 넓이를 구할 수 있음.

마코프 연쇄(Markob Chain)이란?
: 특정 시점의 상태 확률은 그 이전 상태에만 의존함. 즉, $P(q_t|q_{t-1})$이고, 이를 반복하다보면 직전 상태의 확률과 같아지는 수렴 상태가 나타납니다. 이렇게 평형 상태에 도달한 확률 분포를 정적분포(Stationary Distribution)이라고 함.

MCMC(Monte Carlo Markov Chain, 마코프 연쇄 몬테카를로 방법)이란?
: 샘플을 얻으려고 하는 목표분포를 정적 분포를 활용하여 마코프 체인을 만들어 시뮬레이션을 가동하고 초기값에 영향을 받는 burn-in period를 지나고 나면 목표분포를 따르는 샘플이 만들어진다.

Gibbson sampling이란?
: MCMC의 일종으로 모든 샘플이 독립인 몬테카를로 방법과 다르게, 마코프 연쇄에 기반한 MCMC는 이전 샘플의 영향을 받는다는 점이 다릅니다. 깁스 샘플링은 이전 샘플의 영향을 받는 점이 MCMC와 같지만, 나머지 변수는 그대로 두고 한 변수에만 변화를 주는것이 다릅니다. 이렇게 뽑은 표본들의 앞부분은 초기 상태(임의의 값)에 크게 의존하지만 충분한 반복이 이루어진 후에는 초기 상태에 관계 없이 확률 분포에 기반한 표본 수집이 가능하다고 함.

> #### Contrastive Divergence

visible layer에 데이터를 주고 hidden layer의 node 값을 샘플링, 그런 다음 다시 visible layer의 데이터를 다시 예측하도록 하는 과정에서 처음 주어진 visible layer의 데이터와 다시 획득한 visible layer의 데이터가 얼마나 차이가 있는지를 말하는 것이다. log

- visible_to_hidden : 확률분포를 통한 샘플 생성을 의미함.
- hidden_to_visible : 주어진 데이터와 sampling을 통해 얻은 visible layer의 데이터가 원래 데이터와 거의 같아야 함.

> ## DBN(Deep Belief net)

![DBN](https://github.com/eastk1te/P.T/assets/77319450/6543d9a3-93e4-40a5-bae2-b1e83f2397e8)

이 논문에서 제안된 알고리즘을 구현하는 방법은 크게 세 가지로 나눌 수 있습니다. 
1. Greedy Layer-wise Pretraining:
DBN 내부의 각 계층에 있는 Restricted Boltzmann Machine (RBM)을 순차적으로 학습합니다.
RBM은 가시 레이어와 숨겨진 레이어간의 양방향 뉴런 연결을 학습하며 원본 데이터의 특징을 추출합니다.
학습 과정에서 Contrastive Divergence (CD) 알고리즘을 사용하여 RBM의 가중치 및 편향을 업데이트합니다.
각 층의 숨겨진 레이어에서 얻어진 출력은 다음 층의 입력으로 사용되며, 이러한 방식으로 모든 층을 위에서 아래로 선행하는 방식으로 단계별로 학습이 이루어진다.
이러한 방법으로 각 층에서의 특징 추출 과정은 속도가 빠르고 효율적이 된다.
1. Fine-tuning:
사전 훈련된 DBN 구조를 초기 신경망의 초기 가중치로 사용한다.
역전파(backpropagation) 알고리즘이 사용되어 그래디언트 최적화를 통해 가중치와 편향을 업데이트합니다.
목표는 전체 네트워크에서 예측 오차를 최소화하기 위해 가중치 및 편향을 조정하는 것입니다.
이 논문에서 성능 평가는 MNIST 손글씨 숫자 데이터셋과 TIMIT 음성 인식 데이터셋에 대한 실험에서 확인됩니다. 그 결과, DBN은 다양한 문제에서 기존 심층 신경망보다 더 나은 성능을 보여줍니다. 작업을 성공적으로 수행하는 능력이 향상되어, 사전 학습이라는 초기 교육의 도입이 신경망에 호응하여 딥러닝을 적용하기 적합하게 만들어 주었으며, 이것이 딥러닝의 발전 및 다양한 응용 분야에 영향과 기여를 하였습니다.

요약하면, DBM의 학습 과정은 사전 훈련 단계에서 초기 가중치를 설정하고, 파인 튜닝 단계에서 전체 네트워크를 최적화하는 것으로 구성되어 있습니다. 이 과정을 통해 DBM은 데이터의 특징을 효율적으로 포착하고 대규모 양자화 학습에 적용할 수 있습니다.

즉, 이 모델은 RBM을 맨 아래 data layer부터 차근차근 stack으로 쌓아가면서 전체 parameter를 update하는 모델이다. 이 모델을 그림으로 표현하면 아래와 같은 그림이 된다.




마지막 layer는 joint probability를 의미하고, 나머지 layer들은 모두 conditional probability로 표현된다. 참고로 전체를 jointly하게 표현하는 모델을 Deep Boltzmann Machine (DBM) 이라고 하는데, 이 모델의 경우 RBM update를 하는 알고리즘과 비슷한 알고리즘으로 전체 모델을 update하게 된다. 그러나 이 논문이 발표될 당시에는 DBN이 훨씬 간단하고 computational cost가 적기 때문에 DBN이라는 모델을 제안한 것으로 보인다.

이 모델이 의미있는 이유는 joint probability를 잘 표현하는 좋은 graphical model이어서가 아니라, 이 모델로 deep network를 pre-training하고 backpropagation 알고리즘을 돌렸더니 overfitting 문제가 크게 일어나지 않고 MNIST 등에서 좋은 성과를 거뒀기 때문이다. 즉, parameter initialization을 DBN의 joint probability를 maximize하는 (layer-wise로 ℓ
개의 RBM을 learning하는) 방식으로 하고 나서, 그렇게 구해진 parameter들로 deep network를 initialization하고 fine-tuning (backpropation) 을 했을 때, 항상 그 정도 size의 deep network에서 발생하던 overfitting issue가 사라지고 성능이 우수한 classifier를 얻을 수 있었기 때문이다.

DBN으로 unsupervised pre-training한 deep network 모델을 사용했을 때 MNIST 데이터 셋에서 그 동안 다른 모델들로 거뒀던 성능들보다 훨씬 우수한 결과를 얻을 수 있었고, 그때부터 deep learning이라는 것이 큰 주목을 받기 시작했다. 그러나 지금은 데이터가 충분히 많을 경우 이런 방식으로 weight를 initialization하는 것 보다 random initialization의 성능이 훨씬 우수하다는 것이 알려져있기 때문에 practical한 목적으로는 거의 사용하지 않는다.


Explaining away?
: https://blog.naver.com/esemoon/222977153973


---

> ## AutoEncoder
Unsupervised Learning
Representation Learning
Dimensionality Reduction
Generative Model Learing
![img](https://github.com/eastk1te/P.T/assets/77319450/06fddeb7-8ffe-44d9-8896-6fa046a761f6)

오토인코더(Autoencoder)는 비지도 학습 방식의 인공 신경망입니다. 이 구조는 입력 데이터의 차원 축소(representation learning)와 노이즈 제거를 위한 효율적인 데이터 인코딩을 수행할 수 있습니다. 기본적으로 오토인코더는 데이터의 은닉적 표현(hidden representation)을 학습하고 복원된 출력을 생성합니다. 오토인코더 구조는 크게 세 부분으로 구성됩니다:
인코더(Encoder): 입력 데이터를 받아들이고, 은닉 레이어를 통해 저차원의 은닉 표현으로 변환합니다.
은닉 표현(Hidden representation): 인코더를 통해 얻어진 입력 데이터의 압축된 표현입니다. 이 은닉 표현은 중요한 특성을 포착하며, 원래 데이터의 차원보다 낮은 차원의 공간에 위치합니다.
디코더(Decoder): 은닉 표현을 입력으로 받아, 원본 입력 데이터와 유사한 높은 차원의 복원된 출력 데이터를 생성합니다.
오토인코더의 몇 가지 변형과 응용 분야는 다음과 같습니다:
변분 오토인코더(Variational Autoencoder, VAE): 출력 값의 확률 분포를 학습하며, 생성 모델로 작동하는 오토인코더의 한 형태입니다.
완전히 연결된 오토인코더(Fully Connected Autoencoder): 모든 노드가 인접한 레이어의 노드와 완전히 연결되어있기 때문에 기본적인 오토인코더입니다.
합성곱 오토인코더(Convolutional Autoencoder): 이미지 데이터를 처리할 때 사용되며, 합성곱 레이어와 풀링(Pooling) 레이어가 적용된 인코더와 디코더 구조를 가집니다.
순환 오토인코더(Recurrent Autoencoder): 시계열 및 시퀀스데이터 처리를 위해 시퀀스 인코더와 시퀀스 디코더로 구성된 형태입니다.
잡음 감소 오토인코더(Denoising Autoencoder): 입력 값에 노이즈를 추가하여 견고한 특성 표현을 학습하고 입력 데이터의 노이즈를 제거하는 데 사용됩니다.
오토인코더는 비지도 학습의 아이디어를 기반으로하며, 특성 추출, 차원 축소, 노이즈 제거 및 생성 모델링과 같은 여러 데이터 공학 및 기계 학습 문제에 사용될 수 있는 강력한 도구입니다.


오토인코더는 입력과 출력의 크기가 같아야 하는 이유는, 원본 입력 데이터와 복원된 출력 데이터 간의 차이를 최소화할 목적을 가지고 있기 때문입니다. 오토인코더의 목표는 원본 입력 데이터를 압축된 은닉 표현 형태로 인코딩하고, 이것을 다시 복원하여 원본 입력 데이터와 유사한 데이터 출력값을 복구하는 것입니다. 이 과정에서 손실(minimization loss)는 일반적으로 입력 데이터와 복원된 출력 데이터 간의 차이를 나타내는 값입니다. 따라서 출력 데이터 크기가 입력 데이터 크기와 동일해야 이러한 재구성 손실을 계산할 수 있으며, 모델 학습을 통해 원본 데이터를 잘 인코딩하고 복원할 수 있습니다.

Manifold(다양체)란?
: 수학과 기하학에서 사용되는 용어로, 저차원 공간에서의 지역적 구조를 가지는 고차원 공간의 일부를 의미함. 즉, 고차원 공간의 subspace로 차원 축소를 가능케 함. 예시로 고차원에 공간에 한 점으로 이미지를 매핑시키면 유사한 이미지들이 모여 전체 공간의 부분집합을 이루는데 그것을 매니폴드(Manifold)라고 부른다.(ex. [Cloud Vision API Demo](http://vision-explorer.reactive.ai/#/galaxy?_k=n2cees), t-SNE )

---

> ## Autoencoder


Restricted Boltzmann Machine(RBM)은AutoEncoder(AE)와 유사한 목표를 갖고 있다. 히든 레이어에서 데이터에 관한 latent factor들을 얻어내는 것이 목표로 데이터를 압축하고, 다시 압축 해제하는 과정을 거치게 만드는 뉴럴넷

왜 굳이 압축하고 풀어낼까?
1. 뉴럴넷이 압축하는 방식을 배우면 비선형적인 차원감소를 수행하고, 이를 통해 더 낮은 차원으로 표현
2. 새로운 데이터를 생성할떄 힘을 발휘할 수 있기 때문

입력 데이터 &rarr; encoder &rarr; representation vector &rarr; decoder &rarr; 출력 = 입력데이터

인코더의 역할
데이터 압축의 관점에서 '차원 감소'와 비슷할 수 있으나 "투영"이 아닌 비선형적인 방식의 차원감소가 이루어지기에 압축이라는 말이 사용된다.

즉, 인코더의 역할은 주어진 고차원의 데이터를 낮은 차원의 벡터로 압축시켜 표현하고 representation vector가 표시되어 있는 벡터공간을 latent space 라고 부른다.

디코더의 관점에서
latent space안에 있는 representation vector를 임의로 주었을떄, 그것을 갖고있는 원래의 의미를 "압축해제"하여 원본 사이즈의 데이터 형태로 복원시키는 역할

딥러닝의 핵심적인 내용은 단순히 "NN의 층이 깊다" 정도에서 끝나는 것이 아니라, "층이 깊어질때 무슨 효과가있는가"이다.

층이 더 깊어질수록(즉, input layer에서 멀어질수록) 추상적인 feature를 추출할 수 있다고 알려짐.

이렇듯 AE를 이용해 더 깊은 층을 ㅏㅎ는 것을 Stacked AutoEncoder라고 하며, 추상적인 특성들을 fine-tuning하면 깊은 뉴럴 네트워크를 훈련시킬 수 있을 것이라는 생각이 딥러닝 알고리즘을 있게함.

---

> ## Ⅱ. CONCLUSTION.

> ## Ⅲ. REFERENCES


1. ["A fast learning algorithm for deep belief nets"](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
2. ["Reducing the Dimensionality of
Data with Neural Networks"](https://www.cs.toronto.edu/~hinton/absps/science.pdf)
1. [temp](https://www.edwith.org/deeplearningchoi/lecture/15304?isDesc=false)
2. [temp](https://blog.naver.com/esemoon/222977153973)
3. [temp](https://deepinsight.tistory.com/126)
4. [temp](https://www.cs.toronto.edu/~amnih/papers/rbmcf.pdf)
5. temp
6. temp
7. temp
8. temp
9. temp




<br><br>
---


