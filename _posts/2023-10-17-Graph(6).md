---
title: '[Graph]'
author: east
date: 2023-09-12 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

해당 포스트는 공부를 위해 개인적으로 정리한 내용으로 해당 도서에는 다양한 예시를 통해 좀 더 직관적인 이해가 가능합니다.

작성시 목차에 의존하지 말고 내가 직접 생각하면서 정리하기!
만약 목차대로 작성한다면 그냥 "깜지"랑 다를게 없지 않은가?

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω.⋅)



## Graph Neural Networks
Deep Graph Encoders

우리는 graph neural network에 기반한 딥러닝을 얘기한다.

$$
\begin{matrix}
ENC(v) =& \text{그래프 구조에 기반한 } \\
& \text{다층 비선형 변환}
\end{matrix}
$$ 

이러한 모든 deep encoder들은 3에서 배운 유사도 함수를 가지고 결합된다.

![4-1](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/81b33fa7-7663-46f1-8915-a3d733fe37cb)
![4-2](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/c3084cbd-1a24-4bda-bae4-3a3d2fb61c35)
_Figure 4-2_

현대의 딥러닝 toolbox는 간단한 sequence와 grid를 위해 디자인 되어있었따.
네트워크는 임의적인$$_{Arbitrary}$$ 크기와 복잡한 topological한 구조로 위의 형태보다 더 복잡하다.


### Bascics of deep learning

최적화로써의 기계학습.
지도 학습은 x가 주어졌을때 y를 학습하는 것을 목표로한다.
x는 vector(숫자들), sequence(자연어), martirx(이미지), graph(node, endge의 특징을 가진 잠재성)가 될 수 있다.
우리는 최적화 문제로써 이러한 작업을 공식화한다.

$$\min\limits_{\theta}\mathcal{L}(y,f(x))$$

$$\theta$$는 우리가 최적화해야하는 파라미터들의 집합이다.

$$\mathcal{L}$$은 loss function으로 L2 loss를 예로 들면 아래와 같다.

$$\mathcal{L}(y,f(x))=||y-f(x)||_2$$

흔히 사용되는 다른 loss 함수로 , L1 loss, huber loss, max margin(jinge loss), cross entropy 등등이 있다.,
https://pytorch.org/docs/stable/nn.html#loss-functions

$$x^{(l+1)}=\sigma(W_lx^{(l)}+b^l)$$

다층 퍼셉트론 MLP의 각 층은 비선형성과 선형 변환의 결합으로 이루어져 있었다.


### Deep learning for graphs

#### Local network nighborhoods


종합 전략을 묘사한다.계산 가능한 그래프를 정의한다.

Naive Approach
인접행렬과 특징을 결합하여 deep 신경망에 넣는다.
해당 아이디어으 문제는 $$O(|V|)$$의 파라미터이고, 다른 사이즈의 그래프는 적용이 안되고, 노드를 정리하는데 민감하다.

CNN 
목표는 간단한 격자 Leverage 노드의 특징과 특성을 넘어서 convolution을 일반화하는것이다.
하지만 실제로는 지역과 sliding window가 그래프안에서 고정되어있지 않고, 그래프는 순열의 순서를 무시$$_{permutation-invariant}$$한다.

그래프는 노드를 정리하는 규범$$_{canonical}$$이 존재하지 않았다.
그래프의 노드를 정리하는 계획은 같아야한다. 우리는 fucntion f로 그래프 G가 주어졌을때 임베딩 벡터로 매핑하는 것을 학습하는 것을 고려해서 다른 order plan i,j를 가지고 $$f(A_i, x_i)$$와 $$f(A_j, x_j)$$가 같으면 f를 permutation invariant function이라고 한다.

f가 permutauib0iunvariant하다면 모든 순열$$_{permutation}$$에 대해서 $$f(A,X)=f(PAP^T,PX)$$이다.

![4-3](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/802b7009-0670-4ddc-a95b-135831d26fff)
_Figure 4-3_

우리는 graph G = (A,X)를 $$\mathbb{R}^{m\times d}$$로 매핑하는 함수 f 를 학습해야하고 그래프에서 같은 위치 노드의 output vector가order plan에 의해 바뀌지 않는다면 우리는 f를 permutaion equivariant라 부른다.

$$f:\mathbb{|V|\times m} \times \mathbb{R}^{|V|\times|V|} \rightarrow \mathbb{R}^{|V| \times m}$$, f는 permutation-equivariant이고 어떤 P$$_{permutation}$$에 대하여 $$Pf(A,X)=f(PAP^T,PX)$$이다.

Permuation-invariant
$$f(A,X)=f(PAP^T,PX)$$
Permuation-equivariant
$$Pf(A,X)=f(PAP^T,PX)$$

GNN은 여러개의 permutation equivariant와 invariant 함수로 구성되어있다.

![4-4](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/6d1e46d1-cd2b-4bb7-9b46-af59e9461b87)
_Figure 4-4_

다른 신경망 구조에서는 permutaion invariant. equivariant한가?
그렇지 않다. 입력 순서를 바꾸면 결과값이 다르게 나온다. 이게 나이브 MLP 접근법이 그래프에서 실패하는 이유를 알려준다.

이제 이웃에서 passing and aggregationg information하는 방법으로 GNN을 permutation invarinat. equivariant로 디자인하는 법을 배운다.,


![4-5](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/45b36318-566d-4406-b7db-48e5d1c63487)
_Figure 4-5_

노드의 특징을 계싼함으로써 그래프의 정보를 어떻게 전달하는지 학습한다.


주된 아이디어는 local network 이웃들에 기반하여 노드 임베등을 생성한다./

![4-6](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/7012fd39-7a1d-4760-b284-cedba231bdd6)
_Figure 4-6_

노드들의 이웃들은 계산 그래프$$_{computation-graph}$$로 정의 된다.
노드는 신경망을 이용해 그들의 이웃들로 부터 정보를 종합한다. 모든 노드는 그들의 이웃을통해 계산 그래프로 정의 된다.

모델은 임의적인 깊이가 된다.
노드는 각 층에서 임베딩을 가지게 되고
노드 v에 대한 초기 층의 임베딩은 $$x_v$$의 특징을 입력값으로 한다.
k번째 층의 임베딩은 k hops 떨어진 노드에서 정보를 가지고 온다.

주된 차이는 각 층을 넘어서 정보를 종합하는 접근법을 얼마나 다르게 하는가이다.

기본 접근법으로 평균을 사용하는 것이다.


![4-7](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/687695bc-324e-43d6-916e-1922adcd820e)
_Figrue 4-7_


GCN
GCN에서 invariance와 equivariance의 성질은 무엇인가?
노드가 주어졌을때 GCN은 임베딩을 permutaion invariant로 계산한다.
즉, 같은 위치의 노드를 같은 임베딩 벡터로 변환한다.

![4-8](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/d5f57f32-2201-47b0-818b-a9a44042ad26)
_Figrue 4-8_

GCN에서 주어진 노드를 임베딩을 계산하는것은 invariant하다는 것을 안다.
permutaion 이후 입력 노드 특징 행렬에서 주어진 노드의 위치는 변하고, 주어진 노드에서 나온 output 임베딩은 같게 된다. 이것이 permutaion equivariant이다.

즉 permutation equivariant를 

#### stacking multiple layers

모델 파라미터 훈련 등을 묘사한다.
모델을 어떻게 적합하는가?
쉬운 비지도, 지도 학습 예쩨


이제 생성된 임베딩으로 GCN을 어떻게 학습시켜야하는가?
임베딩의 loss 함수를 정의할 필요가 있다.

![4-9](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/eeb1dc18-6f4b-467a-9f03-f8cb4919970f)
_Figrue 4-9_

임베딩을 아무 loss 함수에 넣고 SGD를 통해 파라미터를 학습시킨다.

$$h_v^t$$ : k 층의 v 노드의 은닉 표현
$$W_k$$ : 이웃들의 종합 가중치 행렬
$$B_k$$ : 자신의 은닉 벡터를 변환하는 가중치 행렬

많은 (희소) 행렬 작업으로 많은 aggregation은 효과적으로 수행된다.

$$H^{(k)}=[h_1^{k}, ..., h_{|V|}^{k}]^T$$

$$\sum_{u \in N_v}h_u^{(k)}=A_{v,:}H^{(k)}$$가 된다.

$$D_{v,v}=Deg(v) = |N(v)|$$
D의 역행렬은 또한 대각행렬이므로 $$D_{v,v}^{-1}=\frac{1}{|N(v)|}$$

그러므로 $$\sum_{u \in N(v)}\frac{h_u^{(k-1)}}{|N(v)|} \rightarrow H^{(K+1)=D^{-1}AH^{(k)}}$$


![4-10](https://github.com/eastk1te/eastk1te.github.io/assets/77319450/f57b9e4d-2bc2-4ee5-83cb-38c118d89c06)
_Figure 4-10_

실제로 효율적인 희소 행렬 곱은 $$\tilde{A}$$가 희소하다는 것을 암시한다.


노드임베딩 $$z_v$$는 입력 그래프의 함수이다.
지도 학습은 $$\min\limits_{\theta}\mathcal{L}(y,f(z_u))$$ 하는 것이고, 비ㄷ지도 학습은  노드를 라벨링하는게 불가능하여 그래프 구조를 감독으로 사용하는 것이다.

$$\mathcal{L} = \sum_{z_u, z_v}CE(y_{u,v}, DEC(z_u,z_v))$$

$$y_{u,v}=1$$이면 노드 u와 v가 유사하다는 것을 의미하고
CE는 cross entropy, DEC는 내적 곱과 같은 decoder이다.

노드 유사도는 3장에서 설명한것과 같이 무작위 보행, 행렬 분해, 그래프의 node proximity 등으로 될 수 있따.

지도 학습은 지도 작업에 모델을 바로 학습시키는것이다.

$$\mathcal{L}=-\sum_{v\in V}y_vlog(\sigma(zv^T\theta))+(1-y_v)log(1-\sigma(zv^T\theta))$$

$$y_v$$는 노드 class label이고
$$z_v^T$$는 인코더 아웃풋으로 노드 임베딩이고 $$\theta$$는 분류 가중치이다.


(1) Define a neighborhood aggregation function
(2) Define a loss function on the embeddings
(3) Train on a set of nodes, i.e., a batch of compute graphs
(4) Generate embeddings for nodes as needed, Even for nodes we never trained on!

inductive capability 귀납 능력
같은 종합 파라미터는 모든 노드에서 공유된다.

따라서 귀납 노드 임베딩은 보지 못한 그래프 전체를 일반화할 수 있다.
많은 적용에서 끊임없이 보지못한 노드들을 마주치게 된다. 따라서 새로운 임베딩을 "매번 봐가며"$$_{on-the-fly}$$ 생성할 필요가 있다.


### Graph Convolutional Netyworks and Graphsage

GraphSAGE idea
지금까지 이웃 페시지의 가중 평균을 통해 종합했는데 이보다 더 잘할 수 있을까?

$$h_v^{l+1}=\sigma([W_l \cdot AGG(\{h_u^{(l)}, \forall u \in N(v)\}), B_lh_v^{(l)}])$$

해당 메시지 architecutue를 어떻게 다르게 보낼 수 있을까?

$$l_2$$ Normalization으로 


$$h_v^t \leftarrow \frac{h_v^t}{||h_v^t||_2} \forall v \in V \text{ where} ||u||_2 = \sqrt{\sum_iu_i^2} (\text{L2-norm})$$
L2-norm이 없으면 벡터들은 모두 다른 scale로 임베딩 될 것이다. 어떤 케이스에서는 임베딩 normalization이 성능의 향상을 이끌어내기도한다. 이렇게 L2 정규화를 진행하면 모든 벡터들은 같은 L2-norm을 가진다.

_Figure 4-11_

Neighbor Aggregation : Variants

- Mean : 이웃의 가중합으로 가진다
  - $$AGG = \sum_{u \in N(v)}\frac{h_u^{(l)}}{|N(v)|}$$
- Pool : 이웃 벡터들을 변환하고 대칭적인$$_{symmetric}$$ 벡터 함수를 적용한다
  - $$AGG = \gamma(\{MLP(h_u^{(l)}), \forall u \in N(v)\}); \text{ where }\gamma : \text{Element-wise mean/max}$$
- LSTM : LSTM을 적용하여 이웃들을 reshuffle한다.

GCN, GRaphSAGE
주된 아이디어는 지역 이웃들을 기반으로 노드 임베딩을 생성하는 것이다.
노드들은 그들의 이웃들로부터 신경망을 활용해 메시지를 종합한다.
GCN : 기본 형태$$_{basic-variant}$$ : 이웃들의 정보를 평균하고 신경망을 쌓는다.
GraphSAGE : 이웃들의 종합을 일반화한다.










GNN을 CNN과 같은 유명한$$_{prominent}$$ 어떻게 비교할 수 있을까>?

CNN formulation
$$h_v^{l+1}=\sigma(\sum_{u \in N(v) \cup \{v\}}W_l^uh_u^{(l)}), \forall l\in {0,...,L-1}$$

3x3 필터를 가진 CNN에서는 $$N(v)$$는 v의 8개의 이웃 픽셀을 표현한다.

GNN formulation
$$h_v^{l+1}=\sigma(W_l\sum_{u \in N(v)}\frac{h_u^{(k-1)}}{|N(v)|} + B_lh_v^{(l)})\forall l\in {0,...,L-1}$$
로 CNN 함수를 다시 스면 
$$h_v^{l+1}=\sigma(\sum_{u \in N(v)}W_l^uh_u^{(l)} + B_lh_v^{(l)}), \forall l\in {0,...,L-1}$$
쓸 수 있고,  이미지에서 u의 서로 다른 이웃인 픽셀 v에 다른 가중치 $$W_l^u$$를 학습할 수 있다. 이유는 픽셀의 중심을 기준으로 상대적인 위치를 가진 9개의 이웃들의 순서를 부여할 수 있어서 입니다.

CNN은 순서화되고 고정된 이웃 크기를 가진 특별한 GNN으로 보여질 수 있다.
CNN은 not permutation invariant/equivariant이다.
픽셀의 순서를 바꾸게 되면 다른 결과로 이끌것이다.


<br><br>
---