---
title: '[Paper]knowledge distillation'
author: east
date: 2024-01-01 00:00:00 +09:00
categories: [Paper, Meta Learning]
tags: [Paper, Meta Learning]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---


파라미터 쉐어링
: 동일한 잠재공간으로 보낸다는 의미.






knowledge distillation
지식이 큰 모델로부터 증류한 지식을 작은 모델로 이전하는 일련의 과정.

모델 배포 차원.
“Distilling the Knowledge in a Neural Network” 앙상블 같은 ㅁ복잡한 모델은 다량의 유저에게 배포하는건ㅇ ㅓ려움.

Hinton’s KD

soft labvel
출력값에 분포를 좀 더 soft하게 만들면 모델이 가진 지식

퀀터마이제이션,








> ## . REFERENCES

1. temp
2. temp
3. temp
4. temp


<br><br>
---