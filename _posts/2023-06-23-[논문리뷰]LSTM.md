---
title: '[논문 읽기]LSTM(순환신경망)'
author: east
date: 2023-06-17 00:00:00 +09:00
categories: [TOP_CATEGORIE, SUB_CATEGORIE]
tags: [TAGS]
math: true
mermaid: true
# pin: true
# image:
#    path: image preview url
#    alt: image preview text
---

RNN을 기반으로한 LSTM으로 RNN에 대한 글을 확인하고 싶으면 해당 링크로 이동
[Vanilla RNN](../RNN)


논문을 해석하고, 아래 구조에 맞게 맞출 것.


NLG 태스크는 본질적으로 확률적인 작업에 기반한다
predict the next word in a sequence given the words
P(appe and pair salad) => P(pair | apple and)

RNN 기반의 언어 모델
토크나이저 : 각 단어를 토큰화(벡터화)
지도학습 

Gated Recurrent Neural Networks
gradient가 학습이 진행될수록 그래디언트 소실, 폭발의 문제가 있음
long-term dependencies
vanilla RNN에서 반복해서 multiplication되는 같은 가중치 W가 만약 1보다 작으면 소실, 크면 폭발하는 문제가있음.

LSTM
use self-loops 
can remove.add information to cell state(like memory)
carefully regulated bty structures called gates

> ### ⅵ. Bidirectional RNN

![Bidirection RNN](https://github.com/eastk1te/P.T/assets/77319450/ac782aa5-3b2a-49aa-9d0d-571689370f4e)

이전의 RNN 구조들은 간편한 구조를 지녔지만, output $y^t$가 input sequence 전체에 의존하는 경우.
- ex) speech recognition, handwrite recognition, seq2seq learning

Bidirectional RNN이 나온 배경이 됨.

h : recurrence propagateds forward in time
g : recurrence propagateds backward in time
output units can benefit from past, future $h^t$

image captioning =? Convolutional Neural Networks for Visual recognition(2017)

http://cs231n.stanford.edu/2017/index.html

---

language model

Seq2Seq and Attention
Attention mechanism
sequece -to-sequence model에서는 dimension of context C may be too small to summarize a longsequence한 제한이 있다.

어텐션 메커니즘은 C를 variable-lenth sequence(not fixed-size vector)로 만드는 것.
sequence C의 원소인 $c^t$들을 output sequece의 원소인 $y^t$들과 매치


![context sequence](https://github.com/eastk1te/P.T/assets/77319450/130c09a4-3fd1-4203-af0d-3be27e363c8b)

![LSTM cell](https://github.com/eastk1te/P.T/assets/77319450/d72dba19-9538-4726-b727-cc02129875ae)
![LSTM comparison](https://github.com/eastk1te/P.T/assets/77319450/a8784b07-f672-43a0-b254-bbbbe2484274)
![attention machanism](https://github.com/eastk1te/P.T/assets/77319450/cbbe0628-fc5d-486b-9a6c-3e4fdca77b0a)



> ## Ⅰ. LSTM(Long Short Term Memory)

논문의 제목, 저자, 출판 연도 등을 소개합니다.
연구 주제와 중요성에 대해 간략히 설명합니다.
논문의 목적과 기여도를 요약합니다.

> ### ⅰ. Motivation and Problem Definition

LSTM은 1997년에 제안된 기법으로서, Sepp Hochreiter와 Jürgen Schmidhuber에 의해 개발되었습니다. 이전에 사용되던 기본적인 순환 신경망(RNN) 구조는 시간에 따라 정보를 전달하고 기억하는 데 어려움이 있었습니다. 특히 긴 시퀀스 데이터를 처리하거나 장기적인 의존 관계를 학습하는 데 한계가 있었습니다.

LSTM은 이러한 문제를 극복하기 위해 메모리 셀(memory cell) 개념을 도입했습니다. 메모리 셀은 RNN의 단계마다 이전 시간 단계에서의 정보를 저장하고, 필요한 경우 이를 활용하여 현재 단계의 계산에 반영합니다. 이는 시간적인 상태 정보를 유지하면서 장기적인 의존 관계를 학습하는 데 도움이 되었습니다.

LSTM의 핵심 개념은 게이트(gate)라고 불리는 요소입니다. 입력 게이트, 삭제 게이트, 출력 게이트 세 가지 게이트가 LSTM에서 사용되며, 각 게이트는 시그모이드 함수와 행렬 연산을 통해 정보의 흐름을 제어합니다. 이 게이트들은 현재 입력과 이전 단계의 출력을 기반으로 어떤 정보를 업데이트하고, 어떤 정보를 유지 또는 삭제할지 결정합니다.

LSTM은 이러한 게이트 메커니즘을 통해 장기 의존 관계를 학습할 수 있으며, 긴 시퀀스에서도 효과적으로 정보를 전달할 수 있습니다. 이로 인해 LSTM은 다양한 자연어 처리 작업에 적합하며, 특히 기계 번역, 문장 생성, 감성 분석 등에서 뛰어난 성능을 보입니다.

LSTM은 딥러닝 분야에서 중요한 발전을 이루었으며, 이후 다양한 변형과 확장이 이루어져 LSTM 기반의 신경망 구조가 널리 사용되고 있습니다. LSTM의 성공은 순차적인 데이터 처리와 장기적인 패턴 학습에 대한 이해를 개선하고, 
딥러닝 모델의 발전에 기여한 중요한 기술 중 하나로 평가됩니다.



===


> ## LSTM


배경 

"그래디언트 소실 및 폭발"로 인한 장기적 시퀀스에 대한 학습이 어려움.
"긴 기간의 의존성(long-term dependencies)"
크기가 큰 문맥을 받으면 필요한 정보를 얻기 위한 시간격차는 굉장히 커지게 된다. 이 격차가 늘어날수록 RNN은 학습하는 정보를 계속 이어나가기 힘들어한다.



1. "cell state"
LSTM의 핵심. 그림에서 수평으로 그어진 윗 선
컨베이어 벨트와 같아서, 작은 linear interaction만을 적용시키면서 전체 체인을 계속 구동. 정보가 전혀 바뀌지 않고 그대로 흐르게만하는것은 매위 쉽다.

계층 내부에서 각 시간 단계 간에 값을 전달하여 시퀀스 내의 정보를 보존할 수 있습니다. 메모리 셀은 셀 상태를 업데이트하는 게이트 메커니즘을 사용하여 원하는 정보를 식별 및 기억할 수 있으며 필요한 시점에 사용할 수 있습니다.


cell state에 뭔가를 더하거나 없앨수있는능력이 있는데, gate라고 부리는 구조에 의해서 제어

2. 게이트 메커니즘: LSTM 계층에는 다음과 같은 3개의 게이트가 있습니다.
    a. 망각 게이트 (Forget Gate): 셀 상태에 어떤 정보를 잊어버려야 하는지 결정합니다. 시그모이드 활성화 함수를 사용하여 이전 상태의 정보에 대한 가중치를 0에서 1 사이의 값으로 줍니다 (0은 잊어버림, 1은 완전히 기억).
    b. 입력 게이트 (Input Gate): 셀 상태에 어떤 정보를 추가할지 결정합니다. 시그모이드 활성화 함수를 사용하여 각 정보의 중요성을 결정하고, 기존 셀 상태의 어떤 정보를 업데이트할지 결정하는 마스크 역할을 합니다.
    c. 출력 게이트 (Output Gate): 셀 상태와 현재 입력값을 바탕으로 다음 시각에 전달할 은닉 상태를 결정합니다. 시그모이드 활성화 함수를 사용하여 다음 은닉 상태로 전달할 정보의 중요성을 결정합니다.

첫번째 게이트 sigmoid layer 0 or 1로 어떤 정보를 버릴것인지 정하는 것.

$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$

input gate layer
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C_t} = tanh(W_C \cdot [h_{t-1}, x_t]) + b_C$$

$$C_t = f_t *C_{t-1} + i_t * \tilde{C_t}$$

output
$$ o_t = \sigma(W_o[h_{t-1}, x_t] + b_o)$$
$$h_t = o_t * tanh(C_t)$$

지금까지는 완전 기초적인 모델로 peephole connection(엿보기 구명) 등을 추가한 모델등 다양하게 존재한다.

> ## GRU(Gated Recurrent Unit)

forget gate와 input gate를 하나의 "update gate"로 합쳤고, cell state와 hidden state를 합쳤다.
결과적으로 GRU는 기존 LSTM보다 단순한 구조를 가지고 점점 더 유명해지고 있다.

$$ z_t = \sigma(W_z \cdot [h_{t-1}, x_t]) $$
$$ r_t = \sigma(W_r \cdot [h_{t-1}, x_t]) $$
$$ \tilde{h_t} = tanh(W \cdot [r_t * h_{t-1}, x_t]) $$
$$ h_t = (1-z_t)*h_{t-1} + z_t*\tilde{h_t} $$

 가장 대표적인 것은 Gated Recurrent Unit (GRU)로, LSTM의 게이트 구조를 더 단순화하여 계산량을 줄이고 학습 속도를 높였습니다. 하지만, 네트워크의 표현력은 일부 제한될 수도 있습니다. 또한, LSTM은 단방향과 양방향 구조를 사용하여 시계열 데이터의 의존성을 더 정확하게 모델링할 수 있습니다. LSTM은 시계열 데이터나 텍스트 데이터와 같은 시퀀스 데이터를 처리하는 데 탁월한 성능을 보입니다. 자연어 처리, 금융, 음성 인식 등 다양한 분야에서 사용되며, 특히 계산 복잡성에 대한 문제를 각색한 여러 변형 및 최적화 방법을 통해 다양한 분야에 적용할 수 있도록 발전해 왔습니다.



[temp](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)


> ### ⅱ. Comparison with Related Work

이전 연구나 관련 연구들과의 차이점과 유사점을 분석합니다.
기존 연구의 한계점을 간략히 언급하고, 본 연구의 차별성을 강조합니다.

> ### ⅲ. Method and Model Description

연구에서 사용한 데이터셋, 실험 설정, 모델 아키텍처 등을 상세히 설명합니다.
데이터 전처리, 학습 알고리즘, 하이퍼파라미터 등을 포함한 실험 세팅을 설명합니다.
모델 아키텍처에 대한 세부 설명과 각 레이어 또는 구성 요소의 역할을 설명합니다.

> ### ⅳ. Experimental Results

수행한 실험의 결과와 평가 지표를 제시합니다.
실험 결과를 통해 얻은 주요 인사이트를 강조하고, 그 결과를 그래프, 표 등의 시각적인 자료와 함께 제시합니다.

> ### ⅴ. Discussion and Interpretation

실험 결과를 해석하고, 논문의 기여와 관련하여 결과를 평가합니다.
실험 결과의 잠재적인 한계점이나 제한 사항을 논의합니다.
추가적인 분석이나 실험 아이디어에 대해 제시하고 논의합니다.

> ## Ⅱ. CONCLUSTION.

논문의 주요 결과와 기여를 요약합니다.
본 연구의 한계와 향후 연구 방향을 언급합니다.

> ## Ⅲ. REFERENCES

> ### Overview




[1] : 

<br><br>
---